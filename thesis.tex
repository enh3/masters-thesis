%% Pour voir les accents de ce fichier, assurez-vous que votre
%% éditeur de texte lise le fichier en utf-8!

%% La classe <dms> est construite au-dessus de <amsbook>, donc
%% <amsmath>, <amsfonts> et <amsthm> sont automatiquement chargés.
%% Pour un mémoire
\documentclass[12pt,twoside,maitrise]{dms_ks}
%% Pour une thèse
%%\documentclass[12pt,twoside,phd]{dms}

\usepackage[utf8]{inputenc} %Obligatoires
\usepackage[T1]{fontenc}    %
\usepackage{epigraph}
\usepackage{csquotes}
\usepackage{graphicx}
\usepackage[backend=biber,style=verbose]{biblatex}
\graphicspath{{./graphics/}}
\usepackage{caption}
%%\usepackage{chngcntr}
%%\counterwithout{figure}{chapter}
\usepackage{float}
\usepackage{listings}
\usepackage{parskip}
\raggedbottom
%%\usepackage{courier}
\lstset{
    basicstyle=\fontsize{9}{9}\selectfont\ttfamily,
    showstringspaces=false
}

\DeclareCaptionFormat{custom}{%
    \setlength{\parindent}{0pt}% Remove the paragraph indentation
    %%\centered% Justify the caption text to the left
    \textbf{\fontsize{9pt}{0pt}\selectfont #1 #2}{\fontsize{9pt}{0pt}\selectfont #3}% Adjust the label formatting
}
\captionsetup[figure]{format=custom, labelsep=colon, belowskip=9pt, skip=9pt}

\newlength{\oldparskip}
\setlength{\oldparskip}{\parskip}  % Save the current parskip

\let\oldtableofcontents\tableofcontents  % Save the old definition of tableofcontents
\renewcommand{\tableofcontents}{  % Redefine tableofcontents
    \begingroup
    \setlength{\parskip}{0pt}  % Temporarily remove space between paragraphs in the TOC
    \oldtableofcontents  % Call the original tableofcontents
    \endgroup
    \setlength{\parskip}{\oldparskip}  % Restore the original parskip
}

%% <lmodern> incorpore les fontes en T1, pour
%% faciliter le dépôt final. Ceci n'est pas la
%% seule option :
%%  1. Si cm-super est installé, vous pouvez enlever <lmodern>
%%     (à ce moment, la police est un peu plus fidèle
%%      au Computer Modern orginal);
%%  2. Si vous avez une police préférée, par exemple,
%%     <times> ou <euler> ou <mathpazo> (et bien d'autres),
%%     alors vous pouvez remplacer <lmodern> ci-bas.
%% Par contre, si vous faîtes face à un problème d'encapsulation
%% lors dépôt final, il se peut que la solution soit d'utiliser <lmodern>.
%% (Parfois le problème est au niveau de l'installation, donc
%%  essayez de compiler sur un autre ordinateur sur lequel vous êtes
%%  certain·e que l'installation est bonne.)
\usepackage{mathptmx}

%% Il n'est pas nécessaire d'utiliser <babel>, car
%% les commandes intégrées par la classe <dms>
%% \francais et \anglais font le travail. Néanmoins,
%% certains autres packages nécessitent <babel> (comme
%% <natbib>), donc simplement enlever les % devant <babel>
%% dans ce cas. Attention! Certains packages sont sensibles
%% à l'ordre dans lequel ils sont chargés.
%%\francais % or
%%\anglais
%%
\usepackage[english]{babel}

 % ENGLISH OPTION
 % If you call \anglais here before the \begin{document},
 % all the chater's header will be in english, even if you
 % call \francais. To change this, use
 % \entetedynamique

%% La commande \sloppy peut avoir des effets étranges sur les
%% lignes de certains paragraphes.  Dans ce cas, essayez \fussy
%% qui suppresse les effets de \sloppy.
%% (\fussy est normalement le comportement par défaut.)
%% On redéfinit \sloppy, pour tenter de réduire les comportements
%% étranges. Le seul changement apporté à la version originale
%% est la valeur de \tolerance.
\def\sloppy{%
  \tolerance 500%  %9999 dans LaTeX ordinaire, mauvaise idée.
  \emergencystretch 3em%
  \hfuzz .5pt
  \vfuzz\hfuzz}
\sloppy   %appel de \sloppy pour le document
%%\fussy  %ou \fussy

%% Packages utiles.
\usepackage{graphicx,amssymb,subfigure,icomma}
%% icomma       permet d'écrire les nombres décimaux en
%%                  français (p.ex. 1,23 plutôt que 1.23)
%% subfigure    simplifie l'inclusion de figures côtes-à-côtes

%% Packages parfois utiles.
%%\usepackage{dsfont,mathrsfs,color,url,verbatim,booktabs}
%% dsfont       symboles mathématiques \mathds
%% mathrsfs     plus de symboles mathématiques \mathscr
%% color        pour utiliser des couleurs (comparer avec <xcolor>)
%% url          permet l'écriture d'url
%% verbatim     pour écrire du code ou du texte tel quel
%% booktabs     plus de macros pour faire les tableaux
%%                  (voir documentation du package)

%% pour que la largeur de la légende des figures soit = \textwidth
%%\usepackage[labelfont=bf, width=\linewidth]{caption}

%% les 3 lignes suivante servent à l'affichage de l'index
%% dans le visionneur de pdf. <hyperref> et <bookmark>
%% devraient être les dernier package a être chargé,
%% donc chargez vos packages avant.
\usepackage{hyperref}  % Ajoute les hyperlien
\hypersetup{colorlinks=true,allcolors=black}
\usepackage{hypcap}   % Corrige la position du lien pour les images
\usepackage{bookmark} % Remédie à des petits problème
                      % de <hyperref> (important qu'il
                      % apparaisse APRÈS <hyperref>)

  % Enlever les commentaires du prochaine \hypersetup et
  % le remplir avec l'information pertinente.
  % Ceci ajoute des « méta-données » au pdf.  C'est optionnel,
  % mais recommandé. Vous pouvez voir ces méta-données en
  % ouvrant un visionneur de pdf et en cherchant les propriétés
  % du pdf. (Vous pouvez aussi tapez ' pdfinfo <nom-du-pdf> '
  % dans un terminal.) Ces données sont utiles, par exemple,
  % pour augmenter les chances qu'un algorithme de recherche
  % trouve votre document sur Internet, une fois diffusé.
\hypersetup{
  pdftitle = {Contemporary perspectives on the hyper-organ: conceiving, playing and writing for an augmented Casavant Frères pipe organ},
  pdfauthor = {Sidloski·K},
  pdfsubject = {Developing a hyper-instrument interface for the pipe organ of l'église Saint-Édouard},
  pdfkeywords = {Hyper-instrument, pipe organ, sound design, composition, audio synthesis, sonic architecture, hauntology}
}

%% Définition des environnements utiles pour un mémoire scientifique.
%% La numérotation est laissée à la discrétion de l'auteur·e. L'exemple
%% illustré ici produit « Définition x.y.z »
%%   x = no. chapitre
%%   y = no. section
%%   z = no. définition
%% et la numérotation des corollaires, définitions, etc. se fait
%% successivement.
%%
%% Les macros \<type>name sont telles qu'ils suivent
%% la langue actuelle. (P.ex. si \francais est utilisé,
%% alors \begin{theo} va faire un Théorème et si \anglais
%% est utilisé, \begin{theo} fera un Theorem.)
%%
\newtheorem{cor}{\corollaryname}[section]
\newtheorem{deff}[cor]{\definitionname}
\newtheorem{ex}[cor]{\examplename}
\newtheorem{lem}[cor]{\lemmaname}
\newtheorem{prop}[cor]{Proposition}
\newtheorem{rem}[cor]{\remarkname}
\newtheorem{theo}[cor]{\theoremname}
\theoremstyle{definition}
\newtheorem{algo}[cor]{\algoname}
%% NOTE : Il peut être commode de redéfinir \the<type> pour
%% obtenir la numérotation désirée. Par exemple, pour
%% que les corollaires soit numérotés #section.#sous-section.#sous-sous-section.#paragraphe.#corollaire,
%% on fait
%% \renewcommand\thecor{\theparagraph.\arabic{cor}}

%%%
%%% Si vous préférez que les corollaires, définitions, théorèmes,
%%% etc. soient numérotés séparément, utilisez plutôt un bloc de
%%% commandes de la forme :
%%%

%%\newtheorem{cor}{\corollaryname}[section]
%%\newtheorem{deff}{\definitionname}[section]
%%\newtheorem{ex}{\examplename}[section]
%%\newtheorem{lem}{\lemmaname}[section]
%%\newtheorem{prop}{Proposition}[section]
%%\newtheorem{rem}{\remarkname}[section]
%%\newtheorem{theo}{\theoremname}[section]

%%
%% Numérotation des équations par section
%% et des  tableaux et figures par chapitre.
%% Ceci peut être modifié selon les préférences de l'utilisateur.
%%\numberwithin{equation}{section}
%%\numberwithin{table}{chapter}
%%\numberwithin{figure}{chapter}

%%
%% Si on veut faire un index, il faut décommenter la ligne
%% suivante. Ajouter des mots à l'index avec la commande \index{mot cle} au
%% fur et à mesure dans le texte.  Compiler, puis taper la commande
%% makeindex pour creer les indexs.  Après une nouvelle compilation,
%% vous aurez votre index.
%%

%%\makeindex

%% Il est obligatoire d'écrire à double interligne
%% ou à interligne et demi. On peut soit utiliser
%% le package <setspace> ou \baselinestretch.
%% Le package est un peu plus propre, mais le choix
%% reste à la discrétion de l'usager.
\usepackage[onehalfspacing]{setspace}
\addbibresource{ref.bib}
 % ou
%%\renewcommand{\baselinestretch}{1.5}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%                                     %%%%%%%%%%%%%
%%%%%%%%%% D é b u t    d u    d o c u m e n t %%%%%%%%%%%%%
%%%%%%%%%%                                     %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\entetedynamique

%%
%% Voici des options pour annoter les différentes versions de votre
%% mémoire. La commande \brouillon imprime, au bas de chacune des pages, la
%% date ainsi que l'heure de la dernière compilation de votre fichier.
%%
%%\brouillon
%%
%%
%% \version est la version de votre manuscrit
%%
\version{1}

%%------------------------------------------------- %
%%              pages i et ii                       %
%%------------------------------------------------- %

%%%
%%% Voici les variables à définir pour les deux premières pages de votre
%%% mémoire.
%%%

\title{Contemporary perspectives on the hyper-organ: conceiving, playing and writing for an augmented Casavant Frères pipe organ}

\author{Kjel Sidloski}

\copyrightyear{2024}

\department{Faculté de musique}

\date{\today} %Date du DÉPÔT INITIAL (ou du 2e dépôt s'il y a corrections majeures)

\sujet{musique}
\orientation{composition}%Ce champ est optionnel
%%
%% Voici les disciplines possibles (voir avec votre directeur):
%% \sujet{statistique},
%% \sujet{mathématiques}, \orientation{mathématiques appliquées},
%% \orientation{mathématiques fondamentales}
%% \orientation{mathématiques de l'ingénieur} et
%% \orientation{mathématiques appliquées}

\president{Nom du président du jury}

\directeur{Pierre Michaud}

\codirecteur{Caroline Traube}         % s'il y a lieu
%%\codirecteurs{Nom du 2e codirecteur}         % s'il y a lieu

\membrejury{Nom du membre de jury}

%%\examinateur{Nom de l'examinateur externe}   %obligatoire pour la these

%% \membresjury{Deuxième membre du jury}  % s'il y a lieu

%%  \plusmembresjury{Troisième membre du jury}    % s'il y a lieu

 % Cette option existe encore, mais elle n'a plus sa place
 % dans la page titre. L'utiliser seulement si le directeur
 % insiste...
%%\repdoyen{Nom du représentant du doyen} %(thèse seulement)

%%
%% Fin des variables à définir. La commande \maketitle créera votre
%% page titre.

\maketitle

 % Pour générer la deuxième page titre, il faut appeler à nouveau \maketitle
 % Cette page est obligatoire.
\maketitle

%%------------------------------------------------- %
%%              pages iii                           %
%%------------------------------------------------- %

\francais

\chapter*{Résumé}

Ce mémoire présente un ensemble d'œuvres pour l'orgue à tuyaux explorant la tradition émergente des instruments augmentés, telle qu'elle a été initiée par Tod Machover et d'autres. 
L'orgue est examiné en tant qu'instrument augmenté par une communauté de niche comprenant Lauren Redhead et le projet Orgelpark, tout en représentant une ressource sous-exploitée d'exploration musicale, positionnée de manière unique en tant qu'artefact culturel incarné. 
La nature d'un instrument intégré dans son espace pose la question de savoir s'il y a vraiment une distinction entre le bâtiment qui abrite l'instrument et l'instrument lui-même, ce qui donne une situation dans laquelle l'augmentation de l'orgue à tuyaux peut être considérée comme une architecture sonore. 
L'instrument examiné dans ce travail est l'orgue symphonique de l'église Saint-Édouard où l'auteur est organiste. 
Construit en 1913 par Casavant Frères, cet orgue a une histoire unique et mouvementée. 
Démonté de son emplacement d'origine dans la tribune ouest, oublié et presque abandonné, il a été retrouvé par la nouvelle administration près de dix ans plus tard et réinstallé dans le transept nord où il se trouve aujourd'hui. 
Je crois que le son de cet instrument raconte l'histoire de son patrimoine et qu'en étudiant ses propriétés uniques, nous comprenons mieux les nuances de l'histoire. 

Ce projet comporte un volet expérimental et un volet créatif. 
Le volet expérimental comprend la création d'un module de synthèse sonore écrit en python, appelé OrganLab, et d'un réseau de diffusion, avec cinq haut-parleurs placés dans l'église, afin d'explorer l'espace en profondeur. 
La partie créative comprend la pièce Élégies, inspirée des 10 Élégies de Duino, qui sert d'ensemble d'études pour explorer les capacités de l'hyper-organe, selon trois modalités de musique mixte : orgue acoustique et orgue synthétique, orgue avec traitement, et orgue avec piste de lit. 
L'espace de l'église est également considéré dans sa totalité, invoquant une structure spatiale-narrative. 

\textbf{Mots-clés :} Hyper-instrument, orgue à tuyaux, design sonore, composition, synthèse audio, architecture sonore.

%%------------------------------------------------- %
%%              pages iv                            %
%%------------------------------------------------- %

\anglais

\chapter*{Abstract}

This paper presents a body of work for the pipe organ exploring the emerging tradition of augmented instruments as pioneered by Tod Machover and others. 
The organ is being examined as an augmented instrument by a niche community including Lauren Redhead and the Orgelpark project, yet represents an underexploited resource of musical exploration, uniquely positioned as an embodied cultural artifact. 
The nature of an instrument that is embedded in its space poses the question of whether there is truly a distinction between the building housing the instrument and the instrument itself, yielding a situation in which augmenting the pipe organ can be seen akin to sonic architecture. 
The instrument examined in this work is the symphonic organ of l'église Saint-Édouard where the author is organist. 
Built in 1913 by Casavant Frères, this organ has a unique and eventful history. 
Being taken down from its original position in the west gallery, forgotten about and nearly abandonned, it was found by new administration nearly ten years later and reinstalled in the north transept where it sits today. 
I believe that the sound of this instrument tells the story of its heritage, and by studying its unique properties, we gain insight into the nuances of history. 

This project contains both an experimental and a creative component. 
The experimental component includes the creation of a sound synthesis module written in python called OrganLab, and a diffusion network, with five speakers placed throughout the church, in order to thoroughly explore the space. 
The creative portion involved the piece Élégies, inspired by the 10 Élégies de Duino, which serves as a set of studies to explore the capabilities of the hyper-organ, according to three modalities mixed music: acoustic organ and synthetic organ, organ with processing, and organ with bed track. 
The church space is also considered in its entirety, invoking a spatial-narrative structure. 

\textbf{Keywords:} Hyper-instrument, pipe organ, sound design, composition, audio synthesis, sonic architecture, hauntology

%%------------------------------------------------- %

%%        page v --- Table de matieres              %
%%------------------------------------------------- %

 % Pour un mémoire en anglais, changer pour
 % \anglais. Noter qu'il faut une permission
 % pour écrire son mémoire en anglais.
\anglais
%%\francais
 % \cleardoublepage termine la page actuel et force TeX
 % a poussé les éléments flottant (fig., tables, etc.) sur
 % la page (normalement TeX les garde en suspend jusqu'à ce
 % qu'il trouve un endroit approprié). Avec l'option <twoside>,
 % la commande s'assure que la prochaine page de texte est sur
 % le recto, pour l'impression. On l'utilise ici
 % pour que TeX sache que la table des matières etc. soit
 % sur la page qui suit.
%% TABLE DES MATIÈRES
\cleardoublepage
\pdfbookmark[chapter]{\contentsname}{toc}  % Crée un bouton sur
                                           % la bar de navigation
\anglais
\tableofcontents
 % LISTE DES TABLES
\cleardoublepage
\anglais
\english
\phantomsection  % Crée une section invisible (utile pour les hyperliens)
\listoftables
 % LISTE DES FIGURES
\cleardoublepage
\phantomsection
%% Il est obligatoire, selon les directives de la FESP,
%% pour une thèse ou un mémoire d'avoir une liste des sigles et
%% des abréviations.  Si vous considérez que de telles listes ne seraient pas
%% pertinentes (si, par exemple, vous n'utilisez aucun sigle ou abré.), son
%% inclusion ou omission est laissé à votre discrétion.  En cas de doute,
%% parlez-en à votre directeur de recherche, le coadministrateur ou au/à la
%% bibliothécaire.
%%
%% Le gabarit inclut un exemple d'une liste « fait à la main ».  Il existe des outils
%% plus sophistiqués si vous devez inclure une multitude de sigles et abréviations.
%% Par exemple, le package <glossaries> peut faire des index élaborés.  Comme
%% son utilisation est technique, il n'y a pas d'exemple directement dans ce gabarit.
%% On invite les gens qui aurait à l'utiliser à lire la documentation officielle,
%% soit en allant sur https://www.ctan.org/, soit en tapant dans un terminal :
%%
%% texdoc glossaries
%%

\chapter*{List of acronyms and abbreviations}
 % Option de colonnes: definir \colun ou \coldeux
%%% Exemple
%%% \def\colun{\bf} % Première colonne en gras
%%% Pour numéroté les entrées, on peut faire
%%% \newcount\abbrlist
%%% \abbrlist=0
%%% \def\plusun{\global\advance\abbrlist by 1\relax}
%%% \def\colun{\plusun\the\abbrlist. }
%%\def\coldeux{\relax}
\begin{twocolumnlist}{.2\textwidth}{.7\textwidth}
\end{twocolumnlist}
%% L'environnement <threecolumnlist> existe aussi pour trois colonnes.

%%------------------------------------------------- %
%%              pages vi                            %
%%------------------------------------------------- %

\chapter*{Acknowledgements}

...remerciements...

 %
 % Fin des pages liminaires.  À partir d'ici, les
 % premières pages des chapitres ne doivent pas
 % être numérotées
 %

\NoChapterPageNumber
\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                  %
%%   TEXTE DU MÉMOIRE :  introduction page 1,...    %
%%                                                  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter*{Introduction}

This text details a research-creation project centered on the unique problem space of pipe organ augmentation. 
When I was considering a subject for a master's project in early 2022, several theme's resurfaced prominently in my mind. 
On the one hand, I had begun playing the pipe organ shortly before the outbreak of the SARS‑CoV‑2 pandemic of 2019, and was deeply touched by this experience. 
On the other hand, I was in the process of finishing up a diploma program in digital music at l'Université de Montréal, where I had come in to contact with the concept of the hyper-instrumet. 
This began a slow process of considering whether these two paradigms could be merged. 
This would satisfy my desire to expand my technical skills, through working with various digital technologies, as well as giving me a good excuse to delve more deeply into the pipe organ.

At the beginning of the project, my simple tagline and working title was, "Revisiting the original synthesizer : the pipe organ as an augmented instrument". 
Indeed, the pipe organ can be thought in a sense to be the first instrument in the spirit of the additive synthesizers of the twentieth century. 
Though using complex sounds, rather than simple waveforms, it nevertheless operates through the combination of a set of sonic primitives, which are combined in nearly endless ways to create astonishing timbral variety. 
This project attempts to leverage this rich tradition while extending it through digital synthesis, bypassing the physical constraints of a monolithic physical instrument and lending a certain malleability to the sound profile of the instrument.

This was more or less the jumping off point for the project, but through the course of its development, and especially through the composition of a piece called Élégies, it became clear that I wanted not only to address the pipe organ as an insrument, but also to emphasize the space itself, leading to an extended practice in which I diffuse sound throughout the church, while referencing the sounds of bells, the fire alarm of the church, and other sonic iconography. 

The following text is organized in four main parts. 
The first chapter defines the concept of hyper-instruments broadly, while describing significant contributions to the field, starting with the pioneering work of Tod Machover. 
This chapter lays the groundwork by examining the evolution and characteristics of hyper-instruments, while touching on some innovative ongoing approaches to treating the pipe organ as an augmented instrument.

The second chapter details my personal approach to constructing an augmented interface for the pipe organ at l’église Saint-Édouard, where I've been organist since July of 2022. 
It delves into my aesthetic priorities, practical limitations, and the strategies employed to overcome these challenges. 
This chapter provides a detailed account of the technical and conceptual processes involved in creating a functional and expressive hyper-organ interface.

The third chapter presents my creative approach, detailing my compositional philosophy and the various tools I used throughout the project. 
These tools include GrandOrgue, OrganLab, OpenMusic, and Ableton Live. 
This chapter offers insights into the creative processes and decisions that shaped the development of the project, highlighting the integration of these tools into my work.

The final chapter discusses the composition "Élégies" for solo hyper-organ and voice. 
It explores the thematic and symbolic inspirations drawn from Rainer-Maria Rilke's 10 Duino Elegies and examines how the work navigates the continuum between acoustic and synthetic soundspaces. 
The chapter also discusses the exploration of the symbolic space of the church through references to its sonic iconography, providing a comprehensive analysis of the piece and its place within the broader context of the project.

Finally, the fourth chapter discusses the composition of the piece Élégies, written for my hyper-organ interface at l’église Saint-Édouard. 
Based on the 10 Duino Élégies of Maria-Rainer Rilke, the piece incorporates the aural iconography of the space, making use of the sounds of bells, the fire alarm of the church, and the sounds of footsteps through its many corridors. 
This symbolic and spatial exploration mimics the aural exploration that seeks to navigate the continuum of acoustic and simulated--the sacred and the profane.

%%------------------------------------------------- %
%%                pages 1                           %
%%------------------------------------------------- %

\chapter{The Emergence and Relevance of Hyper-Instruments}

Hyper, or augmented instruments, situated at the confluence of traditional musical artistry and cutting-edge technological innovation, present a complex and diverse field of inquiry.
This chapter seeks to delve into this multifaceted world, highlighting the significance of hyper-instrument design and performance in contemporary music practices and the intricate process of developing and defining them within various musical contexts.

At the core of any musical instrument lies the interface between an artist's musical expression and the resultant sound in their environment.
Augmenting an instrument, therefore, becomes an exercise in expanding this interface.
This task is often far from straightforward, as musicians are typically engaged fully, utilizing their hands, mouth, and sometimes feet, to produce sound.
In such scenarios, where every faculty of mind and body is already employed, the question arises: where is there room for expansion?
Among the various approaches in interface design, are gestural mapping, sensitivity mapping through pressure sensors, and the integration of MIDI interfaces.
Each of these approaches has its unique challenges and opportunities, tailored to the specificities of the instrument being augmented.

The choice between convergence or divergence with the natural workings of the instrument is crucial.
For instance, augmenting a wind instrument involves navigating its intimate relationship with the performer's breathing and the air column.
Here, the lateral movements of the performer, which do not inherently affect pitch, provide an opportunity for introducing an independent dimension of expressivity and virtuosity.
Conversely, in an instrument like the piano, where pitch is intrinsically tied to the horizontal plane, any augmentation must consider this inherent constraint.
While this may limit the scope for independent controls, it ensures that any technological integration remains deeply rooted in the natural mechanics and traditions of piano playing.

This chapter aims to navigate the extensive landscape of hyper-instruments, examining methodologies and concepts that are shaping this evolving domain.
From the pioneering work of Tod Machover to adaptations in instruments like the hyper-cello, hyper-shakuhachi, hyper-piano, and hyper-flûte, we will explore how musicians and technologists are redefining musical expression.
Through this exploration, we seek to uncover the broader implications for music as a dynamic and evolving art form in the digital age.

\section{What is a hyper-instrument?}

\subsection{What is an instrument?}

A musical instrument is an interface \footcite{noauthor_instrument_nodate}. 
It allows one to express thought in audible form. 
Among the first instruments were undoubtedly the voice. 
The human voice allows us to express ourselves in a near infinite variety--from the most practical, logic oriented thoughts, to the most abstract of indefinite feelings. 
The voice provides an interface between their internal, subjective world, to the external. 

Yet a musical instrument is also a technology, which like any other, aids us in extending beyond our natural human capacities, yielding access to insights and abilities that would not otherwise be possible. 
A scientific instrument like the microscope allows us to magnify our visual capacity manifold, in order to see things that would be much too small for the naked eye. 
Similarily, a musical instrument can allow us to do things that would not be possible with the human voice. 
For instance, with a small flute, a person with a very low voice can sound very high frequencies, and vice versa. 
With a bowed instrument like the violin, one can sound the note indefinitely, bypassing the voices necessity to take breaths. 
With a lute, one can sound more than one note at a time. 

The results of a musical instrument are inherently subjective, and the goals of designing an expressive interface that is considered "good" are a reflection of the culture, the time, and the context of the people creating it. 
For instance, one culture may optimize for resonance and slow decay, whereas another may want shorter, more percussive sounds. 
One people may desire maximum expressivity through timbral variety, like with the human voice, while another may see timbral variability as a defect, and seek timbral consistency throughout the range of the instrument. 

The goals of innovation in musical interfaces are not uniform, and with an advance in one area of expressivity, another is lessened or lost. 
This process of aesthetic prioritization makes instrument design inherently personal. It nevertheless represents a process of innovation, albeit non-linear. 

In the twentieth and twenty-first centuries, with the advent of electronic and then digital technologies, people around the world have been reimagining the musical instrument from various perspectives. 
On the one hand, practices like digital lutherie generally seek to create interfaces centered in digital technologies, using various sensors, and computers to input and process data for expressive means. 
These kinds of approaches allow one to rethink what a musical instrument can be, without some of the physical restrictions and cultural baggage that come with a traditional instrument with a long history. 
These devices range from digital interfaces that mimick instrumental approaches, to gestural interfaces that generate sound simply by moving the instrument through the air.

The practice of augmented, or hyper-instruments takes a different approach. 
Rather than starting from scratch, the hyper-instrument seeks to integrate digital technologies with more traditional acoustic instruments. 
This approach has several advantages and obstacles. 
On the one hand, there is something to be said for longevity.
If an instrument has endured the test of time, and come to be used for generations upon generations, that demonstrates a creative and expressive capacity that is simply established by time.

On the other hand, one may easily pose the question: why ruin a good thing?
If the instrument already works, does what it needs to do, and has an established sound that we all love, what is there to add?
Of course, there is nothing wrong with the original instruments, and explorations of augmented instruments typically celebrate a multiplicity of approaches rather than holding the goal of a general improvement.
However, one can also make the argument that these instruments, so loved throughout time, deserve to be revisited with a modern perspective, both in terms of technology that simply wasn't available in the original periods of the instruments' evolutions, and in terms of the ideas and conceptions that the original builders and listeners didn't have access to. Furthermore, this spirit of exploration is the very same process that led to the creation of these instruments in the first place.

Pursuing this expansion comes with both great possibilities, and a certain burden of responsibility.
The historic traditions of these instruments give them a weight, not just in terms of the playing traditions, and the mechanisms involved in their interfaces, but also in the many, often unconscious, symbolic associations that we have formed with these instruments.
This cultural baggage is not something to be overcome however, but is a great strength.
If one is careful to take into consideration the nuances of the original interface, integrating them with their ideas in a way that complements, rather than fights the natural tendencies of the instrument, the result can be an enduring hommage, and a continuation of a tradition that crosses both culture and time.

\subsection{Commonalities}

Hyper-instruments are extremely diverse and non-standardized, with an enormous range of possibilities, each instrument designer, and each instrument containing novel approaches. 
At the same time each hyper-instrument will have a certain framework in common. 
They will always use an acoustic instrument as the basis, which will then be integrated with digital technology of some sort. 
This technology can look vastly different depending on the context, but it will always have an input and an output. 
The input on a traditional musical instrument like a cello would be the detailed gestural information generated by pressing the fingers to the neck in a nuanced way while drawing the bow at a given angle and velocity. 
This information includes not only precisely where the finger is placed on the neck, but how it is placed (whether it is rotating or rocking etc.) This three-dimensional gestural data yields an abundant complexity of data on the input side, which we can also call the interface. 
On the output side is how this data is treated. 
In the case of the cello, the gesture is carefully crafted to produce a certain resonance of the soundboard, which ultimately transmits audible sound to the listener. 
In a hyper-instrument interface, we have the same concept, yet with a plethora of options for both the interface and the treatment of data. 
For the interface, the most commonly used option are gestural trackers. 
Taking the same analogy as the cello, gestural tracking is an intuitive solution, as it simply extends the paradigm of converting gesture into sound, taking in even more data, which can then be used in various ways. 
There are, however, other approaches, such as air pressure sensitivity sensors, touch pressure sensitivity sensors, midi controllers, or even simply microphones. 
As far as treatment, the most common approaches are live effects like distorsion, reverb, or any way in which the audio signal is processed, synthesis, where the synthesized sound is directed by the input data, but may or may not bear a resemblance to the instrumental sound, or triggering of sound files, again, where the sound files in question can bear or not a resemblance to the instrument, and can range from sample level grains, to long recordings.recordings.

The choice of approach, or mix of approaches, will depend both on the intentions of the person performing the inquiry, and on the instrument iself. The historical context of performing traditions of the instrument in question will invariably inform the interface decisions, and the aural historical context will generally inform the method of treatment, either striving for timbral similarity, or juxtaposition, or both.

\subsection{Convergent and divergent mappings}

In the previous section, we discussed the input and output components of hyper-instrument design, yet this says nothing of the relationship between the two.
This is what is broadly referred to as mapping, in which a certain form of data or grouping of data is made to correspond with a given audio event, including layers of multiple events or a sequence of events. 

Andy Hunt and Ross Kirk provide a detailed analysis of mappings in musical interface design in their article textit{Mapping Strategies for Musical Performance} \footcite{hunt_mapping_2000}.
Cléo-Palacio Quintin, whose work on hyper-flute will be discussed later in the chapter, points out that there is an important, distinction between what she calls convergent and divergent mappings, where a convergent mapping implies that several data inputs affect the same output, whereas a divergent mapping means that one input parameter affects multiple outputs \footcite[44--45]{palacio-quintin_composition_2012-1}. 

She then goes on to make the argument that while divergent mappings are often time more intuitive for a new instrument designer, that ultimately convergent mappings are more intuitive for the player, leading to a more holistic approach. 

\customincludegraphics[scale=1]{mapping.png}{The violin is used to demonstrate the important distincion between convergent and divergent mappings. Adapted from Mapping Strategies for Musical Performance Book Title (p. 234), by A. Hunt and R. Kirk, 2000, Trends in gestural control of music. Copyright 2000 by Ircam - Centre Pompidou. Adapted with permission.}

The idea that convergent mappings are more intuitive and easily incorporated makes sense. 
They essentially don't ask anything new of the performer, except maybe a new aural sensibility. 
Physically, they rest nicely in the established paradigm of gestural repertoire, essentially saying to the performer "do what you normally do, and I'll work around it." A divergent mapping however, requires some kind of relearning. 
With a convergent mapping, we are augmenting a part of the interface that is already integral to the functioning of the instrument, an example of which would be a gestural mapping to the horizontal, left/right plane of a piano player, who already needs to use this space to navigate pitch space.
This ensures that the mapping is consistent with the innate gestures of playing the instrument, but does not allow for independant control of these parameters.
With the flute, on the other hand, this exact same mapping could be used more or less independently, as the horizontal left/right plane is not directly coupled with any musical parameter.

To summarize, by my thinking, there are several advantages and disadvantages to each mapping approach:

\textbf{Convergent mappings:}

\begin{itemize}
  \item Advantages:
  \begin{itemize}
    \item More integrated with traditional technique and thus immediately accessible to a performer.
    \item By coupling with an already used interface parameter, the added expressivity could be said to be aligned with the normal modes of expressivity.
  \end{itemize}
  \item Disadvantages:
  \begin{itemize}
    \item Performer has no independent control and might feel restrained (e.g., I'd like to do "x" to achieve "y", but I can't without also producing "z", or vice versa).
  \end{itemize}
\end{itemize}

\textbf{Divergent mappings:}

\begin{itemize}
  \item Advantages:
  \begin{itemize}
    \item Independent control of parameters.
    \item Allows for new forms of virtuosity.
  \end{itemize}
  \item Disadvantages:
  \begin{itemize}
    \item Requires some relearning, which can be especially daunting or undesired by performers who have already put a significant investment into their current technique.
  \end{itemize}
\end{itemize}

\section{History and approaches}

\subsection{Tod Machover and the birth of the hyper-instrument}

The term hyper-instrument comes to us from Tod Machover, a leading figure at MIT's Media Lab, who has been carrying out pioneering research in hyper-instruments since the 1980s. 
His work aims to redefine the relationship between the performer and their instrument by merging human expressivity with machine precision. 
Rather than simply enhancing an instrument's capabilities, Machover's hyper-instruments explore new ways for performers to interact with their instruments, including feedback mechanisms where the machine can also make decisions that influence the performer.

The initial development of hyper-instruments involved integrating electronic keyboards (such as the Yamaha KX88 and Kurzweil Midiboard) and percussion controllers (including the Silicon Mallet, Octapad, and KAT) with a computer system \footcite{machover_hyper-instruments_1989}. 
MIDI data from these instruments was sent to a Macintosh II computer running custom software called Hyperlisp. 
This real-time MIDI/Lisp environment was specifically developed to analyze and process the input data, which was then sent to digital synthesizers, samplers, and outboard processing devices to generate the final musical output.

One of the most significant applications of hyper-instrument technology was the development of the hypercello, designed for the renowned cellist Yo-Yo Ma \footcite{levenson_taming_1994}. 
This instrument incorporated sensors to measure various parameters such as bow position, pressure, and finger placement. 
The data collected was processed by a computer to control and modify the sound in real-time, allowing the cellist to interact with the instrument in a highly nuanced and expressive manner.

The hypercello was prominently featured in Machover's composition "Begin Again Again...," performed by Yo-Yo Ma at the Concertgebouw in Amsterdam. 
This piece exemplified the potential of hyper-instruments to create complex, multi-layered musical experiences. 
The performance involved not only traditional cello playing but also interaction with a pre-programmed score, enabling Ma to control and shape the music dynamically. 
The integration of synthesized sounds and real-time digital processing created a unique blend of acoustic and electronic music.

The development of hyper-instruments involved significant technical challenges, particularly in managing the complex flow of data and ensuring accurate real-time processing. 
The hypercello, for instance, required precise synchronization between the performer's gestures and the computer-generated responses. 
This necessitated robust hardware and software capable of handling large amounts of data with high accuracy.

Not only does Machover seek to extend an instrument's expressive capacity, but cenral to his vision of the hyper-instrument is the redefinition of the relationship of the performer to their instrument \footcite{hoffman_q_2010}. 
For instance, his "double instruments" combine the gestures of two performers playing separate controllers, creating a new form of ensemble performance where musical results are produced collaboratively.

\subsection{Beilharz' hyper-shakuhachi}

The hyper-shaku, developed by Kirsty Beilharz, is an augmented version of the traditional Japanese bamboo shakuhachi flute. 
This project integrates motion sensors and computer vision to create an interactive performance environment that enhances both auditory and visual elements \footcite{beilharz_hyper-shaku_2006}. 

The hyper-shaku project builds on previous works in intelligent sensor environments and gesture-controlled interactive systems. 
The primary objective is to create a system where the performer's movements trigger both immediate auditory responses and generative visual processes. 
This dual-mode interaction distinguishes the hyper-shaku from other hyper-instruments, focusing on both the acoustic performance and the visual representation.

The system uses a combination of wireless sensors and computer vision to capture the performer's gestures. 
These inputs are processed in real-time using Max/MSP and Jitter software, which generates corresponding sound and visual displays. 
The motion data triggers various synthesis processes and generative design elements, creating a dynamic and responsive performance environment.

Gesture data is central to the hyper-shaku's functionality. 
The performer's movements, such as pitch inflections achieved by angling the chin and dramatic articulations, are captured and translated into both sound and visual outputs. 
This approach emphasizes the theatrical and spatial aspects of shakuhachi performance, enhancing the traditional sound with electronic augmentation.

The hyper-shaku can be used for both composed and improvisatory performances. 
It aims to re-invigorate interest in the shakuhachi by integrating it into a contemporary multimedia context. 
This hybridization of traditional and modern elements provides new opportunities for creative expression and audience engagement.

Kirsty Beilharz's hyper-shaku represents a significant advancement in the field of hyper-instruments. 
By combining auditory and visual augmentation, this project not only expands the capabilities of the shakuhachi but also explores new dimensions of interactive and generative performance. 
The hyper-shaku exemplifies how traditional instruments can be transformed and recontextualized through the use of modern technology.

\subsection{McPhersons hyper-piano}

The Magnetic Resonator Piano (MRP) is designed to augment the acoustic piano, allowing continuous control over pitch, dynamics, and timbre. 
The instrument employs 88 electromagnetic actuators to vibrate the strings independently of the traditional hammer mechanism, producing a wide range of sounds while preserving the acoustic richness of the piano. 
This system is controlled through an optical sensor strip on the keyboard, which measures the continuous position of each key, enabling nuanced performance techniques such as gradual key presses, taps, and vibrato gestures.

A common issue in DMI design is ensuring the instrument's relevance beyond its initial performance \footcite{mcpherson_problem_2012}. 
Many new instruments fail to attract a significant following, often because they are tailored to specific pieces or performers. 
The authors emphasize the importance of making the instrument useful to a broader community, allowing musicians to explore its capabilities and develop personal styles.

McPherson and Kim discuss how feedback from initial users can guide design refinements. 
For the MRP, they identified key areas for improvement based on musician input, including enhancing dynamic range, reducing attack time, enriching timbre, and simplifying control mappings. 
They also aimed to expand the instrument's capabilities without compromising its playability for traditional pianists.

To build a community around the MRP, the authors collaborated with six composers from Philadelphia and Princeton, each writing new pieces for the instrument. 
This project culminated in two concert performances, showcasing a diverse range of styles and techniques. 
The composers' engagement with the MRP provided valuable insights into its potential and limitations, driving further refinements.

Iterative Design: Initially link the instrument and piece, then iterate based on feedback to relax constraints and broaden its appeal.
Familiar Models: Connect new techniques and sounds to musicians' existing skills to facilitate adoption.
Audience Engagement: Convincing performances can attract potential collaborators from the audience.
Accessibility: Provide access to the instrument and designer support during the creation process to encourage ongoing use and development.

\subsection{Cléo Palacio-Quintin's hyper-flûte}

The hyper-flute, as developed by Palacio-Quintin, involves attaching various sensors to a standard flute. 
These sensors capture a range of data, including finger positions, breath pressure, and even specific gestures made by the performer. 
The data collected is then transmitted to a computer, where it can be processed in real-time to manipulate electronic sounds or control other digital effects. 
This setup allows the flutist to interact with electronic components seamlessly, creating a rich, hybrid soundscape.

One of the primary challenges addressed by Palacio-Quintin is the synchronization between the acoustic instrument and electronic components \footcite{palacio-quintin_composition_2012-1}. 
Traditional mixed music often required performers to follow rigid, pre-determined electronic tracks, which limited expressive freedom. 
By contrast, the hyper-flute allows for real-time interaction, where the performer's gestures and playing directly influence the electronic output. 
This dynamic interaction is achieved through programming and real-time signal processing, primarily using Max/MSP.

This choice also explains her lack of focus on sound spatialization, a popular domain among electroacoustic composers. 
The presence of an on-stage performer who projects an acoustic sound naturally delimits a sound space. 
To integrate the electroacoustic sound with the acoustic, her works are designed in stereo to be projected mainly by front-positioned speakers near the performer. 
The primary sound source remains the flute, and the electroacoustic sounds should emanate from it, justifying their projection near the source.

A significant focus of Palacio-Quintin's research is on gesture control, which she argues is crucial for creating a natural and intuitive interface between the performer and the electronic components. 
By utilizing gesture data, the hyper-flute can respond to the nuances of the performer's movements, allowing for a more fluid and expressive performance. 
This approach not only enhances the musical capabilities of the flute but also redefines the role of the performer, who becomes an active participant in shaping the electronic soundscape.

In terms of interaction, Palacio-Quintin’s approach aligns with Chadabe's vision, which advises allowing both the performer and the computer a degree of freedom to maintain a lively interactive space. 
The modes of interaction can vary; the music is not entirely fixed, and the computerized system remains flexible, giving the performer some freedom in their play. 
This aligns with the interest in improvised music, where each musician can be surprised by their colleagues' new musical ideas during the performance. 
George Lewis’s definition of improvisation in his article "Interacting with Latter-day Musical Automata" aptly fits this type of human-machine interaction: "Musical improvisation is (...) an interaction within a multi-dimensional environment, where structure and meaning arise from the analysis, generation, manipulation, and transformation of sonic symbols" [35, p.101]. 
Indeed, the hyper-flute’s electronic setup is a multidimensional environment, capable of analyzing, generating, manipulating, and transforming sound materials. 
This interactive system changes traditional subdivisions of musical practice, merging composer, score, work, performer, performance, instrument, and environment into one process.

According to Palacio-Quintin, in the context of hyper-flute, there are three main types of musical interaction between acoustic and electroacoustic components \footcite[52]{palacio-quintin_composition_2012-1}:

\begin{enumerate}
  \item \textbf{Digital Audio Effects on Acoustic Sound:} The acoustic instrument's sound is transformed in real-time by digital signal processing. 
The computer serves as a direct extension of the instrument's sound, with the potential for gestural control over the effects to maintain real-time interactivity.
  
  \item \textbf{Sound Synthesis Controlled by Gestures:} The computer generates sound independently, controlled by the performer's gestures, either captured directly or through sound analysis. 
This synthesis can diverge significantly from the acoustic sound.
  
  \item \textbf{Accompanying Electroacoustic Sounds:} Pre-recorded or computer-generated sounds accompany the performer, independent of the performer's actions once initiated. 
This can include fixed sounds or algorithmically generated sounds, offering interaction through the performer's reaction to the evolving sound environment.
\end{enumerate}

Prior to her doctorate, Palacio-Quintin primarily used the hyper-flute in improvised music contexts, focusing on the real-time transformation of the flute’s sound. 
During her doctoral studies, she aimed to explore forms of composition to deepen the use of other interaction categories.

\section{Hyper-organ}

When speaking of the instrument as a technology, perhaps no instrument is so emblematic of this tradition of continuous innovation than the pipe organ. 
Throughout it's millenia long history, it has continually been reimagined, from the one keyboard hydraulis of 300BC Greece, to the addition of multiple independantly controlled stops and keyboards, the extensive exploration of timbre, not just through the creation of new stops, but through the capacity to combine different stops, essentially bringing the role of instrument design into the hands of the performer. 
Then, throughout the 19th and 20th centuries, the pipe organ embraced pneumatic, then electro-pneumatic, and more recently, digital mechanisms of communication. 
The pipe organ has always played a role in challenging the threshold of the possible, on the one hand, looking back towards a rich history, and on the other hand, embracing and integrating technological innovations as they arise. 
At the same time, the pipe organ is unique in its diversity. 
Being an inherently modular instrument, one pipe organ can look very different from another. 
From one to the seven keyboards of the Boardwalk hall organ, with or without pedal-board, with or without enclosures, with an enormous variety of stops, and many more. 

In terms of the selection of stops, even a stop with the same name, such as Flute 8', can vary widely from one instrument to another based on the era of it's construction, the aesthetic goals of the builder and of the maintainers of the instrument. 
This means that each instrument tells the story of its unique heritage, and serves as a time capsule of shared historical and physical space.

In the context of augmented pipe organs, these considerations manifest in unique and innovative ways.
Projects led by visionaries like Lauren Redhead and the Orgelpark initiative are exemplary in demonstrating how technology can reimagine the traditional pipe organ.
These initiatives not only showcase the potential of technological integration with historic instruments but also highlight the diverse methodologies and creative opportunities within the field of hyper-instruments.

\subsection{Lauren Redhead}

Lauren Redhead doesn't explicitly use the term hyper-organ, or augmented instrument, instead choosing to align her work with the mixed music tradition of electronics and live performer. This is especially pertinent as Alistair Zaldua conceives of and controls the electronics in concert, strengthening the perspective of a duet, rather than a unified instrumental expression. Despite this, her pursuit of improvisation with a high degree of freedom, open forms, and interactivity is in line with the practices of Machover and Palatio-Quentin.

Redhead's approach emphasizes the organ's potential as an interface for electronic music. By utilizing the organ’s MIDI capabilities and incorporating live sound processing, she expands the instrument’s expressive range. This integration enables the organ to control electronic sounds and vice versa, allowing for a dynamic and interactive performance environment.

Lauren Redhead’s work often involves the use of open notation, graphic scores, and real-time generated notations, which challenge traditional notions of musical performance. Her compositions require performers to engage with the scores in a fluid and interpretative manner, often involving real-time decision-making and interaction with electronic elements.

In her paper "Sound and Space: Performing Music for Organ and Electronics," Redhead discusses the collaborative nature of her work with composers and performers. This collaboration is crucial in developing pieces that fully exploit the interaction between the organ and electronics. The performance space itself becomes an integral part of the instrument, influencing the sound and how it is perceived by the audience. This site-specific approach ensures that each performance is unique, tailored to the acoustics and spatial characteristics of the venue.

Redhead’s pursuit of improvisation with a high degree of freedom, open forms, and interactivity aligns her with the practices of pioneers like Tod Machover and Sophie Palazio-Quentin. These practices emphasize the importance of performer agency and real-time interaction, creating a dynamic and responsive musical experience.

One of Redhead’s significant projects is her work on the "Sound and Music Tour," where she performed a series of concerts across the UK, showcasing new works for organ and electronics. This tour highlighted the versatility of the organ when combined with live electronics and featured compositions that explored various aspects of this interaction.

The project "Diapason: Music for Organ and Electronics," released as a CD, documents some of these performances and provides a snapshot of the evolving relationship between the organ and electronic music. The recordings on this CD reflect the diverse sonic possibilities achieved through this combination, with pieces by various composers who have collaborated with Redhead.

Lauren Redhead’s contributions extend beyond performance and composition to include significant theoretical insights. Her work often references Nicholas Bourriaud’s concept of the 'journey form,' which she uses to articulate the relationship between creative, interpretative, and listening processes in music. This theoretical framework underpins her exploration of open notation and the iterative nature of musical interpretation.

In her book chapter "Notation as Process: Interpreting Open Scores and the ‘Journey Form’," Redhead delves into the performative and interpretative challenges posed by open notation. She argues that the interpretation of such scores is an ongoing process involving the composer, performer, and the score itself. This perspective shifts the focus from a finished musical product to the continuous evolution of the work through performance.

\subsection{Orgelpark}

The Orgelpark in Amsterdam has undertaken a remarkable project that merges historical authenticity with contemporary innovation in the field of organ music. 
Central to this endeavor is the creation of a hyper-organ, which serves as both a historically accurate replica of a Baroque-period Zacharias Hildebrandt organ and a modern instrument capable of engaging with contemporary music practices. 
Unlike other projects that retrofit digital technologies to existing organs, the Orgelpark had the unique privilege of conceiving this dual-purpose instrument from its inception. 
This approach has allowed for a seamless integration of historical and modern elements, setting a new standard in organ design and performance.

The primary goal of the hyper-organ at Orgelpark is to achieve historical accuracy while providing a platform for modern musical expression. 
The instrument replicates the sound and mechanics of a Hildebrandt organ, renowned for its clarity and brilliance. 
Simultaneously, it incorporates advanced digital technologies that extend its capabilities far beyond those of a traditional organ. 
This dual functionality is made possible by a sophisticated interface that allows for the dissociation of pipe and key, enabling any pipe to be assigned to any key independently of the stops. 
This feature is complemented by touch sensitivity, where varying depths of touch can trigger different combinations of pipes and air pressure, and variable tremulants that add nuanced expressivity to the music\footcite{fidom_digital_2014}.

The technological innovations of the hyper-organ are achieved through the use of spring chests and electromagnets. 
Spring chests, a Baroque technology, allow for independent control of individual pipes, while electromagnets enable key expressivity, translating the organist's touch into subtle variations in sound. 
These features are controlled through both a traditional mechanical keyboard, which facilitates conventional playing techniques, and a hyper-organ interface. 
This dual-console system allows organists to explore the full range of the instrument's capabilities, seamlessly blending historical and modern performance practices\footcite{peters_how_2014}.

The hyper-organ interface is designed to control not only the new Baroque organ but also the Romantic-period Sauer organ housed at the Orgelpark. 
This integration enables the interfacing of disparate periods of sonic aesthetics, providing musicians with an unparalleled level of flexibility and creativity. By bridging the gap between different historical periods and incorporating cutting-edge technology, the Orgelpark hyper-organ offers a unique platform for exploring new soundscapes and performance techniques \footcite{van_heumen_new_2014}.

The Orgelpark's approach to organ design and performance exemplifies a forward-thinking vision that respects historical traditions while embracing contemporary innovations. 
The hyper-organ project demonstrates how digital enhancements can enrich the expressive potential of historical instruments, offering new possibilities for both performers and composers. 
By highlighting the interplay between historical authenticity and modern technology, the Orgelpark sets a precedent for future developments in the field of organ music.

\section{Concluding thoughts}

As we can see, augmented instrument design is far from a monolithic, and this inherent plurality is one of the most intriguing aspects of the field.
The term 'hyper-instrument' encompasses a broad spectrum of augmented musical tools, each with its unique characteristics and requirements.
Developing an intuitive interface for a hyper-instrument involves a deep understanding of the instrument's core essence and a thoughtful integration of technology that enhances without overshadowing its intrinsic qualities.
This diversity in approach underscores the difficulty in pinning down hyper-instruments as a singular practice.
Instead, they represent a constellation of practices, each illuminating different aspects of the intersection between modern and traditional technologies.

\chapter{Constructing an augmented interface for the pipe-organ}

In the last chapter, we discussed the general principles behind hyper-instruments, and examined several important examples. 
We also established that hyper-instrument design is highly personal, depending not only on the background of the designer, but on the instrument itself. 
In the current chapter, I'd like to discuss my own personal approach, from a composer-performer perspective. We'll take a look at my background with the pipe organ, the constraining factor, namely the pipe organ of l'église Saint-Édouard, where I've been organist since July of 2022, and then we'll highlight my aesthetic priorities while thinking through my approach to extending the expressive paradigm of this pipe organ. Finally we'll discuss my tri-modal approach, making use of synthesis, live effects, and triggering of pre-recorded audio files (bed tracks). 

\section{My background with the organ}

First, I’d like to provide an account of my background with the pipe organ.
In 2019, I travelled for several months in South America, beginning in Peru and moving up through Ecuador, ending in Colombia.
Upon returning from this trip, I moved into a house in Vancouver, agreeing to move in before visiting or even seeing photos of the house.
During my walk to the house on moving day, I was struck by the towering church, with nearly black stones, auspiciously stationed on the corner of this unsuspecting Kerrisdale intersection.
I then realized that my new home was directly beside this church.
After a couple weeks of living here, one day returning from a grocery trip, I heard music coming from the church and saw that the doors were open.
I peeked inside and saw not only that the choir was rehearsing, but also that behind them was a magnificent pipe organ.

I had always been interested in the pipe organ, starting from an obsession with several synthetic pipe organ sounds on my Roland RD300NX keyboard, especially what are called puff organ, and nason flute.
I used these sounds on my first album, Ad viger, on the track Cyclogram 1, originally written for two sopranos, but adapted for the organ.
Around this same time, in 2014, I approached Michael Murray, and he graciously invited me to the St.
Philips Anglican church, where I played the pipe organ for the first time.
He gave me a tour of the instrument, and had me stumble through reading some simple hymns in three staves, clumsily and comically attempting to manage the pedals for the first time.
At the time I felt I was too busy to take on the challenge of learning the pipe organ, and I was deterred by the inaccessibility of the instrument in comparison to an electronic or midi keyboard that I can fit in my bedroom.
The beauty and variety of the sounds of the instrument, as well as the delightful challenge of managing the pedal part with the other voices,  stayed in my mind in the following years.

Returning to 2019, I approached the music director of my neighbouring church, which I had by that time learned was the Pacific Spirit United Church, Bryn Nixon, and asked him if he would like any help with the music, mentioning that I had some keyboard skills, and that I’d be interested in trying the organ.
We scheduled a meeting, and he gave me a tour of the instrument, explaining the history of the instrument and its many stops, as well as giving me some pointers on what to focus on in terms of technique and repertoire.
At this point, I wasn’t sure if I would continue learning the organ or whether it would end with one or two sessions, but I was soon captivated by the instrument, and I received an email from Bryn telling me that the keys to the church were waiting for me in the office.
I bought some organ shoes, and I then spent many evenings quietly working through various physical and musical problems, slowly becoming more and more at ease at the console.
At the same time, Bryn employed me as a pianist, and as organist for simple pieces and accompaniments for the church choir.
This was a profoundly transformative experience for me, and gave me a regular outlet for both personal, and collaborative music making.

At this same time, I enrolled in part time studies at the University of British Columbia, in order to take two courses that I wasn’t able to fit into my Bachelors degree because of time table conflicts and course requirements: counterpoint and physics of music.
In order to receive part time student loans from StudentAidBC, they asked for proof of continuing education.
At the time I had no plans of continuing education, and the simplest option would have been to put UBC as my plan for graduate studies.
At the same time, being a curious person, I decided to do a bit of research on different institutions, and stumbled upon l'Université de Montréal.
I was immediately struck by what I felt was an openness and plurality of approaches and aesthetic that was coming out l'Université de Montréal music department, and soon enrolled as a student in their D.E.S.S.
one year diploma program in digital music.

I wanted to continue my studies of the pipe organ in Montréal, and I knew that the city of a thousand bell towers would have opportunities to continue this exploration, but it wasn’t until the summer after my D.E.S.S.
that I began to explore the many churches of Montréal, speaking with organists and priests in order to find people with which to collaborate with.
After several dead ends, I stumbled upon l’église Saint-Édouard, where the current organist Georges-Aimé, who was looking to retire, was all too thrilled to be relieved of his post.
I began as organist at this church in July of 2022, and began the master’s in music composition and sound design at l’Université de Montréal in the following September.

My proposed master’s project was based on treating the organ as an augmented instrument.
I had heard of augmented instruments since my time at UBC as part of what was then misleadingly named, the laptop orchestra.
This wasn’t a traditional orchestra, however, and included a cellist, a french horn player, a pianist, a trombone player, a singer, as well as several dancers.
This ensemble, which was later renamed interactive performance systems, focused extensively on gesture tracking through input from gametrackers, gyroscopes, and the microsoft Kinect 2, and consisted in a group of performers, and a group of coders.
I was in the coding group, creating interfaces for the performers.
This was a formative experience for me, but it wasn’t until the time during my D.E.S.S.
that I began considering the idea of crafting an interface for myself.
During my master’s program, I suddenly had access to l’église Saint-Édouard and the time to focus on a major project.

\section{Context: L'église Saint-Édouard}

One of the central issues of hyper-instrument design is the use of a pre-existing instrument as a starting point. 
The chosen instrument provides the limitations and challenge space for augmentation. 
In my case, the organ of l'église Saint-Édouard is a 1913 Casavant Frères work in the symphonic tradition. 
It has three manuals, with two expressive boxes (the récit and the positif divisions), and electro-pneumatic action. 
One of the first major limitations in terms of integrating a hyper-organ interface was the lack of MIDI. 
Many modern organs, such as the hyper-organ at Orgelpark mentioned in the previous chapter, allow access to it's interface through MIDI, yielding the possibility of an integration of acoustic and digital controls. 
One can imagine, for instance, coupling the positif to a synthesized sound, simply by sending it's MIDI output to a laptop running synthesis software, or an analog synthesizer, essentially playing both acoustic and synthetic from the same keyboard. 
One can also imagine playing against a prefabricated, live-sampled, or algorithmic MIDI track to create the effect of having many hands, providing for density that wouldn't be possible with one body alone.

This lack of direct MIDI integration means that any MIDI based approach, such as using a MIDI keyboard alongside the pipe organ as I've chosen to do (this will be discussed thoroughly in the section on synthesis), is not directly linked to the pipe organ itself.
This leads to what I'll call the question of coherence, namely, can this be considered to be one hyper-instrument?
Isn't it a rather a duet between two different instruments?
This is a subjective question, and the conclusion that it is a duet between two instruments is certainly reasonable.
My case, however, for a one-instrument perspective is reliant on the history of the pipe organ as one of the most modular instruments that exists.
Where one draws the line between what is considered one instrument, has never been as blurry as with this gargantuan instrument.
One could easily say that the positive division is one instrument, whereas the swell is another.
They are often even neatly separated in their own wooden boxes, called enclosures.
The interface is also already modular, with multiple keyboards representing different tonal palettes.
My keyboard is in line with this tradition, simply opening up yet more opportunities for timbral explorations.
Furthermore, the fact that the MIDI keyboard is intended to be played by the same person (me in this case), increases the sense of a unified interface.
The concept of a duet could be said to be a dialogue between to instruments, but it could be equally defined as a dialogue betwen two people.

Furthermore, the organ of l'église Saint-Édouard has a unique and interesting history. It was taken down entirely in the 1950s, with the intention of being sold. The market of pipe organ buyers is fairly limited, however, and it remained unsold, sitting in boxes in the crypt, until a new administration arrived. Upon finding these boxes full of pipes, this new administration realized hat they had found a pipe organ and reinstalled the instrument. Rather than reinstalling it in its original place, however---the conventional spot in the west gallery---they decided to place it in the north transept where it is today. I haven't been able to find why they made this decision, but I imagine that it was to bring a sense of intimacy and presence to the music-making, as the north transept is much closer to the congregation and the singer. 

It wasn't immediately clear to me whether this fact could be exploited, but I ultimately decided to integrate it into my diffusion network, by placing a bluetooth speaker in the west gallery where the organ originally resided. I will discuss this in detail in the following chapters, but the intention was to represent the spirit of this phantom organ, in dialogue with the instrument as it stands today.

\section{Aesthetic prioritisation and modal approach}

In developing an interface for augmenting the pipe organ, several questions are central.
First of all, in what way will the interface be augmented?
Historically, many hyper-instruments have made use of gesture, or touch sensitivity as a way to augment the instrument.
Taking the pipe organ, one could make the case that a gestural mapping would be logical, as it has the same issue as the piano, where the horizontal left/right plane is essential for navigating pitch space.
At the same time, the infinitely nuanced nature of gesture is a completely foreign concept to the organ, which operates almost exclusively in the discrete realm, a note being either on or off, and a stop being either engaged or disengaged.
The endlessly subtle, multidimensional contours of gesture couldn't be further removed.

This approach certainly has expressive potential, as would using pressure sensors on the keys, which could make use of another parameter not accessible through traditional pipe-organs.
At the same time, for the construction of my interface, I wanted to remain as true as possible to the tradition of the instrument itself, and what could be more traditional for the pipe organ than... adding another keyboard.

Midi keyboards have been used since the very beginnings of hyper-instruments, and have reached a state of ubiquity that in some circles they are considered passé, not worthy of more expressive and nuanced interfaces of the future.
At the same time, I argue that a midi keyboard is a particularly appropriate choice for the pipe organ, as it is exactly in line with the traditional method of interfacing with the instrument.

Here there comes another dilemna, however.
What does this midi interface play?
It would be all too easy to have the midi keyboard play some nice filtered saw waves or evolving pads and call it a day, but where would the sonic link be?
We could say that there is a link in interface, and a link in shared space, and that would be enough, but I wanted to go further and emulate the sound of the pipe-organ, based on analyses of the pipe organ of l'église Saint-Édouard, not with the goal of simple reproduction, but with the end of manipulating, transforming, and mutating these emulations in ways that would be impossible for the traditional instrument, in the hopes of achieving a dialectic, or a sort of continuum of acoustic and synthetic, which interact, overlap, disagree, and sometimes fuse.

Another option would be to add key sensitivity to the keyboard, like with Mcpherson's hyper-piano. This would provide a convergent mapping and the possibity for new forms of virtuosity, and certainly deserves to be explored in a future project. The hyper-organ at Orgelpark integrates electro magnets to map different key depths to different registrations \footcite[14]{van_heumen_new_2014}, and I could imagine this pushed even further with digital synthesis and loudspeakers. 

Ultimately, I had four main priorities in designing my interface:

\begin{enumerate}
  \item Should integrate with the pipe organ's soundworld

  \item Should extend this soundworld
   
  \item Should use both convergent and divergent mappings

  \item Should facilitate the navigation of the continuum of acoustic
and synthetic timbral profiles
\end{enumerate}

In order to fulfill these priorities, it soon became clear that a multimodal approach would be helpful. The three modalities that I ended up with are:

\begin{enumerate}
  \item Synthesis(divergent)
  
  \item Live effects(convergent)
  
  \item Bed tracks(divergent)
\end{enumerate}

\subsection{Synthesis (intentions of mimicry and mutation}

Synthesis, in particular additive synthesis, is a fairly logical approach to extending the pipe organ. 
In a sense, the pipe organ behaves in a similar way, starting with nothing, and creating more and more complex sounds out of the addition of various stops, which are always simpler as individuals than taken in the whole. 
In fact there is a long tradition of emulating the pipe organ this way. 
A prominent early example is the Bradford Musical Instrument Simulator, designed by Peter Comerford at University of Bradford in early 1980s\footcite[61]{comerford_simulating_1993}. 
I found that additive synthesis was very useful for emulating the harmonic component of organ sounds, but less so for the transient attacks, and the sustained wind noise. 
For this, subtractive synthesis, with resonant filters applied to noise generators was a perfect complimentary approach to bring the emulations to life.

\customincludegraphics[width=\textwidth]{IMG_3414_copy.jpg}{The midi keyboard is placed to the right of the organ, and plugged into a laptop running OrganLab and Ableton Live}

This synthesis is controled by a MIDI keyboard to the right of the organ console. 
This keyboard is connected to my laptop, which runs a python server called OrganLab, which I've been developing using Olivier Belanger's Pyo library. 
In addition to emulating various organ stops, OrganLab permits what I call mutations. 
Unlike the tradition meaning of this term in organ design (a stop that sounds a note other than the key depressed (usually the fifth or the third), the mutations of OrganLab are quite different. 
In fact, there are a theoretically limitless number of ways that the pipe organ emulaions could be mutated, but for a few examples, the harmonics can be transposed gradually, creating a sort of glissando, which becomes inharmonic, due to the linear transposition of the harmonics, and the exponential nature of frequency, stops can be gradually interpolated from one to another, the harmonic content can be decoupled from the noise content, the envelope can be modified, or one can even use frequency modulation to turn the organ inspired sounds into inharmonic, bell-like sounds. 
These mutations allow me to fulfill my second and fourth aesthetic priorities, allowing me to extend the organs soundworld in a way that can be seamlessly navigated, or interpolated. 
Finally, the keyboard represents a prominent divergent mapping, allowing strong independant control of the synthesis software.  
The main concept of OrganLab is to take as a starting point the acoustic sound profile of the sounds of the acoustic organ. 
This is to satisfy the first priority to integrate with the instrument's soundworld. 
I achieve this through analyses of various stops using audio recordings which are then analysed using Sonic Visualizer. 
The analyses are very rudimentary, and in fact, the first analyses were done purely visually by estimating the differences of the various partials and their envelopes. 
I then tweaked the spectral profile based on sound, trying to get it to sound as close as possible to the real thing. 
In the future it would be good to have methods of extracting more objective data from the analyses, both for the objective mesure itself as a jumping off point, but also for the increased speed that an automated approach could have on the modeling of a given instrument.   

\subsection{Live processing and effects}

\subsubsection{Issues of microphone capture and amplification in live setting}

The second modality, live effects, provided another approach. 
Like synthesis it satisfied the aesthetic priority of integrating with the sound world. 
In this case, the process is much simpler, rather than having to recreate the timbral profile from scratch, I could simply place a microphone around or inside of the instrument and amplify it. 
Then, satisfying my second aesthetic priority of extending this sound world was a matter of applying any imaginable effect to the microphone input. 
Furthermore, it allowed me to satisfy the third priority, to use both convergent and divergent mappings. 
While I was more intrigued by the divergent approach, I was nevertheless inrigued by experimenting with a convergent approach, and I felt that adding effects to a pipe organ, essentially treating it more like an electric guitar than a pipe organ, would immediately situate it in an aesthetic paradigm that wouldn't normally be associated with the pipe organ or a church space. 
While I wanted to celebrate the space and the heritage of the instrument, I also found that this dissonance of questioning the nature of the instrument also appealed to me. 

Of course, before one can even consider adding effects to a sound, the essential ingredients are at least on microphone and at least on speaker. 

When I began this project, I had a Roland MIDI keyboard ED PC-300, with an early predecessor to OrganLab, and I rented a single QSC K-10 speaker from the loan desk of the faculty of music at l'Université de Montréal. 
Initially, I hadn't considered any furher amplification than this one speaker. 
I placed it next to the organ, in order to give the impression that both sound sources came from the same direction, lending unity to the instrumental perception. 

It was my director Pierre Michaud who first brought up the idea of spatialising the sound. 
At first, this idea felt intimidating, as my project was already fairly ambitious and time-consuming. 
At the same time, when I initially began testing microphones, including my Shure Beta 58, that I've used to amplify my voice in the past, various condensor microphones, both in mono and in stereo, and a contact microphone, it became clear to me that using any delay based effects, and especially reverb, would be nearly impossible with the speaker so close to the instrument. 
At first, this was a disappointment, as it presented a technical hurdle. 
At the same time, I began to imagine a speaker in the opposing, south transept. 
This would not only position the speaker as far away from the microphone as possible, minimizing feedback concerns, but would also provide an appealing symbolic reference. 
By using a delay line, I could create a sort of call and response element, harkening to both the gospel tradition, and to the ancient tradition of antiphony, where two choirs would alternate between eachother. 
This delay line could represent an echo through history, paying hommage to the vast tradition that one draws upon in addressing an instrument as historically and culturally charged as the pipe organ. 
At the same time, at this point in late 2023, I had recently become the caretaker of l'église Saint-Édouard, which gave me more access to the space, and more room to experiment. 
I decided to hook into the house speakers of the church, situated on the ground level, to the right and left of the choir. 
In order to achieve this level of spatialisation, I odered a second QSC K10 from Ebay, and I also purchased several hundred feet of XLR cables. 
All of this is routed into my Steinberg UR44 interface.

With this four speaker setup in place, and now having clearly entered into the territory of spatialised audio, it was very tempting to create a surround sound environment, with the fifth speaker placed behind the audience. 
The main reason that this idea was so alluring, is that it would permit me to put the speaker in the west gallery, where the organ was originally installed before it's relocation to the north transept. 
This way I could represent the phantom of the original organ. 
Another echo through time. 
However, this presented yet more technological hurdles. 
On the one hand reaching the west gallery would mean yet more cables, and my interface only had four outputs, even if I acquired the cables. 
I could find another interface, but another idea stuck me. 
I could have a friend stationed in the west gallery, with a parallel setup. 
Another computer, with another interface, with another speaker. 
This would avoid the cabling problems and would allow us to use a generic interface, but came with the issue of syncronisation. 
Not being integrated with the rest of the sound system, it would be impossible for me to trigger audio events from the central hub of my laptop. 
The options in this case were to either design the audio cues so that strict syncronisation wasn't necessary, to rehearse the piece thoroughly so that this friend could precisely follow all cues where necessary, or to have this speaker not overlap with the others. 
That's to say, to have it only sound by itself. 
Ultimately, for the current project I decided that the third option was the most in line with what I was looking for. 
This will be discussed in further detail in the following chapter. 

Instead of using another speaker like the K10, I decided that it would be more practical to use a bluetooth speaker. 
This way, my friend wouldn't need to have an interface or a computer, and could simply play a soundfile off of their phone. 
I purchased a Uboom L bluetooth speaker, and tested it in the west gallery, walking around the church and finding that the volume level was more than adequate. 

\customincludegraphics[width=\textwidth]{stageplot_ste-edouard.png}{A view of l'église Saint-Édouard from above, showing the microphone and five speaker diffusion layout in relation to the organ console and the organ itself}

In order to apply he effects themselves, I considered using python as well, but it seemed intuitive to me to keep OrganLab as synthesis software, and to treat the live effects as something separate. 
I've been using Ableton Live since 2012, and so it was natural to incorporate it into the project. 
Faced with innumerable possibilities for effects, I decided to start simply, but experimenting with several classic effects, namely delay, distorsion, and reverb. 

\subsection{Bed tracks and triggers}

Bed tracks, or audio sampling, is a controversial subject in hyper-instrument design. 
On the one hand, there is an argument that using pre-recorded material takes away from the spontenaity of the music, especially restricting in an improvised context. 
At the same time, in an environment where the performer is juggling many things at once, triggering a bed track can free up the hands to perform other tasks, while not cutting the musical flow. 
Sampling can also add thickness and timbral complexity that would be hard to achieve by other means. 

Early on in the creative process, I was against the use of bed tracks, even though I've made extensive use of them in former projects. 
My concern was that I wanted to emphasize the synthesis and it's relationship with the organ, treating them as one instrument. 
The use of bed tracks seemed to confuse this relationship, introducing a non-live element.

At the same time, in Élégies, as will be discussed further in the next chapter, I thought it would be appropriate to leave the organ console in order to highlight the narrative arc of Rilke's Elegies. 
This leaving of the organ console represented a steep obstacle, however. 
On the one hand, if I was to continue to play the keyboard "offstage", the idea of taking the keyboard with me was awkward if not impossible, and the idea of having a second keyboard was similarily impractical. 
Furthermore, the journey to leaving the organ without accompaniment was fraught with uncertainty. 
In a long period of silence where the performer is leaving their instrument, the audience would invariably begin to applaud, thinking the piece was over.

Ultimately, in discussing this dilemna with my director Caroline Traube, we realized that using a bed track would solve sll of these problems. 
I could trigger a transitory track that would give me time to leave the organ, and another track could be triggered while I was away from the keyboard. 
The only thing lost is a sense of liveness, but actually this is even more aligned with my intentions, as this moment of leaving the stage is supposed to represent a moment of disembodiment. 
The fact that the keyboard heard during this segment is non-live forces the audience to wonder whether it is live or not--to imagine the unseen sound source. 

For the triggering of these sound files, I decided to use Ableton Live. 
The sound files are organised into the clips of two tracks, to allow for certain sound files which overlap others, and are triggered as scenes using MIDI. 
Originally I used OSC, but found during a test performance during les journées de patrimoine of 2023, that the OSC was simply not reliable enough to be a sole solution. 
Ulimately, I still made use of OSC, using Touch OSC on my phone, for moments when I am no near the interface, but otherwise I used a small Akai keyboard sitting to the left of me on the organ bench.

\chapter{Creative approach}

The construction of a hyper-instrument is no easy task.
As we've seen in the previous chapter, there are many theoretical questions and technical challenges that present themselves along the way.
This creative chasm is rivalled, however, if not dwarfed, by the realm of creative possibilities presented in the application of this interface, once constructed.
The structure of this thesis attempts to provide a neat narrative, chapter by chapter, going from historical context, to interface design and construction, and finally ending with creative application, but this is merely for the reader's convenience, and to organize my own thinking in a coherent way.
The reality of the process has been anything but linear, and all three of these components have informed and directed the others in the course of this research.
My original vision of the master’s program before setting upon this challenge was to segment the work into these three stages, but my director, Pierre Michaud, urged me to begin the compositional portion as soon as possible.
This led to a situation which I’ve described as, “building a bridge while walking across it”.
While inherently precarious, this approach has the advantage that the two creative processes are intrinsically linked, forming a recursive bond.
The bridge may not be perfectly arched, and may meander from its intended path—but it nevertheless represents the unique path walked by its builder, advancing step by step, and somehow, miraculously, arriving at the other side.

This fact notwithstanding, for the purposes of this paper, I wanted to compartmentalize the roles I’ve played.
In this chapter I’d like to take the perspective of a composer and a performer as much as possible, imagining that the interface was designed by a third party, and is something that I’ve received and have to contend with.
I’ll attempt to describe not only the technical constraints of the interface, but the aesthetic implications of the environment, and the various ways that I’ve exploited these possibilities.

\section{Compositional considerations}

In my compositional world, the dialectic is central.
As far as I can tell, all thought stems from a discrimination, between self and other; between here and there; between this and that.
Of course, this binary, which is also intrinsic to the functioning of computers and of the brain of humans and other animals, is often not discrete in practice.
Though the polarities define the space of knowing, they frame a continuum which includes all gradations within that binary.
From an aesthetic perspective, one could say that beauty lies in finding some kind of balance in this continuum.
This is often called symmetry.
Of course, I am not the first artist to be struck by the beauty of symettry.
This has been a fascination of artists and mathematicians since at least the time of ancient Athens and the Pythagorean experiments with geometry.
Symmetry implies a formal balance between opposing parts.
From a visual perspective, this could mean the balance of luminosity vs shadow, or the density of visual information.
At least in the traditional realm of painting or drawing, all of these fulcrums are played out in a global binary which is the balance between right and left, and up and down.
In the auditory realm, which takes time as its canvas, the symmetry has to do with the dialectic between before and after.
Other polarities are then mapped onto this time space.
Parameters like spectral brightness vs darkness, an auditory version of luminosity, density of auditory information, or loudness vs softness, are all in constant flux in time space, creating proportions that are more or less symmetric.

A beginning artist might hear this proclamation and say, well wouldn’t the most beautiful thing be something perfectly symmetric?
With a single non-retrogradable rhythm and a palindromic melody based on one of the modes of limited transposition à la Messiaen?
However, the fascinating thing about symmetry is not that it’s beautiful, but that it isn’t.
Take the human form, or any animal for that matter.
If you crossed paths with a human that had an upside down head touching the pavement, or a face with upside down eyes and nose mirroring their normal features, you would likely be perplexed, fearful, and repulsed, cursing yourself for not having more pleasant thoughts before falling asleep.
The human form is laterally symmetric, but not vertically.
Yet even laterally, the symmetry is not a perfect symmetry.
We often don’t notice, but any human face has more or less pronounced differences in lateral features like shape and size of the eyes and ears.
Artificially constructed photos of faces with perfect symmetry are generally described as unnatural and eery, a phenomenon called uncanny valley.

This means that beauty is much more illusive, and entails a careful organization of symmetry and asymmetry, creating a balance of forces that nevertheless maintains an aspect of instability.
It’s funny to note that symmetry itself, which was originally our way of describing the concept of dialectic itself, has now become another dialectic: symmetry vs asymmetry.
This is the way thought works, from top to bottom, beginning to end, branching into infinite divides.
To me, this recursive act of co-definition, and the impossible fusion of two opposing elements, is the primordial creative force.
One could say that a single contradiction holds the entire world in its arms.
There are many more of these polarities which are fundamental to my thinking, and I’ll enumerate several of them in order to provide an account of my creative approach.

\subsection{Constraint vs freedom}

A key consideration in the design of any structure, be it physical, musical, social, or ethical, is the equilibrium between constraint and freedom.
On the one hand, too rigid a structure, and it lacks the ability to adapt to the needs of its environment.
In the case of a bridge, this might mean that the impact of an earthquake will simply break it altogether, as its lacking the suppleness needed to absorb the energy of the tremor.
In the arts, this may be a lack of spontaneity, of an exploratory sense of intrigue, relying too heavily on formulaic calculations without responding to the natural tendencies and needs of the medium.
On the other hand, one wouldn't want to attempt walking across a bridge with no rigidity whatsoever.
The rigidity provides the support to allow the bridge to serve its function.
In the same way, music with no sense of rigidity can seem like a nebulous path--a bridge that constantly falling away, letting us drop down into the murky depths below us.

This balance is essential, but is not straightforward in finding.
There are certainly artists who compose with very little or no conscious constraints.
Morten Feldman was often this way, emblematized in the famous story of John Cage asking him how he composed an early piece, and his replying "I don't know".
That being said, even Feldman may place unconscious constraints on his music, and at the very least he is constrained by the medium itself--the music, and the particular instruments, spaces, and players he's working with.

But beyond these inescapable constraints, there are also constraints at the compositional level.
These can be at the level of melody, harmony, counterpoint, meter, rhythm, form, and many more.
Constraining all of these parameters would be something akin to total serialism, like Boulez's Structures I and II.
These pieces and others like it could be said to be on the extreme side of the authoritarian polarity.
That's not to make an aesthetic judgement, but only to say that it does represent an extreme of controlled parameters.
On the other hand, constraining nothing and simply letting the music express itself, while a completely valid approach, can feel like leaping from our bridge into the unknown.

Often, constraining one, or several of these parameters, and building the other parameters intuitively based on the suggestions of the constrained parameters is an excellent, concrete way to make sure that there is a musical coherence, yet an expressive freedom.
An example of this kind of approach is the sitcom Curb your enthusiasm, which uses sketches showing the overall direction and main points of the story, constraining the overall form, yet doesn't provide the actor with their lines, allowing them to improvise based on the situation.
This leads to a wonderful situation where there is a sense of genuine interplay, surprise, and liveness that simply doesn't exist with fully scripted sitcoms.
The same is true in music.
Sietze de Vries, the great Netherlands-based organist, who is a proponent of this kind of a approach, often invokes the paradox "absolute control is the way to freedom", placing emphasis on the strict adherence to a constrained element, usually melodic, in order to hone one's creative potential.

Of course, before constraining an element, one first has to have some material to constrain.
This part of the process, often called inspiration, is one of the most challenging and mysterious parts of the creative process.
There are many approaches to the generation of ideas, and infinite subtle gradations within each approach that will vary from person to person, but they fall into two main categories, which I’ll call internal vs external.
The internal approach is an intuitive approach, it relies on bringing something that is heard or felt in the mind’s eye, and bringing that to conscious awareness.
This could be through singing or playing a melody, chord progression, rhythmic idea, etc.
The external approach relies on the use of data outside the artist’s inner world.
Rather than using audiation to generate material, one might use a series of numbers, either random or based on some mathematical series, or any conceivable data, mapping these values to a musical parameter or several, such as pitch, duration, loudness, etc.
The process of mapping data onto musical parameters is called signification, a good example being the piece Searsville 1891-2015, written by Marc Evanstein for the Stanford New Ensemble, which uses data from a CT scan of a sediment core from Searsville Dam at Stanford's Jasper Ridge Biological Preserve to map the wetter and dryer periods in the dam’s history with musical density, with wet periods being more dense and dry periods being more sparse.
https://www.youtube.com/watch?v=LAT6OCifCvQ.

The difference between this internal/intuitive approach, and the external/analytic approach is significant, John Cage going so far as to claim in rule eight of his ten rules for students and teachers, creation and analysis are independant processes that can’t be mixed.
Cage likely had a different definition of analytic thinking than I do, in the sense that he seems to equate it to analyzing his work once created, rather than analyzing data in order to generate material.
That being said, his words still hold true in this case.
The divide is akin to inverse ways of approaching the creative process itself.
I like to think of sculpture.
On the one hand, there’s the perspective that the sculpture is already inside the block of stone, and that the sculptors task is simply to make that form appear.
This is the internal approach, and in music it’s reminiscent of the operatic ideal during the time of Verdi, which was not to create a shockingly surprising melody, but to construct a melody that sounded familiar, as if it had always existed.

The analytic approach is more like making the stone into the sculpture.
Rather than revealing the statue within the stone, we create it, through trial and error, and force of will—arriving perhaps at a result that we never could have imagined.
Of course, these two approaches aren’t completely independent.
In my own experience I have historically leaned more towards the intuitive side, but I am very inspired by the generative power of the analytic approach, and in my ideal vision, the two processes form a sort of symbiotic relationship, nourishing eachother along the way.

Now, once material is generated, one can start using it as constraints, against which other elements can be built upon.
The question is then which elements to constrain, and they are not equal.
For example, for an improviser, it is often much more challenging to have constrained durations, with free pitch, either limiting the player to a certain set of durations and allowing them to choose freely between them, or placing the durations in a certain order, and allowing the player to play any pitch, than it is to do the inverse, giving the player a pitch set, either as a pool to be arbitrarily selected from, or as an ordered sequence, allowing the player to freely select the durations of each note event.

The first thought when considering constraints may be constraining a single musical parameter like pitch, duration, or loudness, and this is certainly a valid approach, but often it is more pragmatic to package several musical parameters together, essentially working at a higher level of abstraction.
This way the constraint is more in line with musical thought, embodying a holistic form of the musical experience.
In language this would be the equivalent of constraining someone to speak without using the letter t, or only using words that use the first thirteen letters of the alphabet, or perhaps to speak with every third word being an adjective.
It’s possible, and sounds like a fun, though somewhat sadistic party game, but it’s not a particularly natural way to represent semantic communication.
A more appropriate constraint might be to limit one to a subject, like speaking about things that are blue, or of a certain place or person.

In the western music tradition, oftentimes the word subject is used to describe an important melodic structure in a given piece of music.
Indeed, if one examines cultures from around the world and throughout time, often melody is considered a foundational element.
From the origins of notated western music, melody has been central, for centuries being sung monophonically by Gregorian monks, and later being used as a basis for early polyphonic music, beginning as what is known as organum, thought to have originated in the 10th century and culminating in the writings of the Notre Dame school masters Léonin and Pérotin in the 12th and early 13th century, and later in the complex motets of the ars nova school led by Guillaume de Machaut and Philippe de Vitry.
These flourishing periods of creation and polyphonic innovation have one important thing in common: they all rely on a constrained element.
This element came from the Gregorian chant which was so established at the time, placing the chant in the tenor voice and composing the other voice, or voices around this tenor.
This constrained voice is often called a cantus firmus, coming from the latin for fixed melody.

\subsection{Dialectic and fusion}

As seen through the various axes discussed--the voice vs the organ, constraint vs freedom, artificial vs real, history vs contemporainity, etc., dialectic thought is very important to my approach.
In my philosphy, the binary is the most fundamental building block of thought.
Even the most basic idea precludes a distinction--between self and other, or between this and that, for instance.
To me the entire world exists inside this divine contradiction, and art in its highest form is like the impossible resolution of these opposing forces.
As far as we know, nuclear fusion is the most immense creative force of energy production that exists.
The triggering of this fusion process is not trivial however, requiring a temperature of more than one-hundred million degrees fahrenheit.
Perhaps the fusion, or synthesis, of ideas in the realm of art is just such a creative force, requiring an equally massive force to hold them together.

\section{As a performer (physical constraints)}
As far as the issues of interacting with the interface from the perspective of a performer, there are plenty of new obstacles to overcome and integrate. As a relative newcomer to the pipe organ in general, the traditional acoustic instrument already provides a significant challenge, and when I started playing the pipe organ at l’église Saint-Édouard, I found enough difficulty in playing the instrument as it was, that I was intimidated by taking on the additional challenge of the augmented component. I nevertheless began to incorporate little by little the practice of managing the fourth keyboard. 

Having a fourth keyboard may seem trivial after already traversing three manuals and a pedalboard, but the principal challenge was the angle. Whether at the piano, the harpsichord, or the organ, a keyboardist will generally place themselves somewhere near the centre of the instrument, facing it directly. My original plan was to place the keyboard underneath the others to maintain this continuity of angle, but it quickly became apparent that there was not enough space to make this feasible, and the idea was born to place the keyboard off to the side. This was a very new and uncomfortable experience for me. The practice of keyboards at 90 degrees is not unheard of. Artists like Chick Corea and Cory Henry have used this setup, and although normally they switch from one to the other, facing each instrument as normally, on rare occasions they will play both instruments at the same time, generally providing chordal pad sounds with the left hand while soling on the other keyboard with their right hands. 

While this approach may have a niche precedent in jazz circles, it seems to be new to the pipe organ world. In German and in Dutch, the manuals are often named by their position, including Oberwerk, or upper manual, and the Rückpositif which as the name suggests, originated as a separate manual placed behind the player, but eventually became incorporated into the main instrument in front of the player. A “Seitewerk” or “Seitepositif”, which would be the logical German translation of “side manual” seems to not exist at all, as far as I can tell. In any respect, this new orientation certainly took some getting used to, as it threw off my entire basis of proprioception. Not only this, but it introduced with it another problem. I mentioned that Chick Corea and Cory Henri typically play the solo with their right hand while chording with their left, but what if I want to put the melody in the acoustic organ and play the bass part with a synthesized sound? This may seem like a niche requirement, but is not an unreasonable orchestration choice. Yet if the midi keyboard is set to my right side, this means playing the bass part with my right hand, which sounds like a keyboardist’s uncanny, nervous dream. Placing the keyboard to my left, which was less practical at the console at l’église Saint-Édouard in any case, would mean that synthetic sounds in the melody would have to be played with the left hand, which is even stranger. 

Two obvious solutions present themselves here, one being to move the keyboard from side to side during a given performance, and the other is simply to have two keyboards on either side. In the first case, after some consideration, I realized that not only would cabling make it a nightmare, but it would take too much time. Of course I could hire a sort of stage crew to move the keyboard around, but in the spirit of DIY, I didn’t want to have to depend on this kind of arrangement, if only for the practical reason that I could rehearse without needing to manage a team. For the second option of having two keyboards, I don’t have an excellent reason why I haven’t chosen to go in this direction, and it is certainly something that could be explored in the future, however, my main contentions are that the instrument is already incredibly complex, and adding one keyboard is already stretching the limit, and two additional keyboard seems excessive. I also like the aesthetic of asymmetry, and of course there is the practical issue that two keyboards, would box me in, creating a claustrophobic environment. After reflecting on all of these options, I ultimately elected to maintain the original version of one keyboard on the right. Originally I thought I would just avoid putting the bass part in the synthesizer altogether, but in the end, being something of a masochist, I decided to treat playing the bass part in the right hand as a new avenue of virtuosity, incorporating it in small doses to begin integrating the new spatial and cognitive mappings.

Besides the issue of keyboard angle there are also the issues of sustain pedal, patch changes, sample triggering, and volume control. The midi keyboard permits the pianistic privilege of the sustain pedal, which is not found on a traditional pipe organ, and is particularly useful for drone sections where one can simply depress the damper pedal, freeing the hands to do other things. One of the things that the hands might be freed up to do, besides playing the other manuals or changing the registrations, is triggering samples or regulating the volume of the synthesized sounds. Originally, all of this was done through OSC, using a TouchOSC patch on my phone, but the internet connection in the organ loft is not reliable enough for this to be a sound solution, and MIDI was selected. This way, various keys are assigned to control changes on channel 6, stepping through a global navigation scheme which modifies the settings of both OrganLab and Ableton Live, as well as triggering samples. Two sliders are assigned respectively to the volume of Pyo and Ableton, with a third slider assigned to both volumes. 

\section{Tools}

\subsection{GrandOrgue}

\customincludegraphics[width=\textwidth]{grand_orgue.png}{}

\subsection{OrganLab}

\subsection{Open Music}

\subsection{Ableton Live}

\chapter{Élégies}

\customincludegraphics[width=\textwidth]{2024-04-28_MH_sat50.png}{}

\section{Context and precedents}

\section{Expanded practice}

Until now I've been speaking of the pipe organ as an instrument. That's to say, an interface, with keyboards that receive gestural data and pipes that radiate sound to the listener. At the outset of this project I intended to focus on this instrumental perspective, expanding the timbral possibilities with synthesis.

While creating élégies, however, it soon became clear that the space itself was integral to the context of the instrument. The pipe organ is unique in this regard, being not only an instrument, but a permanent architectural installation. This fundamental connection to the sacred space of the church was too compelling to ignore, and soon inspired me to look for ways to expand my artistic vision of the piece beyond a straightforward instrumental perspective.

I call this approach an expanded practice, and in the following sections I'll describe several ways in which I explore this expansion.

\subsection{Spatialisation}

Firstly, the sound diffusion discussed in chapter two was a way for me to play with the fullness of the space. In this way, a listener hears not a unified sound source coming from the organ, but a wide variety of sources surrounding them. Each source has a poetic significance. The synthesis comes from the same direction as the pipe organ, creating a unified instrumental experience.

The effects come from the opposing south transept, implying a dialectic between the instrument, and it's transformation, or in the case of the delay effect, an echo. The echo is perhaps the most cogent effect from a poetic perspective, as it evokes the sacred traditions of antiphony and call and response, while implying the echo of history. The echos create a feedback where each iteration becomes quieter and quieter, similar to how the further we move into the past, whether collectively in terms of history, or personally in terms of memory, the more foggy and dim. Another metaphor is the mirror. I imagine the organ as the protagonist, facing a distorted reflection of itself.

\subsection{Bed tracks}

The other sense in which I attempt to embody the richness of l'église Saint-Édouard is through the use of bed tracks, and in particular through what I call "le chemin", a reference to "le chemin de croix", or in english, the stations of the cross. In english we emphasize the moments in the fourteen stages of Christ's cruxifiction, yet in french, we emphasize the movement between them---the journey itself. My piece also represents a journey. Both musically and dramaturgically, as well as symbolically. The use of bed tracks is a prominent way that I access this symbolic dimension. Throughout the piece are heard audio recordings in various areas of the church, including outside the church. During my master's program, in addition to being organist, I also became caretaker at l'église Saint-Édouard, allowing me access to the many areas of the church that would not be otherwise accessible. I used this privilege to my advantage, taking audio recordings in the sprinkler room, in la salle Morin, in the furnace room, and others. The use of both recordings from the church itself, and the basement of the church has a personal dimension, representing the complementary, parallel roles of organist and caretaker that occupied so much of my attention during the writing process. The general narrative of the path is that the subject walks towards the church as the bells sound in the distance. She then enters the church through the basement, moving past the sprinkler room in la salle Saint-Édouard and through la salle Morin, eventually coming to the furnace room. In my mind, the furnace room represents the bowels of the church–a key moment of darkness in the narrative structure. At the same time the ominous sound of feedback in the church, throbs incessantly in the background. Later a match is struck, referring both to the candles that burn day and night in the church, as well as to the darkness of the path being walked. Finally, the sound of rain enters with the same sound of bells from the beginning of the piece, but in reverse. The last tableau is the sound of the subject walking up the steps from to the organ balcony, accompanied by the sound of pigeons.

The reason that I call this path symbolic is that first of all, like a photograph, it is not the experience itself, but a representation of that experience, but furthermore, the sound recordings heard throughout the piece were not captured on the same day, and are not continuous through the space, at times juxtaposing several different spaces at once. This presents not a real path through the church, but an imagined one that escapes the bounds of temporal and spatial limitations.

In the following sections, I'll outline the different stations of the chemin, in order of their appearance in the piece. Rather than fourteen, there are seven, with each recording representing different places in and outside of the church. Sometimes these recordings are used as linking material between movements, and sometimes they are heard during a movement.

\subsubsection{First station}

The first station opens the piece, and is a combination of two recordings. On the one hand, a recording of me walking down Beaubien from my apartment to l'église Saint-Édouard during the summer rain storms of July 2023. This recording is placed against a recording I downloaded from YouTube channel "à trouver" of the bells of l'église Saint-Édouard. This latter recording was distorted with "à trouver" and treated with the Spectral Blurring plugin by Michael Norris. These two recordings enter gradually, and are passed between the four speakers connected to my interface in a circular fashion. The sound of the rain in the speakers when encircling the listener is similar to the sound of the wind, which is punctuated by the sound of thunder in the distance. I was fascinated by the idea of placing into question the distinction between inside and outside. The church is a contained unit, which is often heavily sonically isolated from environmental sound, especially with stone constructions like l'église Saint-Édouard. By using audio recordings of the outside of the church within these confines provides a rare experience of hearing the external from within, as if this sacred boundary had been dissolved.

This bed track serves a poetic purpose, but it also. serves a very practical purpose. It allows me to get to the organ discreetly. Before the concert, I give a brief talk describing the piece, my perspective as I was writing it, and what the listener can expect to hear. After his talk, I trigger the first bed track using Touch OSC on my phone. The track enters gradually, building, and eventually looping a portion of the two recordings. This looping means that my ascent to the organ loft is not constrained to a specific time. Upon arriving at the organ and verifying that everything is well prepared, I simply press the next key on the midi keyboard, triggering a slow fade out of the opening recordings.

\customincludegraphics[scale=0.25]{DSC00200_1.JPG}{L'église Saint-Édouard from la rue Beaubien after nightfall.}%%Station 1

\subsubsection{Second station}

The second station is a recording of entering the church through the basement, and entering the sprinkler room in the basement of the church. This is the room that stores all the compressors and valves that connect to the elaborate sprinkler system throughout the church. I found that this room emitted an interesting electrical hum, and seeing as it's right beside the entrance on Beaubien, it seemed fitting to begin the journey inside the church at this point.

This recording also serves a practical purpose, this time as linking material between movements. I trigger the recording with the attack of the final chord of the second movement, with the hum of the compressors copied and doubled at the fifth below, giving me the E natural to begin the next movement.

\customincludegraphics[scale=0.25]{DSC00149_1.JPG}{The sprinkler room upon entering the basement of the church.}%%Station 2

\subsubsection{Third station}

The third station takes place in la salle Morin. This is a hall that is very frequently rented for events, and is named after the first priest of l'église Saint-Édouard Joseph-Napoléon Morin in 1895. This recording also provides a link between movements, this time between the fourth and fifth. The recording is triggered at the peak of a dynamic swell as the left and right hands drop out leaving only the sound of the buzzing fridge of la salle Morin and the ticking of a clock in the background. Here I play with the unique lens of subjectivity that the microphone provides. Elsewhere in les chemins, often I am holding the microphone as I walk, providing a close representation of my subjective aural experience. But in the third station, I leave the microphone on the fridge while I leave the room. This recording is then crossfaded with another recording where I move the microphone closer and closer to the ticking clock. At the same time, I double the sound of the clock, with a time offset, creating the sense of fracturing of time, while my footsteps recede into the distance.

\begin{figure}[h]
    \centering
    \subfigure{
        \includegraphics[scale=0.25]{DSC00123.JPG}
    }
    \subfigure{
        \includegraphics[scale=0.25]{DSC00136.JPG}
    }
    \caption{The fridge of the salle Morin pictured on the left, and the clock viewed from the fridge on the right.}
    \label{fig:sidebyside}
\end{figure}

\subsubsection{Fourth station}

The fourth part of le chemin begins in the furnace room. This room has a fantastic sliding metal door that creates a loud, foreboding sound when opening or closing it. This sound opens the seventh movement, which is the only movement that is wholly electroacoustic. By this point the listeners are completely in the dark, and the sound of this door opening is preceded by the sound of a match being lit, as if preparing to enter the darkness of the furnace room. As we enter this room, a soft, dissonant drone is heard in the background. This drone appeared as a feedback loop while testing reverb treated amplification in the church. This was a very strange feedback loop, however, because generally feedback grows quickly out of control, looping the amplified signal again and again through the microphone until the sound becomes unbearably loud. In this case, the feedback established some sort of equilibrium point, where the signal was self sustaining, not growing or diminishing. I took advantage of this unusual moment, placing my field recorder on the floor in the middle of the church. Then, during the ominous drone, I decided to run towards the microphone. This is essentially the opposite of the subjective effect I described in the last station, where instead of moving away slowly from the microphone, I runs violently towards it, getting louder and louder as my footsteps approach. In both cases this creates a transformation from subject (me coupled the microphone), to object (the microphone listening to me).

The rapid footsteps, combined with the foreboding nature of the feedback resonance, the door and clicking sounds of the furnace room, and the darkness of the church, establish an aura of urgency and unease which I thought appropriate for the seventh elegy. In the fragment that I selected, "Non, plus d’imploration, voix maintenant mûrie, plus de clameur.", Rilke invokes a call to action, a plea to resist the infirmity described in the preceding elegy. Furthermore, this is the first time in the piece where two different spaces are juxtaposed. By placing the listener simultaneously in the furnace room and in the church, there is a dreamlike, disorienting element, if not directly experienced, as the listener will not recognize the places of recording without prior knowledge, at least at a poetic level.

\customincludegraphics[scale=0.25]{DSC00157.JPG}{The metal sliding door of the furnace room. When the recording took place this door was burgundy, but it has since been painted white.}%%Station 4

\subsubsection{Fifth station}

The fifth station punctuates an important moment in the piece. It precedes the first moment–a part from the short moment at the end of the fourth movement–when the voice prominently joins the organ. In this station, another match is heard being struck, this time with more prominent crackling, and the voice enters with a variation of the original melody singing subharmonics. This inhuman sound is supposed to reference "la créature" of Rilkes eighth elegy, opening its eyes, symbolizing a transformation from the undeveloped fragility of the figuer of the sixth movement into a more realized form. To emphasize the inhuman, fearsome nature of this transformation, as well as to connect it to other thematic elements of the piece, I gradually introduce a vocoder to my voice during this brief subharmonic section. The carrier of this vocoder is the same bell recording from the beginning of the piece, representing a merging of bells with the human voice.

At the same time that this vocoder is being introduced, a four bar beat gradually fades in, initially coming from only the south transept speaker. As the beat reaches maximum volume, it is suddenly diffused in all four speakers, while at the same moment, a prominent strike of thunder is heard. This beat then accompanies the first prominent moment of singing along with the pipe organ in the piece.

\customincludegraphics[scale=0.02]{candles.JPG}{The ever burning candles of l'église Saint-Édouard flicker in the darkness.}%%Station 5

\subsubsection{Sixth station}
%%\customincludegraphics[scale=0.25]{DSC00160.JPG}{}
%%\customincludegraphics[scale=0.25]{DSC00171.JPG}{}
The sixth station continues the previous station's reference to the opening sounds of the piece. Another sample of the July storms of 2023 are heard, this time with an emphasis on the rain rather than the distant thunder sounds featured in the opening bed track. Later on, this rain sound is crossfaded into the same bell recording from the beginning of the piece, this time in reverse. This reversal of time provides a poignant reference to the theme of transcendance evoked by Rilke's final elegies, while creating a sense of aesthetic unity in the piece.

\customincludegraphics[scale=0.1]{eglise_mh.JPG}{The bell towers whose sound features prominently throughout the piece.}%%Station 6

\subsubsection{Seventh station}

In the creative process, it is somewhat rare that ideas come in order, and more often than not a major part of this process is organizing the material in a coherent way. That was certainly the case with Élégies, with the exception of this final station. Not only was this the last part of le chemin to be finalized, but the realization of its necessity came only the day before the first performance on April 25, and the recording took place on the morning of the concert itself. This station is in two parts, and the first part, the climbing of the stairwell leading to the organ loft, had been my leading idea for how to end the piece for some time. It seemed to both symbolize the ascent from the long experience in the depths of the furnace room, which is connected to the organ loft stairwell through what's know as "la mezzanine", but it created a link to the beginning of the piece, which begins by my entering the organ loft from the stairwell. In fact, on thing that I like about this element is that I imagine the listener to experience the ascent of the stairwell at the beginning of the piece as preceding the beginning of the piece, while after the recording of this ascent featured prominently at the end of the piece, the beginning might be re-interpreted as having started earlier, with the ascent being included as an integral part of the piece.

This idea always seemed, however, sonically lacking. The sound of climbing the stairwell in and of itself is not remarkable. There is not the same timbral intrigue that the other stations have. While preparing for the concert, I suddenly reflected on the first time I had ever seen l'église Saint-Édouard. I was walking with my friends Marie-Hélène and Francis, and as we passed the church, admiring it's beauty, Marie-Hélène stopped to take photos of the many pigeons who call the church their home. The sound of pigeons is an ever present part of the Saint-Édouard soundscape, and I suddenly realized that this would be the perfect reference to end the piece. The sound of the flight of these gentle birds would parallel the ascent of the stairwell, adding to the spirit of transcendence while offering a more interesting timbral profile than the sound of the steps on their own. I found an area of the church near a window where the pigeons often sit, and placed my recorder near the open window. I left it recording for several hours, waiting for them to arrive, occasionally gesturing at the window to inspire them to fly away. I used the sounds of their cooings and flutterings, layered, and gradually slowed down, against the slowed down recording of my footsteps ascending the stairwell. Each fluttering is slightly longer, and as the recorder was placed at the middle of the stairwell between the mezzanine and the organ loft, my footsteps gradually become louder before fading off into the distance to end the piece, creating a sense of peace, yet uncertainty, as the footsteps march off to the unknown, leaving the listener behind.

\begin{figure}[h]
    \centering
    \subfigure{
        \includegraphics[scale=0.25]{DSC00117_3.JPG}
    }
    \subfigure{
        \includegraphics[scale=0.201]{pigeons.jpg}
    }
    \caption{The stairwell leading to he organ loft on he left, and the pigeons of l'église Saint-Édouard nestled into the church's stone walls on the right.}
    \label{fig:sidebyside}
\end{figure}

\section{1e Élégie}

\epigraph{\textit{Qui, si je criais, qui donc entendrait mon cri parmi les hiérarchies des Anges?}}{}

\subsection{Narrative context}
Rilke's Élégies begin not with a cry, but with the question of a cry.
A question seemingly addressed to no one.
A plea of solitude and desperation.
Perhaps this question is addressed to God, to the reader of his poem, or to himself.
In any case this question represents perhaps a sort of internal cry, representing the subjectivity of our protagonist's lament.
He goes on to evoke the level of solitude felt in his unknown setting, wondering if anyone is listening at all, calling into question the purpose of crying, or of communicating in any way.

From a sonic perspective, the issue of sound is immediately relevant, the cry evoking at once a sound source---the human voice---and the volume, and intensity of this sound.
The subjective nature of sound is also essential, with "qui donc entendrait" illustrating the importance of hearing and perception to the auditory experience.
This startling juxtaposition between the immense loudness of the subject's inner world, and the expansive quietude of the environment provides an immediate dialectic between inner and outer worlds.

The second part of the sentence describes the setting somewhat, though in obscure terms.
He depicts the subject as among the hierarchies of angels.
This brings another essential dialectic lense to Rilke's world: the spiritual and the human.
Here is a startling image.
We were quickly led to believe that we were alone, contemplating the futility of communication, when not only are we not alone, but we are found among a cacophonie of angels.
The reason that the angels might not hear the subject's plaintif cry is unclear. 
Maybe the angels are simply too far away.
Maybe they are making too much noise themselves to hear anything.
Or maybe they do not contain the capacity for subjective experience at all.
If we take this last case, this quickly paints an image of Rilke's angel, not as an anthropomorphized, man's best friend, but as a very different creature entirely.
The idea that they would hear nothing implies an austere, cold distance from the angels---the small human alone underneath a swarm of ethereal beings.

\subsection{Techniques}

Software:

One of the main advantages of a digital pipe-organ over an acoustic instrument, is the ability to use non-discrete pitches.
To represent the hierarchies of angels, I thought that it would be appropriate to make use of an extended glissando, starting in a lower register and moving upwards, signifying some kind of transcendental nature.
It might seem strange to represent a hierarchy with something non-discrete and essentially non-hierarchical, but the hierarchy is already present in the harmonic series.
The way that this glissando function works, is that over the course of a set amount of time (in this case two minutes), the frequency of each of the eight partials is incremented by one about ten times a second.
Because the frequency is being incremented, and not the midi equivalent, the harmonic series quickly becomes inharmonic.
For instance, if 100 and 200 are the first two partials, after an addition of 10, 110, and 210 are no longer multiples of eachother.
This process creates the sensation of an alien sort of movement, pulsating and morphing, yet not in a coherent, human way.    
\begin{figure}[H]
\begin{lstlisting}[language=Python]
glissC = [0 for i in range(8)]
def glissUp():
    global glissC
    for i in range(len(glissC)):
        glissC[i] == 0
    if glissC[0] < 600:
        stop1.setTrans(glissC)
        for i in range(len(glissC)):
            glissC[i] = glissC[i] + 2
    else:
        for i in range(len(glissC)):
            glissC[i] = 0
\end{lstlisting}
\caption{The glissUp function takes a python list and increments it until it reaches 600, this list is used to increment the first 8 partials of the synthesis module}
\end{figure}

Compositional:

For this movement, I wanted to represent the binary form of the poetic fragment with a juxtaposition of proportional notation and semi-metric notation.
The proportional notation is supposed to represent the angelic form.
Here again the choice is somewhat arbitrary.
One could easily make the case that metrical time is more hierarchical.
At the same time, metrical time has a long tail of history in western notation, and has for me a notion of human comprehensibility, whereas proportional notation is newer, with less historical associations.
Furthermore, proportional notation seems to give the sense of music frozen and static in time, which I associate with the timelessness of the ethereal.

\subsection{Composition Process}

This movement begins with a sung melodic fragment, which represents the principal melody of the piece, taken from a session of singing the poetry of Rilke in l'église Saint-Édouard in the fall of 2022.
The melody comes from the first line of the first elegie and has been modified only slightly from the original recording. 

\customincludegraphics[width=\textwidth]{3-1-1_melody}{The main melody of the piece, based on the 19 syllables of the first line of Rilke's elegies.}

I wanted to then invoke a sonic interpretation of the cry.
Even though the cry is simply a hypothetical, and is not actually enacted in the poem, I wanted to represent the internal, subjective cry.
Of course there is no correct, objective way to represent subjectivity, but I've attempted a poetic approach.
To do this, I call upon a distinctly north american property of the pipe-organ: the crescendo pedal.
This pedal allows the player to quickly open or close many stops simply by flicking the foot.

One loses detailed control over the registration, yet gains access to a quick dynamic swell that surpasses the capacity of the normal expressive pedals.
I begin the piece with the crescendo pedal open about 60% of the way, with the acoustic and digital organs playing the same D minor chord.
At first, the acoustic instrument masks the digital sound completely, evoking the sens of solitude, but gradually the crescendo pedal closes and the acoustic instrument is masked, leaving the glissando as the focus.
The gradual widening of registral distance between the acoustic organ and the digital glissando evokes the sense of distance between the human and the angelic realms.

As far as the notation of the glissando, my original sketches included lines symbolizing the gesture of the sound and the many partials, and the ascent of the angels, but in the final manuscript, it felt like an unnecessary use of space on the page.
The organ-lab setting is desribed with the number one placed in a box, and upon placing the hand in the right position, the glissando will be handled by the software.

\customincludegraphics[width=\textwidth]{3-1-2_gliss}{Original sketch of the glissandos of the first movement}

\customincludegraphics[width=\textwidth]{3-1-3_system}{Final version of the first organ system, with the 1 in a box representing the first setting of Organ Lab, the sustain pedal used to maintain the glissando in the upper stave, and the crescendo pedal notated at the bottom.}

The choice of pitches in this movement come from a range of influences.
For one, one of the original inspirations of this piece was the Magnificat of Jehan Titelouze, and particular the sixth piece, Sexti Toni, which is based in F major.
For the first chord, I didn't want the brightness of F major, and decided to use the relative, D minor.
Not long after, on analysing the bells of l'église Saint-Édouard, I realized that they were also based in F major, which further jutified this decision.

Towards the end of the movement, the pipe organ slowly enters with ascending chords built around open fifths and fourths.
These mostly parallel, open harmonies for me evoke two influences.
On the one hand the organum tradition of Leonin and Perotin, and on the other hand, the open harmonies of Boards of Canada.
Ultimately, these chords rise like the synthesized glissando, representing the human who is caught under the ethereal.
Here the notation changes from proportional, to semi-metrical.
I use this word because the quarter note and half note of metrical notation are present, yet meter itself is not.

\customincludegraphics[width=\textwidth]{3-1-4_system}{Rising chords based on open fifths and fourths}

\section{2e Élégie}

\epigraph{\textit{Tout Ange est terrible.
Et pourtant, malheur à moi!}}{}

\subsection{Narrative context}

In this movement, the haunting opening of Rilke's verse, "Tout Ange est terrible," forms the thematic crux.
The word 'terrible' presents a complex duality, oscillating between connotations of fear and awe and a sense of pathos and frailty.
This ambiguity is not just a linguistic puzzle but a profound emotional landscape that the music seeks to explore and express.
The latter part of the verse, "Et pourtant, malheur à moi!
pourtant je vous invoque," resonates as a solemn invocation, a plea before embarking on an arduous, soul-searching journey.
This movement attempts to capture the essence of this prayer, infused with both trepidation and a resigned yearning.

\subsection{Techniques}

Software:

The use of a gradual dynamic envelope is a pivotal element in this movement.
By enhancing the traditional bourdon sound with a brighter tone, and accentuating the fourth partial, a unique auditory experience is created.
This approach transcends the limitations of an acoustic pipe organ, illustrating the intersection of tradition and innovation.

\begin{figure}[H]
\begin{lstlisting}[language=Python]
def dynEnv():
    print('Enveloppe dynamique')
    stop1.setPart([1, 2, 3, 4, 4, 4, 0, 0])
    stop1.setMul([0.588, 0.338, 0.665, 0.773, 0.512, 0, 0, 0])
    stop1.setEnvAtt([0.285, 0.450, 0.327, 0.338, 0.385, 0.277, 0, 0])
    stop1.setEnvDec([0.02, 0.04, 0.085, 0.008, 0.008, 0.008, 0, 0])
    stop1.setEnvSus([0.446, 0.523, 0.404, 0.05, 0.05, 0.542, 0, 0])
    stop1.setEnvRel([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0, 0])
    stop1.setNoiseAtt(0.081)
    stop1.setNoiseDec(0.146)
    stop1.setNoiseSus(0.7)
    stop1.setNoiseRel(0.1)
    stop1.setNoiseMul(3)
    stop1.setNoiseFiltQ(3)
    stop1.setSumMul(0)
\end{lstlisting}
\caption{The dynEnv function alters the dynamic envelope of both the harmonic sound and the noise sound, while tripling the fourth partial}
\end{figure}

Compositional:

The structure of the movement employs a binary form to parallel the contrasting themes of Rilke's verse.
Part A, representing the 'terrible angels,' is characterized by dense chromatic harmonies and a rich, plein orgue registration, employing proportional notation for depth.
In contrast, Part B adopts a simpler harmonic language, employing metric notation.
This section features an inverted version of the initial melody, serving as a cantus firmus.
Interestingly, all durations in this voice are equalized as whole notes, played with the pedals along with the bourdon and flute 8’.
This structure serves both as an improvisational guide and a compositional framework, offering the performer the liberty to improvise around the notated cantus firmus, colored in red, with an alternative pre-composed version available as a guideline.

\subsection{Composition Process}

The creation of this movement began with the creation of the sung melodic line.
I considered using new motivic material, but ultimately decided that it would be better to use a variation of the original melody.
This is partly to maintain coherence, and partly in reference to the Magnificat of Jehan Titelouze which uses a repetition of a sung melodic fragment as a formal element.
This thematic variation is now infused with additional chromaticism, reflecting a progression into complexity and uncertainty.

\customincludegraphics[width=\textwidth]{3-2-1_melody}{A variation of the first sung melody, with more chromaticism}

The creation of Section A of the organ composition began with a dissonant harmonization of the main melody, making extensive use of an altered dominant sharp 7 chord.
This chord, a variation of the classic altered dominant chord in jazz, swaps the flat seven for a sharp 7, alongside the dominant 7 sharp 13 flat 9, and the minor major 7.
These harmonies, particularly resonant with the plein orgue registration, evoke an intense, almost ethereal quality.
The subtle voice leading within this dense texture aims to create movement and emotional depth.
Section B's composition process, focusing on the outer voices, was governed by the principles of traditional counterpoint.
This involved careful resolution of dissonances and maintaining a fluid musical dialogue between the voices.
The movement, starting from a melodic D minor perspective, culminates in a half cadence to the A minor dominant, signifying a transition yet an unfinished journey.

\customincludegraphics[width=\textwidth]{3-2-2_system}{System showing the progression from dense chromatic harmonies in proportional notation to metrical notation with outervoices braiding an inner cantus firmus}

\subsection{Linking}

The movement concludes with an auditory link to the next – a sound sample of footsteps echoing in the basement of the church, leading into the sprinkler room.
The mundane, yet eerily resonant sounds of the church's underbelly, including the hum of compressors, provide a sensory bridge to the third movement.
This blending of the sacred with the profane mirrors the thematic journey of the music, from the divine to the earthly, the celestial to the subterranean.

\section{3e Élégie}

\epigraph{\textit{Chanter l'Amante est une chose. C'en est une autre, hélas! de chanter cet occulte Fleuve-Dieu du sang.}}{}

\subsection{Narrative context}

I interpret this poetic fragment as a reflection on the difficulty of walking the path ahead--contemplating the dichotomy between the relative simplicity of singing of love, versus the profound challenge of vocalizing the sacred, yet enigmatic, 'God river of blood.' 
This highlights a dialectic between thought and action, and the contrast between superficial perception and embodied reality.
This narrative conflict sets the tone for the movement's musical exploration.
Musically, I interpret "chanter", in a few ways, on the one hand, as the voix humaine stop on the récit, with tremolo, which imitates the sung human voice.
"Chanter l'Amante" is represented by a simple song-like melody, and "Chanter cet occulte Fleuve-Dieu du sang" is interpreted as the compositional and timbral disintegration of the movement.

\subsection{Techniques}

Software:

I thought that it would be appropriate to emulate the tremolo of the voix humaine stop with frequency modulation of a voix humaine emulation in OrganLab.
With this idea, it seemed like a logical step to increase this frequency modulation in speed, while changing the index and ratio, such that the natural trembling of the voice, passes through a comic, exaggerated warble, gradually losing the sense of sung tremolo altogether, giving way to an inharmonic, bell-like sound.
This is all done by interpolating with SigTo() objects, and the entire process takes place over 180 seconds.
The one part that is not interpolated, due to limitations of ADSR objects in Pyo, is the envelope, which simply changes suddenly at the end of the 180 seconds.

\begin{figure}[H]
\begin{lstlisting}[language=Python]
bellCall1 = None
bellCall2 = None
bellCall3 = None
bellCall4 = None

def bell():
    global bellCall1, bellCall2, bellCall3, bellCall4 
    bellCall1 = CallAfter(stop1.setEnvAtt, time=180, arg=(.001, .001, .001, .001, 0.001, 0.001, 0.0001, 0.0006, 0.0007, 0.0005, 0.0006, 0.0003, 0.0005, 0.0003, 0.0006, 0.0005, 0.0004, 0.0002, 0.0001, 0.0001)).play()
    bellCall2 = CallAfter(stop1.setEnvDec, time=180, arg=(1.3, .05, .02, 0, 0, 0.04, .004, 0.04, .04, 0.04, .04, 0.04, .04, 0.04, .04, 0.04, .04, 0.04, .04, 0.04)).play()
    bellCall3 = CallAfter(stop1.setEnvSus, time=180, arg=(.4, .1, .02, .01, .01, 0.01, .01, 0.01, .01, 0.01, .01, 0.01, .01, 0.01, .01, 0.01, .01, 0.01, .002, 0.002)).play()
    bellCall4 = CallAfter(stop1.setEnvRel, time=180, arg=(2, 0.1, 0.1, .01, .03, 0.4, .04, 0.04, .04, 0.04, .04, 0.04, .04, 0.04, .04, 0.4, .04, 0.04, .04, 0.4)).play()
    setInterpol(180)
    stop1.setRamp(180)
    stop1.setMul([1, 0.01, 0.1, 0.01, 0.07, 0, 0.02, 0, 0.01, 0, 0.003, 0, 0.003, 0, 0.001, 0, 0.001, 0, 0.001, 0])
    stop1.setRatio(0.43982735)
    stop1.setIndex(4)
    stop1.setNoiseAtt(0.001)
    stop1.setNoiseDec(0.1)
    stop1.setNoiseSus(0.01)
    stop1.setNoiseRel(0.1)    
    stop1.setNoiseMul(0.9)
    stop1.setNoiseFiltQ(4)
    stop1.setPartScRat(1.02)
    print(bell)
\end{lstlisting}
\caption{This function morphes the ratio and index of frequency of modulation, as well as a very slight exponential expansion of the harmonic series on the attack, which creates an additionally inharmonic attack. The envelope is altered after 180 seconds through the bellCall function calls}
\end{figure}

Interpolation in general was a major goal of this project, and one of my initial ideas was the transition between harmonic organ sounds, and inharmonic and/or chaotic sounds.
This process is semantically justified through the passage from the simplicity of singing of love, versus the reality of singing the God river of blood.
It is as though the simple dream is interrupted by a descent into a more complextimbral world.

Compositional:

The movement adopts a song form to represent 'singing,' starting with an 12 bar melody as the A section.(Grove reference)
Instead of following a traditional AABA form, the A section is repeated incessantly in a variation form.
These variations expand exponentially at a rate of 5/4, symbolizing a narrative disruption akin to a rupture in the fabric of time.
During these variations, the left hand chords and rhythms are constrained, while the right hand is free to improvise. 
A composed version is offered as a suggestion, or as an alternative to improvisation.
To parallel the software techniques, frequency modulation is also applied at the notational level.
A patch in OpenMusic uses the A section melody as the carrier and the bass line as the modulator to generate chords.
Selected sonorities from this process are integrated into the variations, enhancing the movement's thematic and textural depth.

\subsection{Composition Process}

The movement begins with a sung fragment, introducing a lilting melody, rising by thirds and spelling out an e minor triad.
The organ then enters with a similar rising motive, harmonized with perfect fifths below the melody.
Throughout this process the tonal center is tenous, first suggesting B minor in the voice, then A minor in the organ, with a thin, hollow texture of 8' and 2' flutes, before pivoting towards D minor.
This sets the stage for the entrance of the main motive of this movement, a lamenting descending line, beginning on the third scale degree, in this case F, and moving down towards the fifth scale degree, C in this case.
This descending line is in stark contrast with the rising motive heard earlier.
Both the tonal and motivic contrast between the opening sung melody, and the following part create a dissonance, not in a harmonic sense, but in a temporal, or symbolic sense.
The relation between the sung material, and the organ material is not yet clear.
Various alternatives to the lilting rising motive were tried for the introduction.
The original intention was to write something that felt like a classic song introduction in the tin pan alley tradition.
In the end, none of my more elaborate introductions seemed as appealing as the stark simplicity of the original rising motive.

\customincludegraphics[width=\textwidth]{3-3-1_system}{Sung melodic fragment introducing lilting ascending motive}

\customincludegraphics[width=\textwidth]{3-3-2_system}{Organ introduction, referencing the rising motive, but leaning towards A minor instead of B minor, which then leads into the main, lamenting melody at the end of the system.}

As for the composition of the descending motive, the first four measures came to me around the same time as the rising motive, around september, 2022, when I first started as organist at l'église Saint-Édouard and started my masters program at l'Université de Montréal.
Throughout the following months, I found myself adding on pieces to this melody, and finding new variations.
Once I decided to move in the direction of song form, I tried to crystallize the melody into a version that would fit within the bounds of an 8, 12 or 16 bar section I did this by taking all my motivic fragments and shuffling them around in different orders until I found a version that felt like it had a logical progression.
In the end, the idea seemed to naturally solidify into 12 bars.

At first, the continuation of this movement was supposed to simply dissolve into a textural mass, as the sound of the voix humaine undergoes it's painful transition into a bell.
This was the working idea until I realized that variation form, a form that I was looking to incorporate into the piece at some point, would be particularily appropriate for song form, as there is a great jazz tradition of playing variations on traditional songs.
In order to facilitate these variations, and to reference the jazz tradition, I decided to find jazz chords that could harmonize the original melody.
The original accompaniment is with an octave ostinato, and didn't lend itself well to variations in the jazz style, leading to an alternative harmonisation.

\customincludegraphics[width=\textwidth]{3-3-3_melody}{Descending "lament" motive, with jazz changes annotated above}

The obvious and simple choice would have been simply to simply repeat the twelve bars over and over again, but I felt that this simple repetition wasn't necessarily in line with the subject matter, devolving into the God river of blood.
I was also hearing continuations in 3/8, and wanted to find a way to alter the meter, while keeping in line with the variations idea.
It then came to me to incorporate exponential variations.
The idea is that each variation is either dilated, or expanded by a certain proportion.

In this case, I chose the proportion of 5/4, finding that a proportion of 2/1 wouldn't give me the metric variety I was looking for, and 3/2 would be too long.
5/4 seemed to give me the right amount of variety without being overly long.
In order to verify this, I drafted a patch in OpenMusic that calculates the total time of a given number of measures with a given time signature at a given tempo.
With 12 measures at a tempo of quarter note equals 64 beats per minute, I found that with four variations, the movement would take about 4.3 minutes, not including the introductory material.
This seemed like a reasonable length, and gave me the time signatures of 5/4, 25/16, and 125/64.

Obviously, these latter two are all but unreadable to mere mortals, and required some clever compartmentalization to yield a musical, communicative notation.
I decided to break up 25/15 into three measures of 3/8 plus one measure of 7/16.
The 125/64 was more problematic.
After many attempts, including a systematic attempt to find every possible decomposition of the fraction, using a site similar to this one: https://www.mathcelebrity.com/decompose-fraction.php?num=125%2F64&pl=Decompose, only to find that all possible combinations would need a measure with a denominator of 64.
Using 64th notes as the rythmic pulse is not particularily practical, so my first instinct was to round to the 32nd note, but I then considered the possibility of incorporating a metric modulation.
Through some trial and error, I found that making five eighth notes equal to the new quarter note would allow me to use a measure of 2/4 plus a measure of 9/32 to equal my 125/64 beast.
9/32 is still not a particularily common time signature, but the grouping of three 32nd notes in compound time makes it more tenable.

\customincludegraphics[width=\textwidth]{3-3-4_system}{First variation, in 5/4, with constrained elements notated in red, and free, optional element notated with small noteheads}

\subsection{Linking}

As the movement concludes, the sound of the bells gradually fades, transitioning into the sound of singing glasses.
This transition not only serves as a bridge to the next movement but also symbolizes the continual evolution of the thematic elements–-from the tangible to the ethereal, from the known to the unknown.

\section{4e Élégie}

\epigraph{\textit{Vous, Arbres de la Vie, oh! quand donc hivernaux? Nous n'allons pas à l'unisson.}}{}

\subsection{Narrative context}

On the one hand, this poetic fragment speaks to the trees of life.
The object that these trees symbolize is not clear.
Are they the angels, the readers of the poem, or perhaps life itself?
In any case, the poem then places the trees in the inevitability of the impending winter.
This winter can be read as an intimation of death, or at least a dormant state.
The musical term \textit{'unisson'} and its deliberate negation in the next sentence express a sense of divergence or disharmony, which becomes a central theme in the musical interpretation.

From a musical standpoint, I wanted to set the listener in the winter, which for me entails a sense of space and vastness, of stillness.
To evoke this environment, I thought it would be appropriate to use sounds with slow attacks and decays, to create a sense of slow evolutive timbre.
As I had been working on the writing of Jardin de Givre, which made extensive use of singing wine glasses, and I had high quality chromatic samples of these wine glasses on hand, I decided to create a sampler out of these recordings, so that I could play the wine glasses on my midi keyboard.
This also presents a pun, as the french word for ice is glace, which is a homophone with glass, the material of the wine glasses, evoking the ice covered ground of the desolate winter landscape.
As far as the second part of the poetic fragment, I decided that this would be a good moment to bring back the glissando motive from the first movement, this time moving in contrary motion.

\subsection{Techniques}

Software:

In this movement I make use of sampling for the first time, in two senses.
In one sense, with a sampler VST in Ableton Live which uses some recordings of singing wine glasses recorded in my apartment.
This sampler is driven by my midi keyboard and can be played chromatically.
On the other hand, I have a longer sample that is not played on the keyboard, but is simply triggered, acting as a bed track, to support the finale of the movement, creating additional tension and filling the space.
Lastly, in Pyo, I decided to revisit the glissando motive from the first movement, this time rather than moving all the partials in the same direction, moving the even partials higher in pitch, and the odd partials lower in pitch, creating a kind of contrary motion, while at the same time exploding the harmonic series.
This process refers to the text "Nous n'allons pas à l'unisson", creating a sonic divergence in opposing directions in pitch space.

\begin{figure}[H]
\begin{lstlisting}[language=Python]
def glissCont():
    global glissC
    for i in range(len(glissC)):
        glissC[i] == 0
    if glissC[0] < 600:
        stop1.setTrans(glissC)
        for i in range(len(glissC)):
            if i % 2 == 0:
                glissC[i] = glissC[i] + 0.4
            else:
                glissC[i] = glissC[i] + -0.4
    else:
        for i in range(len(glissC)):
            glissC[i] = 0
    print("0", glissC[0])
    print("1", glissC[1])
\end{lstlisting}
\caption{The function glissCont defines a glissando where each even partial is gradually incremented, whereas each odd partial is gradually decremented. This function is later called by a Pattern object 10 times a second}
\end{figure}

Compositional :

Several compositional constraints and techniques are explored in this movement.
On the one hand, the application of an isorhythm as cantus-firmus, which is placed in the bass part but is played with the right hand on the midi keyboard.
The right hand is very much not accustomed to playing the bass part, and so this represents a sort of étude.
Seeing as the keyboard can't easily be switched from one side to another, and playing it with the left hand would make it impossible to play the acoustic organ with the right hand, this provides a unique limitation and possibility for new cognitive and proprioceptive pathways.

I also wanted to develop the ascending by thirds motive, and decided to use it as an ostinato that begins slowly, later becoming a fast arppegiation, which in turn becomes the accompaniment for a variation of the melody of lamentation.
This provides a first attempt of the synthesis of these two motives, so central to the composition as a whole.
The arpeggiations also form, both visually in the score, and sonically, a set of arches.
These arches are symbolic of the physical arches of l'église Saint-Édouard and represent the broader iconography of the church.

\subsection{Composition Process}

This movement begins with an evocation of winter's vast stillness.
Long, monophonic singing glass samples with gradual attack and decay, unachievable on a traditional pipe organ, form the foundation.
These melodies are particularily centered on different combinations of the minor third, referencing both the initial melody and the ascending motif from the third movement.
These minor thirds are explored and combined with other intervals to form a fairly free and wandering section with ambigous tonality.

\customincludegraphics[width=\textwidth]{3-4-1_system}{Free monophonic section, using singing bell samples with long attacks and decays.}

This melody uses proportional notation, to add to the metric freedom of the evolutive sounds. Later, a meter of 4/4 is introduced, and as the movement progresses, the intial long melody is truncated into a chroma of 12 notes, set against a talea of four rhythms: a dotted half note, a half note, a dotted half note rest, and a whole note.
This cycle repeats twice, leading to a shift where the right hand retakes the treble pitches, and the bass moves to the pedals.
The introduction of an ascending triad motive ostinato in C minor follows, with an audio file triggered to reinforce the ostinato, pitch-shifted an octave higher for a brighter sound.
However, just as C minor becomes established, the movement shifts to G minor.
A low G in the bombarde from the bed track cues rapid arpeggiations in the left hand, while the right hand introduces a descending melody reminiscent of the lamentation motif from the third movement.
This section, with its faster left-hand figurations and the bed track's added tension, can be likened to a winter storm, symbolizing chaos and intensity.

The crescendo pedal is used strategically to create large-scale dynamic waves, swelling and fading over approximately four-measure intervals.
In the final phase, the MIDI keyboard transitions from singing wine glasses to glissandi in contrary motion, culminating in a single sustained chord that fades into an ambient mass.

\customincludegraphics[width=\textwidth]{3-4-3_system}{The introduction of the arch motive, symbolizing the staggering arches of l'église Saint-Édouard}

\subsection{Linking}

The transition from the third to the fourth movement is seamless, moving from bell-like sounds to singing glass sounds.
This continuity, however, necessitates a deviation from the established structure of singing before each Elegie.
To address this, the melody of the fourth Elegie is placed after the fourth movement, followed immediately by the melody of the fifth Elegie.
This arrangement, while a stark contrast, serves a specific purpose: the first melody acts as an echo of the preceding movement, while the second heralds the beginning of something new.
The common thread between these contrasting elements is the shared instrument of the voice, providing a thematic and auditory link.

\section{5e Élégie}

\epigraph{\textit{Mais les errant, dis-moi qui sont-ils, ces voyageurs fugaces?}}{}

\subsection{Narrative context}

The fifth elegie dwells on the fleeting nature of life and the enigmatic journey of the soul.
This movement is situated between the third and fourth "stations" of the path.
Before it begins, the protagonist passes through the salle Morin, named after the first priest of l'église Saint-Édouard, where the hum of a refrigerator is heard, the complex sound becoming more and more intense.
Then the distant sound of a clock ticking comes to the forefront, dominating the texture and becoming more and more incessant.
As the intensity of the clock builds, the footsteps, which thusfar have always been near, suddenly begin to recede into the distance, calling the subjectivity of the listener/protagonist into question.
The sound of the dissolving footsteps against the growing intensity of the clock creates an eery dialectic, at once referenceing the non-unisson of the fourth movement, and the nebulousness of time in the fifth.
Just as the clock climaxes, it splits off into several iterations of itself, like time fractionning and dissolving into the fugal mass.
This is the third station on the journey through the church, paralleling the fifth and sixth stations of the cross.

Musically, the movement begins with the voice, singing the poetic fragment with a densely chromatic theme.
This theme is then explored at the organ, on the one hand with the cornet registration, which I associate with thick, incense filled air, and on the other hand, the pyo synthesis server doing stop interpolation, that is, transforming from one stop to another.
This creates a nebulous effect evokative of the voyageurs fugaces mentionned by Rilke.

It ends with a recording that represents the fourth station in the path, that of the furnace room.
Deep in the church's underbelly, we hear the opening of a heavy, sliding, steel door that gives way into the furnace room, gently clicking.
The use of the furnace at this point is a reference to the word fugaces, like the thick incense filled air, the fumes of the furnace spiral up into the sky above the church.

\subsection{Techniques}

Software:

In the origins of this project was a fascination with interpolation.
I find it very intriguing to take a sound that seems permanent, stable, fixed in place and historical context, and to transform it into something else.
This capability is one of the key advantages afforded to the use of digital synthesis.
A traditional pipe organ operates in a very binary fashion, and this is especially true of electro-pneumatic organs, in which pulling a stop simply sends a control voltage to open the valve to the rank or ranks in question.
Moreover, even with a tracker organ, where half stops exist and are exploited in some experimental music, they are not so much an interpolation of the timbral space of that stop and silence, but are more like a distortion of the normal register, much like overblowing a flute.
This is not to speak of the idea of morphing from one stop to another.
In order to do this with a traditional organ, one would need to transform the metal in real time.
It's simply not possible!

In the digital world, sound is not linked to physicality in such a direct way.
The loudspeaker opens us up to much more flexible ways of generating and manipulating air pressure waves.
Once I had developed my first iteration of the sound synthesis server in python, one of the first ideas that I wanted to try out was this idea of morphing between stops, as if the metal is being stretched or contracted in realtime.
There's not necessarily an objective way to go about doing this, and the most obvious solution would simply be to use a crossfade, making one stop quieter while another enters.
This would be akin to increasing the transparency of one photo while another increases in opacity.
I considered this approach, and when this visual analogy came to mind, I imagined another possibility, where each pixel moves to where it needs to be for the transformed state.
Technically, this doesn't really make sense, as it would necessitate that both the original and transformed image have the same collection of pixels, but in a different order, which would likely not be the case.
In reality the more appropriate analogy would probably be that each pixel stays exactly where it is, but gradually alters it's RGB values until it reaches the desired state.

In any case, this visualisation gave me the idea to manipulate the timbral space itself for my transformations, using the spectral profiles of each stop, and in particular the amplitudes of each partial, as my pixels--a sort of data primitive which can be morphed as needed.
I had already designed functions which instantiated the profiles of different stops.
The only functionality missing was to define a way to interpolate between these states.
After some research, it seemed that the most straightforward approach would be to replace the Sig() objects with SigTo() objects.
The former generates a constant value, and jumps to a new value when changed, whereas the latter object takes a second argument, that of time, which determines the rate at which it moves to a new value once set.

\begin{figure}[H]
\begin{lstlisting}[language=Python]
for i in range(len(part)):
    self.amps.append(SigTo(mul[i], time=self.ramp))
    self.att.append(SigTo(att[i], time=self.inter))
    self.dec.append(SigTo(dec[i], time=self.inter))
    self.sus.append(SigTo(sus[i], time=self.inter))
    self.rel.append(SigTo(rel[i], time=self.inter))
    self.envs.append(MidiAdsr(self.note['velocity'], attack=att[i],
    decay=dec[i], sustain=sus[i], release=rel[i], mul=self.amps[-1]))
    self.part.append(SigTo(part[i], time=0.2))
    self.snds.append(Sine(freq=(self.part[i]**self.partSc) * 
    (MToF(FToM(self.note['pitch']-0.15))) + Randi(-rand, rand, 5) + 
    self.trans + self.mod, mul=self.envs[-1]))
    self.mixed.append(self.snds[-1].mix())
\end{lstlisting}
\caption{This snippet from stop.py uses SigTo() objects to manage interpolation between values.
self.ramp is used to determine the rate of transformation of the amplitudes of the harmonics, and self.inter, short for interpolation, governs the changes between envelope parameters.
For the moment self.inter is not functional, due to MidiAdsr() not allowing live signal for it's parameters (the first value is used at instantiation).} 
\end{figure}

This way of spectral morphing can work well at both large and small time scales.
The former giving a sense of evolutive expansion or contraction, depending on whether the spectrum is brightened or dampened.
The latter has a more elastic effect, as if the sound is a physical object and bounces like a rubber ball.
For this movement, I wanted to automate the morphing process, while experimenting with a range of medium time-scales.
To do this, I drafted a function called stopInter() which simply draws a number between zero and three, at an interval between 1 and 10 seconds.
This number represents the bourdon 8', principal 8', voix humaine 8', and cornet 5Rgs.
stops.
Each time a new time interval is selected, the ramp is changed accordingly, creating a sense of spectral instability and constant shifting, like the smoke of our "voyageurs fugaces".

\begin{figure}[H]
\begin{lstlisting}[language=Python]
stopInterPRand = Sig(1)
def stopInter():
    global stopInterPRand
    x = randint(0, 3)
    stopInterPRand.value = randint(1, 10)
    stop1.setRamp(stopInterPRand)
    print(x)
    if x == 0:
        bourdon()
    elif x == 1:
        principal()
    elif x == 2:
        voixHumaine()
    elif x == 3:
        cornet()
    print('stopInter', stopInterPRand)

stopInterP = Pattern(function=stopInter, time=Sig(stopInterPRand))
\end{lstlisting}
\caption{The function stopInterPRand interpolates between the spectral composition of the bourdon, the principal, the voix humaine, and the cornet.
The pattern stopInterP triggers the function at a randomly chosen time interval between 1 to 10 seconds.
The interpolation is five seconds long, which is defined in the midi\_nav.py file (see GitHub respository).}
\end{figure}

Compositional:

The use of the word fugace in the poetic fragment strongly suggests he traditional form of the fugue.
This realization struck me well into the conception of this piece, long after the idea to use interpolation to represent the nebulous, smoky evocations, and presented several technical challenges.
In some sense, these two ideas are completely opposed.
The interpolation idea was fundamentally ambient and evolutive, whereas the fugue has historically featured clearly defined rhythms and melodic motives as a key element.
Moreover, the interpolation works best with the sustain pedal depressed, and with a wide spread of intervals, to create an immersive resonance, but the fugue theme, which naturally sprung from the text, is tightly weaved and chromatic.

My solution to these problems is to treat these two ideas as largely distinct formal elements.
I view the more traditional fugal writing like a thread, or a dimly lit path, which at times becomes shrouded in darkness.
This "lost path" is represented by the ambient, evolutive sections using stop interpolation.
It is indeed as if our travellers give the impression of going somewhere, only to vanish in smoke.
At the same time, it has a psychological effect, as if following an idea, only to become lost in thought.

For these ambient sections, I allow myself to use non-fugal material, but in order to fuse the two ideas, I do reference the fugue theme from time to time, though with some important alterations.
In order to create an ambient texture, I generally apply augmentation to the durations, and to avoid extreme chromaticism, I use a technique borrowed from the fugue of the sixth movement of Messiaen's Vingt Regards sur l'enfant Jesus.
In this movment, Messiaen uses octave displacement throughout to disguise his subject.
Though in a very different paradigm, I use this technique to lessen the dissonance of the texture, and open up the resonance during the ambient sections.
I displace the notes according to the closest partiel in the spectral analysis of the bell of l'église Saint-Édouard.
This gives me a sound that is based in a physical object, tied to the space itself, and yet which maintains a level of chromaticism, yielding an uneasy tension, like difficult dreams.

\subsection{Composition Process}

\subsection{Linking}

\section{6e Élégie}

\epigraph{\textit{Figuier, depuis longtemps déjà ce m'est un signe que presque entièrement tu te dérobes à la gloire des fleures...}}{}

\subsection{Narrative context}

To me this is the crux of Rilke's texts, and the key narrative moment of the piece.
It is like the voice of God acknowledging that the subject is not living up to their fullest potential, that they are shrinking before the greatness of their task.
It is the point in the story where the protagonist must face his mortality, questioning the possibility of continuing, feeling crushed under the weight of burden, as a small fragile child.
For this movement I wanted to make use of what I call the dissociated bourdon.
In this synthesis patch, the harmonic content, and the transient noise content (marking the attacks of notes), are treated independently, where the noise sounds on every note onset, but the harmonic content sounds every sixth onset.
This creates an effect which is haunting and fragile--largely rhythmic.
The sound world is based in F minor, which refers to the minor triad contained in the harmonics of the spectral analysis of the lowest bell of l'église Saint-Édouard, and fragmented bursts of sound are heard interspersed with the clicking of the transient sounds.
Interestingly, there would have been a fuller contrapuntal message, but it is truncated, hidden, and lost to time.
This is a metaphor for the fig tree, hiding itself and dissociating from its truer form.
For this movement, I thought it would be appropriate to leave the stage, further dissociating the sound from the physical source.
This sort of theatrical element has interested me for a long time, and it provides a dramatic support to the narrative importance of this movement.

\subsection{Techniques}

Software:

The idea for the dissociated bourdon came as a result of an error of polyphony management when designing my initial synthesis program.
The issue was that my sound was separated into six channels, and instead of triggering each channel on each note event, pyo was cycling through the channels, and putting all the harmonic content on the first step.
The noise content was not subject to this polyphony issue and came through on each step, creating an intriguing texture, largely rhythmic, and punctuated with bursts of organ sound.
I quickly realized that I needed to sum the channels with a Mix() object before passing the sound to the output, but I was quite captivated by the result of this "happy error".
I kept that broken patch set aside, and soon after I tested it at the church, recording the result, and this is what ultimately became the bed track for this movement.

\begin{figure}[H]
\begin{lstlisting}[language=Python]
dissCount = 0
def dissocie(x):
    global dissCount
    print(x)
    if x != 0:
        dissCount += 1
        print(dissCount)
        if dissCount > 1:
            stop1.setMul([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
            print("set0")
        elif dissCount == 1 :
            stop1.setMul([1, 0.01, 0.1, 0.01, 0.07, 0, 0.02, 0, 0.01, 0, 0.003, 0,
	    0.003, 0, 0.001, 0, 0.001, 0, 0.001, 0])
            print("setnon0")
    if dissCount == 4:
        dissCount = 0
\end{lstlisting}
\caption{The function dissocie mutes the harmonic content of the organ synthesis every five notes.
For the other four attacks, only the noise of the transient and the air is heard.}
\end{figure}

Compositional:

Compositionally, the piece is largely freely improvised.
I instinctively reached for a pan-tonal palette, which seemed to adequately represent a sort of pathetic nature.
In this case, my improvisation was based in F minor.
This was not planned in any meaningful way at the time, but in retrospect, it is an appropriate choice, as the spectral analysis of the bells of l'église Saint-Édouard reveal a duality of F major and F minor.
My piece attempts to put this relationship in dialogue, and the fact that structurally, at the point of great doubt, the tonality arrives in F minor, feels thematically consistent.

\subsection{Theatrical elements}

Prior to the sixth movement, there are already present several important, though more subtle, theatrical elements.
At the beginning of the piece, the fact that I climb the stairs to arrive at the organ while the initial bed track plays can be seen as a theatrical staging.
From the perspective of the audience, they do not necessarily know whether this is simply a practical necessity, or a dramatic element--in other words, it is unclear whether the piece has already started during the bed track, or whether my arrival at the organ console initiated the beginning.
My sense is that initially, the natural interpretation is likely that the piece starts when I arrive, with the bed track serving as an introduction, but that in retrospect, once more theatric elements are introduced, the beginning of the piece is reinterpreted as having already begun without the audiences' knowledge.
Besides the initial entrance, the movements from the organ console to vocalise are a form of staging, though fairly restricted in scope.

In the sixth movement, I underline the narrative importance of this movement with the introduction of a much more marked theatrical element.
I leave the stage at the end of the fifth movement, before I do, triggering an audio clip which on the one hand transitions from the fifth movement with recordings of the final chord doing the stop interpolation, in order to give me time to leave the stage and enter the room without too much dead space.
Then, the audience hears a disembodied voice before the entrance of the dissociated bourdon.
This yields a sensation of alienation, highlighting the tension between the hopelessness of our protagonist and the call to a higher state of being.

Originally, I imagined that I would play the keyboard for this movement off stage, but this was a point of frustration, because it was technically infeasible.
As the keyboard is connected to my interface by a cable, it would require managing a very long cable, or having a separate setup with another computer, interface, and keyboard in another room, for only one movement.
Ultimately, in discussing the options with my co-supervisor Caroline Traube, we devised the plan to have this movement be entirely pre-recorded.
In this way, something is undeniably lost.
The immediacy of music performed in real-time is replaced with the automaton.
At the same time, from the narrative perspective, this is even better.
The timbral dissociation is linked with the physical disembodiment of the performer and their sound.
Since the performer is in another room, the audience is left to wonder what the source of this sound is.
Is the performer playing it from somewhere?
Or is it pre-recorded?
They are not told ahead of time so they are left to ponder this, putting the role of presence and physicality in music-making in question.

Just after the ambient, evolutive final chord from the fifth movement fugue ends and the dissociated bourdon of the sixth movement begins, the performer sings from another room sings an inversion of the principal melody, but begins with an incessant loop of the first gesture, "Figuier", as if frozen in time, or in thought.
As this syllable is uttered, the closed door of this room is slowly opened, mimicking the opening of the jalousies of an organ enclosure/expressive box.
After this disembodied voice is heard from afar, the recording of the dissociated bourdon enters.
Originally I considered triggering this recording with OSC, but during a testing of various techniques and excerpts from the Élégies, I found that the internet connection was not reliable enough to make this a good solution, and instead planned to simply time the sung material approximately with the space left between the end of the fifth movement and the sound file trigger of the sixth.

At the same time that I was experimenting with the role of physicality, and space, I wanted to increase the poignancy and effectiveness of this dramatic effect of alienation by further exploring of the auditory space, not only relocating the performer, but also the sound itself.
Instead of using the same speaker beside the pipe organ that has been used thus-far, a separate sound system located in the second balcony in the back of the church where the organ originally stood sounds the dissociated bourdon recording.
At the same time, a flashlight with a cold light is illuminated in this same space.
From a technical standpoint, this in theory could be controlled with software, LEDs, and a cabled connection, but it was more practical to simply ask a friend to stay in the balcony with a flashlight during the performance, awaiting the auditory cue, which is the end of my singing.
The friend not only turns on the light, but also triggers the recording, ensuring that they are well timed and synchronous.
Seeing as the light is behind the audience, it's possible that this won't even be noticed by certain audience members.
At the same time, seeing as the performer has left the stage, they will no longer have a visual aspect to attract their attention, and in the confusion of identifying their sound source, most of them will instinctually look behind them to find it, and doing so will see the light.
This simple visual elaboration signifies on the one hand, the simple, fragile, yet constant spiritual presence of our protagonist, symbolized as a fig tree, and simultaneously represents the phantom of the organ in its original home, now vacant.
It also invokes the separation of the traditional physical performer and the mechanized, automated version--the ghost in the machine.

\section{7e Élégie}

\epigraph{\textit{Non, plus d’imploration, voix maintenant mûrie, plus de clameur...}}{}

\subsection{Narrative context}

Following the fragile unsurety of the sixth movement, the seventh represents a call to action.
It is as if our protagonist, half awake from their frozen dream, summons themself with an urgent plea.
It isn't clear whether this plea is from within, or whether it comes from some external source, but it seems to imply some impending event.
To me this symbolizes a calm before the storm, a rising tension before the final climax.
I explore this tension is several ways.
On the one hand, the performer is still not present at the instrument, but makes several short appearances elsewhere in the church, creating a sense of uneasiness as these appearances are on the one hand unexpected, and as the space is relatively dark, the identity of the figure will be hard to determine, giving the impression of a multiplicity of our protagonist, none of which are wholly embodied, only existing for several moments, like many transient ghosts.

In several ways, the last four movements are merged into one.
In the most pragmatic terms, there is no break in between, unlike the preceding movements.
Narrative elements such as the delay from the ninth elegie is used at the end of the seventh movement in the form of an antiphonic echo, and the singing of hymns of praise from the text of the tenth elegie is evoked in the beginning of the ninth elegie.
This fusion of formal elements parallels the spiritual awakening of Rilke's poetry.
Attempts will be made throughout these last sections to carefully clarify where these overlapping elements are in play.

\subsection{Technique}

The techniques of this movement are different than all the preceding movements.
Instead of being based on synthesis in python, they are based in the electroacoustic domain.
Samples of bells, of the organ, and of the dissociated bourdon are transformed through pitch shifting and envelope shaping.
The sampling of the bells in particular, done using Ableton's Sampler plugin, attempts to take a sonic artefact, which is though inharmonic, remarquably stable.
That's to say, in a physical sense.
The carillon is in fact the heaviest instrument in the world by a large margin, and the thought of a pitch bend with the carillon would be unthinkable, for instance.
Unlike the voice, which is extremely maleable and versatile, the carillon is rooted and unmoveable.
The application of pitch envelopes in particular, gives the carillon a more organic quality, as if this massive, suspended, titan, is given liquid form, called down from the heavens to express itself in a vocal way.

Another key element is a sample of the fire alarms of the church, which sounded one day while I was recording and experimenting with dissonant clusters.
The sound surprised me, and I wasn’t sure what it represented at first.
I considered stopping to investigate the sound, but decided to continue.
This alarm gives a sense of dramatic tension and impending, appropriate with the poetic fragment.
Towards the end of the piece, the sound of footsteps, recorded in l'église Saint-Édouard with my Zoom H4N recorder, become more and more densely layered.
These footsteps were recorded by myself, as I ran towards the microphone.
The intention was to accent this sense of urgency, playing off the footsteps motive that has been heard intermittently throughout the piece, yet now completely decoupling the subject and the sound source, creating an almost cinematic effect, where we are no longer following over the shoulder of the protagonist, but we are suddenly viewing the protagonist, or some person or animal, which is running towards us.
This sound is spatialised in all five speaker, representing the first time since the introductory pad since all speakers are used.
The sound is passed from speaker to speaker, providing the sensation of being encircled by the sound.

These running footsteps were recorded in two sessions, and in the first session, I had previously been attempting to get a near-infinite reverb without feedback, trying different microphones, and different decay intervals, and most of the feedbacks quickly became out of control, needing to be shut off.
One of the feedbacks however, which was less of a feedback of the organ, and moreso of the space itself, as in my recollection I hadn't played a note as I had in other experiments, established a sort of stasis, neither growing nor diminishing, remaining in a constant periodic throbbing.
I liked this sound so much, that I decided to try recording the sound of the footsteps overtop of it.
This experience of running and accelerating towards the microphone, through the central aisle of the church, gave me an eerie sense of traversing a portal, a strong wave of shivers going up my spine each time I passed the microphone.

\subsection{Composition}

After the sixth movement, the light goes out leaving the audience in the dark and the silence for a moment.
Out of this silence, an electroacoustic piece begins using mainly samples of the church bells, but mutated with pitch and dynamic envelopes, and a sample of the fire alarms of the church, which began one day while I was experimenting with dissonant clusters.
The sound surprised me, and I wasn’t sure what it represented at first.
I considered stopping to investigate the sound, but decided to continue.
The alarm gives a sense of dramatic tension and impending, appropriate with the poetic fragment: “Non, plus d’imploration, voix maintenant mûrie, plus de clameur.”.
This text suggests a sense of immediacy for something, but this something is not clear.
The alarm helps with establishing this sense of a calm before the storm, and the thick clusters of organ chords create a sense of ominous tension.
This movement reaches a sort of energetic climax, where the sound of urgent footsteps are heard traversing the church and encircling the space, becoming layered and more and more dense.
At the same time, the throbbing of a sound of feedback that mysteriously appeared in the church one day when I was testing the reverb adds to the sense of tension.
Both the footsteps and the throbbing feedback represent the fifth station in the stations of the cross, (each station represents two actual stations, so this would correspond to the 9th and 10th stations), which corresponds to me entering the church from the exterior.
This movement then reaches a stage of quietude, with simply a low pulsating drone and the fire alarm and rumblings far off in the distance.

\subsection{Theatrical elements}

During this movement, the performer remains off stage.
The intention with this decision is to continue the sense of disembodiment established in the sixth movement, and whereas the musical element in the sixth could imaginably be played on a single instrument, due to its minimal, soloistic nature, the seventh movement is so densely layered that it is clearly an electroacoustic piece.
This has the effect of dissolving even further the relationship between the sound source and the physicality of the performer.
The audience is no longer imagining the musician's hands, but is now lost, in a dreamlike way, in visions, symbols, and memories.
This mirror's the effect of the footsteps described in the previous section.
Just as the sound of the footsteps approaching the microphone creates a sense of embodied experience, rather than simple voyeurism, the aural landscape is transformed from one that is voyeuristic and observational, to one that is more interiorized and subjective.
During this movement, rather than resting off stage as in the sixth, I traverse the space of the church, appearing briefly several times, first in the north entry of the east side of the church, and then in the balcony opposite to the organ (the south-transept).
During both of these apparitions, I appear, remain completely immobile for a short time, and then disappear.
The strangeness of these appearances, and the disconnect between the sonic experience and the human form, increases the sense of disembodiment and general tension, and parallels the themes of hauntology, acting as a phantom presence.
Due to the relatively dark conditions, my identity will be unclear, and audience members may question whether it is me or someone else, as if the phantom is duplicated, or transported throughout the space.

Once the music sonically reaches the low pulsating drone, I reenter the stage, putting on a lavalier microphone on, and approaching the organ.
In my mind, the moment I begin to play the organ represents neither the seventh nor the eighth movement, but rather a liminal space between the two.
The sound of the organ, based on the ascending motive, is echoed on the opposite balcony (the south transept), using a delay line in Ableton which is fed into a speaker placed on the opposing balcony.
This echo is reminiscent of the call and response practices of african and african diaspora, as well as the ancient practice of antiphony, where two choirs would be located in different parts of a church and would dialogue between them.
This echo is also evocative of several themes of the piece: the echo of history, as the historic practice of antiphony is reinstated in a modern context, and the spiritual aspect of the echo being like a recursion of subjectivity—the voice speaking to itself; the eye staring back into itself; the void rushing in to fill itself.
This echo, digitally rendered in Ableton, solves the problem of not being able to play the acoustic pipe organ from the other side of the church, and inherently integrates the antiphony idea with the technological experiments.
At the same time, the possibility of musically altered responses, like responding with a different harmony, is lost.

\section{8e Élégie}

\epigraph{\textit{A plein regards, la créature voit dans l’Ouvert.}}{}

\subsection{Narrative context}

\subsection{Techniques}

\subsection{Composition Process}

\subsection{Linking}

\section{9e Élégie}

\epigraph{\textit{Pourquoi, s’il est loisible aussi bien de remplir son délai d’existence en laurier, sombre un peu plus que tous les autres verts, avec ces vaguelettes...}}{}

\subsection{Narrative context}

The ninth elegie is a departure from the patterns established in the previous movements of "Élégies," both in its structure and thematic content. 
This movement begins with text drawn not from the ninth elegie, as one might expect, but from the tenth. 
This choice emerged from a deliberate consideration of how to integrate Rilke’s dense and multifaceted final elegy into the musical narrative. 
Musically, the tenth elegy seemed to demand a reflective, non-vocal resolution, serving as a calm after the storm of the preceding movements. 
To maintain the established pattern of incorporating sung lyrics in each movement, the text of the tenth elegy is instead woven into the fabric of the ninth.

Unlike the other elegies, which each make use of the opening words of Rilke's poems, the lyrics from the 10th elegy are taken from from various parts of the 10th elegy. 
This reconstruction is a fragmented mosaic, creating a tapestry of incomplete thoughts and potentialities, mirroring the themes of joy, grief, and longing in Rilke’s poetry. 
This movement, like the text it draws from, grapples with the fear that accompanies happiness--the knowledge that it is transient and inevitably fleeting. 
Musically this movement emerges directly from the eighth, with the beat and baseline that arrive at the end of the previous movement as a link.

\begin{figure}[H]
\begin{verse}
Vienne le jour, enfin sortant.\\ 
La pluie qui vient tomber dans les venelles de la Cité,\\ 
prête à se briser. Mon visage resplendissant, baigné.\\ 
Aux Anges qui l'agréent, des marteaux du cœur.\\
\end{verse}  
\caption{My reconstruction of poetic fragments from Rilke's 10th elegy} 
\end{figure}

\subsection{Techniques and compositional considerations}

The ninth elegie begins as the voice joins the sparse texture of the sample based beat against the baseline in the pedals that ends the eighth elegy. 
The aesthetic of this section is perhaps the closest to a UK dubstep sound, with the throbbing bass, skittering rhythm, and plaintiff, repetetive vocal line recalling Archangel of Burial. 
The repetitive, plaintive vocal line "Vienne le jour, enfin sortant," sung in D minor, is amplified and processed with a vocoder. 
This technique uses the original church bell sample as the carrier, creating a synthesis between the expressive human voice and the immutable sound of the carillon.

\customincludegraphics[width=\textwidth]{9e_1}{The entrance of the voice over the sparse texture of pedal baseline and sample-based rhythm.}

After the initial section, the vocal line drops out, leaving the bassline and beat to carry the momentum for six measures. 
A new idea then emerges in A minor, where the melody of lamentation is transformed from its original 17th-century idiom into a modern, R\&B-inspired expression. 
This transformation is achieved through the use of parallel minor 7 chords and a more pop oriented vocal style. 
The lyrics "La pluie qui vient tomber..." are sung over three iterations of this melody, each iteration progressively higher and more intense, with the organ accompaniment shifting from the positif to the grand orgue.

Following these iterations, the beat is gradually lowered in volume, moving further into the background until it becomes a distant echo, reminiscent of the groanings and clatterings of the church or the city. 
This shift in dynamics allows the organ to take center stage, presenting a solo variation of the "melody of lamentation" in D minor. 
This variation involves a tonicization of G minor, leading into a descending-third, ascending-second sequence, further deepening the sense of unresolved questioning and yearning.

\customincludegraphics[width=\textwidth]{9e_2}{The transition from the second vocal melody into the extended solo organ version with the rhythmic loop in the background.}

As the beat fades out, the 17th trigger initiates the sound of rain, taken from the same recording used in the opening of the piece but focused on the rain rather than the thunder. 
The melody of lamentation reappears, this time simplified into a descending diatonic line in augmentation, stretched to one note per measure. 
This is a direct reference to the fourth movement, though the roles of the hands are now reversed: the left hand plays the melody, while the right hand takes up the arpeggiated "spire" motive. 
Over this framework, the voice re-enters, singing an incomplete question to the assenting angels: "Pourquoi, s'il est loisible aussi bien de remplir son délai en laurier. 
Pourquoi s'il est loisible."

\customincludegraphics[width=\textwidth]{9e_3}{The lament melody simplified and quantized to whole notes in the left hand, with the arch theme in the right hand, with voice.}

\section{10e Élégie}

\epigraph{\textit{Vienne le jour enfin, sortant de la voyance encolérée, où je chante la gloire et la jubilation aux Anges qui l’agréent. Que des marteaux du cœur au battement très clair aucun ne vienne à faux tomber sur une corde molle, ou encore douteuse ou prête à se briser.}}{}

\subsection{Narrative context}

The tenth elegy stands as a contemplative epilogue, a reflective meditation that diverges from the narrative drama of the previous movements. 
This elegie uniquely omits the human voice, a deliberate choice that carries poetic significance. 
The absence indicates otherness--a distance that suggests the elegie is not participating in the unfolding drama but is instead observing, reflecting, and internalizing the previous events. 
This silence points to the idea that the text of the tenth elegy is not meant to be embodied or expressed outwardly but is rather an internal, imagined experience.

Rilke’s final elegy grapples with the themes of joy, grief, and the haunting realization that our most profound desires and potentials may remain forever unfulfilled. 
The decision to relegate the text of the tenth elegy to the ninth movement, where it is fragmented and distorted, introduces a layer of ambiguity. 
This dislocation raises the question of whether the events depicted in the ninth elegy truly transpired or if they were merely a projection of the protagonist's inner world—a dream of fulfillment that remains just out of reach.

\subsection{Technical and Compositional Considerations}

The tenth elegie opens with the ethereal sound of reversed bells, immediately setting a reflective tone. 
This is layered with a shimmering plein orgue sound, executed through a two-handed tremolo technique with each hand playing the same chord an octave apart. 
The combination of tremulants and a 50ms delay adds a fluttering, almost dizzying texture that mirrors the swirling thoughts of reflection and introspection. 
Beneath the active surface, the harmonic progression moves slowly, repeating IV-V-I cadences in F major and Ab major. 
This harmonic ambiguity reflects the coexistence of F major and F minor in the spectral composition of the bells.

\customincludegraphics[width=\textwidth]{10e_1}{A tremolo between the two hands, with both tremulant motors active, and a slight delay.}

\customincludegraphics[width=\textwidth]{10e_2}{A 6/5 5/3 sequence, followed by the last iteration of the initial theme of the piece.}

This section ends with a 6/4 5/3 sequence followed by a final, reharmonized version of the initial theme firs heard in the voice in the first movement. 
The last trigger then turns on a long reverb effect of about 60 seconds which attempts to augment the already significant reverb of the church to something that is essentially frozen in time. 
To represent this timelessness, the notation dissolves into proportional notation, and several frozen, falling by step motives are heard, as the final audio recording is heard: a montage of the sounds of pigeons speaking and fluttering their wings, at first in realtime and gradually slower, combined with a recording of myself coming from the basement of the church and ascending the stairs to the organ loft. 
Because the microphone was placed at the bottom of the stairs to the organ loft, and at the top of the stairs from the basement, my steps are heard approaching the microphone, and then moving into the distance, creating a sense of mystery. 
On the one hand our protagonist has ascended, like the ascension mentioned in the last passage of Rilke's Elegies. 
At the same time, what this ascension represents is unclear.

\customincludegraphics[width=\textwidth]{10e_3}{The final moment of the piece, with a repeated descendng motive with frozen reverb against the sound of pigeons and footsteps.}

%%--------------%
%%     index    %
%%--------------%

%% S'il y a lieu, décommenter la ligne pour mettre votre index

%%\printindex

%%------------------------------------------------- %
%%         références --- bibliographie             %
%%------------------------------------------------- %
  % Enlever les commentaires de la prochaine commande si vous préférez que le
  % chapitre s'appelle « Références » plutôt que « Bibliographie » (au choix selon le contexte).
%%\let\bibname=\refname

%% Lorsque vous serez prêt à faire afficher votre bibliographie
%% et vos références, enlevez les commandaires des commandes suivantes
%% et donnez le nom de votre fichier .bib à la commande \bibliography{..}
%% (consultez l'exemple au besoin).  Vous pouvez utiliser le style de votre
%% choix.
%%\bibliographystyle{plain}     % Le style de la bibliographie. Notons que
                                        % les extensions ne sont pas données pour ces deux fichiers.
%%\def\bibname{R\'ef\'erences bibliographiques} % Nom obligatoire de la section des références.
                                              % On utilise \'e car le é cause des problèmes
                                              % dans la table des matière
%% ENGLISH
%%\def\bibname{References}
\printbibliography
%%\bibliography{ref}     % La base de données contenant des entrées bibliographiques.
                                    % Seules celles référencées dans le texte seront ajoutées
                                    % \`a la bibliographie.

%%------------------------------------------------- %
%%                  Annexe A                        %
%%------------------------------------------------- %

\appendix
\chapter{Le titre}

\section{Section un de l'Annexe A}

...texte...

\chapter{Les différentes parties et leur ordre d'apparition}

J'ajoute ici les différentes parties d'un mémoire ou d'une thèse ainsi
que leur ordre d'apparition tel que décrit dans le guide de
présentation des mémoires et des thèses de la Faculté des études
supérieures.  Pour plus d'information, consultez le guide sur le site
web de la facutlé (www.fes.umontreal.ca).

\newcount\colnum
\colnum=1
\def\i{\number\colnum. \global\advance\colnum by 1\ignorespaces}
\begin{table}[p]
  \begin{center}
    \begin{tabular}{|l|l|r|}\hline
       \noindent\hfil
         \textbf{\strut Ordre des éléments constitutifs du mémoire ou de la thèse}
         \hfil\span\omit\span\omit\\\hline % \span\omit pour couvrir plus d'une
                                           % case sans utiliser le package multirow ou autre
      \i &  La page de titre & obligatoire\\\hline
      \i &  La page d'identification des membres du jury & obligatoire\\\hline
      \i &  Le résumé en français et les mots clés français\kern3em& obligatoires\\\hline
      \i &  Le résumé en anglais et les mots clés anglais & obligatoires\\\hline
      \i &  Le résumé dans une autre langue que l'anglais & obligatoire \\
         &  ou le français (si le document est écrit dans &\\
         &  une autre langue que l'anglais ou le français)&\\\hline
      \i &  Le résumé de vulgarisation& facultatif\\\hline
      \i &  La table des matières, la liste des tableaux,& obligatoires\\
         &   la liste des figures ou autre &\\\hline
      \i &  La liste des sigles et des abréviations& obligatoire\\\hline
      \i &  La dédicace& facultative\\\hline
      \i &  Les remerciements & facultatifs\\\hline
      \i &  L'avant-propos & facultatif\\\hline
      \i &  Le corps de l'ouvrage& obligatoire\\\hline
      \i &  Les index& facultatif\\\hline
      \i &  Les références bibliographiques & obligatoires\\\hline
      \i &  Les annexes & facultatifs\\\hline
      \i &  Les documents spéciaux & facultatifs\\\hline
    \end{tabular}
  \end{center}
\end{table}

\end{document}

\endinput
%%
%% End of file `gabaritmem.tex'.
