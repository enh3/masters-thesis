%% Pour voir les accents de ce fichier, assurez-vous que votre
%% éditeur de texte lise le fichier en utf-8!

%% La classe <dms> est construite au-dessus de <amsbook>, donc
%% <amsmath>, <amsfonts> et <amsthm> sont automatiquement chargés.
%% Pour un mémoire
\documentclass[12pt,twoside,maitrise]{dms}
%% Pour une thèse
%%\documentclass[12pt,twoside,phd]{dms}

\usepackage[utf8]{inputenc} %Obligatoires
\usepackage[T1]{fontenc}    %
\usepackage{epigraph}
\usepackage{csquotes}
\usepackage{graphicx}
\usepackage[backend=biber,style=verbose]{biblatex}
\graphicspath{{./graphics/}}
\usepackage{caption}
%%\usepackage{chngcntr}
%%\counterwithout{figure}{chapter}
\usepackage{float}
\usepackage{listings}
%%\usepackage{courier}
\lstset{
    basicstyle=\fontsize{9}{9}\selectfont\ttfamily,
    showstringspaces=false
}

\DeclareCaptionFormat{custom}{%
    \setlength{\parindent}{0pt}% Remove the paragraph indentation
    %%\centered% Justify the caption text to the left
    \textbf{\fontsize{9pt}{0pt}\selectfont #1 #2}{\fontsize{9pt}{0pt}\selectfont #3}% Adjust the label formatting
}
\captionsetup[figure]{format=custom, labelsep=colon, belowskip=9pt, skip=9pt}

%% <lmodern> incorpore les fontes en T1, pour
%% faciliter le dépôt final. Ceci n'est pas la
%% seule option :
%%  1. Si cm-super est installé, vous pouvez enlever <lmodern>
%%     (à ce moment, la police est un peu plus fidèle
%%      au Computer Modern orginal);
%%  2. Si vous avez une police préférée, par exemple,
%%     <times> ou <euler> ou <mathpazo> (et bien d'autres),
%%     alors vous pouvez remplacer <lmodern> ci-bas.
%% Par contre, si vous faîtes face à un problème d'encapsulation
%% lors dépôt final, il se peut que la solution soit d'utiliser <lmodern>.
%% (Parfois le problème est au niveau de l'installation, donc
%%  essayez de compiler sur un autre ordinateur sur lequel vous êtes
%%  certain·e que l'installation est bonne.)
\usepackage{mathptmx}

%% Il n'est pas nécessaire d'utiliser <babel>, car
%% les commandes intégrées par la classe <dms>
%% \francais et \anglais font le travail. Néanmoins,
%% certains autres packages nécessitent <babel> (comme
%% <natbib>), donc simplement enlever les % devant <babel>
%% dans ce cas. Attention! Certains packages sont sensibles
%% à l'ordre dans lequel ils sont chargés.
%%\francais % or
%%\anglais
%%
\usepackage[english]{babel}

 % ENGLISH OPTION
 % If you call \anglais here before the \begin{document},
 % all the chater's header will be in english, even if you
 % call \francais. To change this, use
 % \entetedynamique

%% La commande \sloppy peut avoir des effets étranges sur les
%% lignes de certains paragraphes.  Dans ce cas, essayez \fussy
%% qui suppresse les effets de \sloppy.
%% (\fussy est normalement le comportement par défaut.)
%% On redéfinit \sloppy, pour tenter de réduire les comportements
%% étranges. Le seul changement apporté à la version originale
%% est la valeur de \tolerance.
\def\sloppy{%
  \tolerance 500%  %9999 dans LaTeX ordinaire, mauvaise idée.
  \emergencystretch 3em%
  \hfuzz .5pt
  \vfuzz\hfuzz}
\sloppy   %appel de \sloppy pour le document
%%\fussy  %ou \fussy

%% Packages utiles.
\usepackage{graphicx,amssymb,subfigure,icomma}
%% icomma       permet d'écrire les nombres décimaux en
%%                  français (p.ex. 1,23 plutôt que 1.23)
%% subfigure    simplifie l'inclusion de figures côtes-à-côtes

%% Packages parfois utiles.
%%\usepackage{dsfont,mathrsfs,color,url,verbatim,booktabs}
%% dsfont       symboles mathématiques \mathds
%% mathrsfs     plus de symboles mathématiques \mathscr
%% color        pour utiliser des couleurs (comparer avec <xcolor>)
%% url          permet l'écriture d'url
%% verbatim     pour écrire du code ou du texte tel quel
%% booktabs     plus de macros pour faire les tableaux
%%                  (voir documentation du package)

%% pour que la largeur de la légende des figures soit = \textwidth
%%\usepackage[labelfont=bf, width=\linewidth]{caption}

%% les 3 lignes suivante servent à l'affichage de l'index
%% dans le visionneur de pdf. <hyperref> et <bookmark>
%% devraient être les dernier package a être chargé,
%% donc chargez vos packages avant.
\usepackage{hyperref}  % Ajoute les hyperlien
\hypersetup{colorlinks=true,allcolors=black}
\usepackage{hypcap}   % Corrige la position du lien pour les images
\usepackage{bookmark} % Remédie à des petits problème
                      % de <hyperref> (important qu'il
                      % apparaisse APRÈS <hyperref>)

  % Enlever les commentaires du prochaine \hypersetup et
  % le remplir avec l'information pertinente.
  % Ceci ajoute des « méta-données » au pdf.  C'est optionnel,
  % mais recommandé. Vous pouvez voir ces méta-données en
  % ouvrant un visionneur de pdf et en cherchant les propriétés
  % du pdf. (Vous pouvez aussi tapez ' pdfinfo <nom-du-pdf> '
  % dans un terminal.) Ces données sont utiles, par exemple,
  % pour augmenter les chances qu'un algorithme de recherche
  % trouve votre document sur Internet, une fois diffusé.
%%\hypersetup{
%%  pdftitle = {Titre de la thèse / du mémoire},
%%  pdfauthor = {auteur·e},
%%  pdfsubject = {Ex: Transformation de Fourier ; régressions linéaires ; ... },
%%  pdfkeywords = {Ex: mathématiques, statistiques, groupes, variables aléatoires,...}
%%}

%% Définition des environnements utiles pour un mémoire scientifique.
%% La numérotation est laissée à la discrétion de l'auteur·e. L'exemple
%% illustré ici produit « Définition x.y.z »
%%   x = no. chapitre
%%   y = no. section
%%   z = no. définition
%% et la numérotation des corollaires, définitions, etc. se fait
%% successivement.
%%
%% Les macros \<type>name sont telles qu'ils suivent
%% la langue actuelle. (P.ex. si \francais est utilisé,
%% alors \begin{theo} va faire un Théorème et si \anglais
%% est utilisé, \begin{theo} fera un Theorem.)
%%
\newtheorem{cor}{\corollaryname}[section]
\newtheorem{deff}[cor]{\definitionname}
\newtheorem{ex}[cor]{\examplename}
\newtheorem{lem}[cor]{\lemmaname}
\newtheorem{prop}[cor]{Proposition}
\newtheorem{rem}[cor]{\remarkname}
\newtheorem{theo}[cor]{\theoremname}
\theoremstyle{definition}
\newtheorem{algo}[cor]{\algoname}
%% NOTE : Il peut être commode de redéfinir \the<type> pour
%% obtenir la numérotation désirée. Par exemple, pour
%% que les corollaires soit numérotés #section.#sous-section.#sous-sous-section.#paragraphe.#corollaire,
%% on fait
%% \renewcommand\thecor{\theparagraph.\arabic{cor}}

%%%
%%% Si vous préférez que les corollaires, définitions, théorèmes,
%%% etc. soient numérotés séparément, utilisez plutôt un bloc de
%%% commandes de la forme :
%%%

%%\newtheorem{cor}{\corollaryname}[section]
%%\newtheorem{deff}{\definitionname}[section]
%%\newtheorem{ex}{\examplename}[section]
%%\newtheorem{lem}{\lemmaname}[section]
%%\newtheorem{prop}{Proposition}[section]
%%\newtheorem{rem}{\remarkname}[section]
%%\newtheorem{theo}{\theoremname}[section]

%%
%% Numérotation des équations par section
%% et des  tableaux et figures par chapitre.
%% Ceci peut être modifié selon les préférences de l'utilisateur.
%%\numberwithin{equation}{section}
%%\numberwithin{table}{chapter}
%%\numberwithin{figure}{chapter}

%%
%% Si on veut faire un index, il faut décommenter la ligne
%% suivante. Ajouter des mots à l'index avec la commande \index{mot cle} au
%% fur et à mesure dans le texte.  Compiler, puis taper la commande
%% makeindex pour creer les indexs.  Après une nouvelle compilation,
%% vous aurez votre index.
%%

%%\makeindex

%% Il est obligatoire d'écrire à double interligne
%% ou à interligne et demi. On peut soit utiliser
%% le package <setspace> ou \baselinestretch.
%% Le package est un peu plus propre, mais le choix
%% reste à la discrétion de l'usager.
\usepackage[onehalfspacing]{setspace}
\addbibresource{ref.bib}
 % ou
%%\renewcommand{\baselinestretch}{1.5}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%                                     %%%%%%%%%%%%%
%%%%%%%%%% D é b u t    d u    d o c u m e n t %%%%%%%%%%%%%
%%%%%%%%%%                                     %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\entetedynamique

%%
%% Voici des options pour annoter les différentes versions de votre
%% mémoire. La commande \brouillon imprime, au bas de chacune des pages, la
%% date ainsi que l'heure de la dernière compilation de votre fichier.
%%
%%\brouillon
%%
%%
%% \version est la version de votre manuscrit
%%
\version{1}

%%------------------------------------------------- %
%%              pages i et ii                       %
%%------------------------------------------------- %

%%%
%%% Voici les variables à définir pour les deux premières pages de votre
%%% mémoire.
%%%

\title{Contemporary perspectives on the hyper-organ: conceiving, playing and writing for an augmented Casavant Frères pipe organ}

\author{Kjel Sidloski}

\copyrightyear{2024}

\department{Faculté de musique}

\date{\today} %Date du DÉPÔT INITIAL (ou du 2e dépôt s'il y a corrections majeures)

\sujet{musique}
\orientation{composition}%Ce champ est optionnel
%%
%% Voici les disciplines possibles (voir avec votre directeur):
%% \sujet{statistique},
%% \sujet{mathématiques}, \orientation{mathématiques appliquées},
%% \orientation{mathématiques fondamentales}
%% \orientation{mathématiques de l'ingénieur} et
%% \orientation{mathématiques appliquées}

\president{Nom du président du jury}

\directeur{Pierre Michaud}

\codirecteur{Caroline Traube}         % s'il y a lieu
%%\codirecteurs{Nom du 2e codirecteur}         % s'il y a lieu

\membrejury{Nom du membre de jury}

%%\examinateur{Nom de l'examinateur externe}   %obligatoire pour la these

%% \membresjury{Deuxième membre du jury}  % s'il y a lieu

%%  \plusmembresjury{Troisième membre du jury}    % s'il y a lieu

 % Cette option existe encore, mais elle n'a plus sa place
 % dans la page titre. L'utiliser seulement si le directeur
 % insiste...
%%\repdoyen{Nom du représentant du doyen} %(thèse seulement)

%%
%% Fin des variables à définir. La commande \maketitle créera votre
%% page titre.

\maketitle

 % Pour générer la deuxième page titre, il faut appeler à nouveau \maketitle
 % Cette page est obligatoire.
\maketitle

%%------------------------------------------------- %
%%              pages iii                           %
%%------------------------------------------------- %

\francais

\chapter*{Résumé}

...sommaire et mots clés en français...

%%------------------------------------------------- %
%%              pages iv                            %
%%------------------------------------------------- %

\anglais

\chapter*{Abstract}

This paper presents a body of work for the pipe organ exploring the emerging tradition of augmented instruments as pioneered by Tod Machover and others. 
The organ is being examined as an augmented instrument by a niche community including Lauren Redhead and the Orgelpark project, yet represents an underexploited resource of musical exploration, uniquely positioned as an embodied cultural artifact. 
The nature of an instrument that is embedded in its space poses the question of whether there is truly a distinction between the building housing the instrument and the instrument itself, yielding a situation in which augmenting the pipe organ can be seen akin to sonic architecture. 
The instrument examined in this work is the symphonic organ of l'église Saint-Édouard where the author is organist. 
Built in 1913 by Casavant Frères, this organ has a unique and eventful history. 
Being taken down from its original position in the west gallery, forgotten about and nearly abandonned, it was found by new administration nearly ten years later and reinstalled in the north transept where it sits today. 
I believe that the sound of this instrument tells the story of its heritage, and by studying its unique properties, we gain insight into the nuances of history. 
This project contains both an experimental and a creative component. 
The experimental component includes the creation of a sound synthesis module written in python called OrganLab, and a diffusion network, with five speakers placed throughout the church, in order to thoroughly explore the space. 
The creative portion involved the piece Élégies, inspired by the 10 Élégies de Duino (Rilke, 1986), which serves as a set of studies to explore the capabilities of the hyper-organ, according to three modalities mixed music: acoustic organ and synthetic organ, organ with processing, and organ with bed track. 
The church space is also considered in its entirety, invoking a spatial-narrative structure. 
Throughout the creative process, attempts have been made to not only extend the sonic capacity of the instrument, but to integrate with it, basing for instance, the sound synthesis on various organ stops, which can then be mutated in ways which would be impossible on the original instrument, creating a dialectic of real and artifical sound environments.

%%------------------------------------------------- %

%%        page v --- Table de matieres              %
%%------------------------------------------------- %

 % Pour un mémoire en anglais, changer pour
 % \anglais. Noter qu'il faut une permission
 % pour écrire son mémoire en anglais.
\anglais
%%\francais
 % \cleardoublepage termine la page actuel et force TeX
 % a poussé les éléments flottant (fig., tables, etc.) sur
 % la page (normalement TeX les garde en suspend jusqu'à ce
 % qu'il trouve un endroit approprié). Avec l'option <twoside>,
 % la commande s'assure que la prochaine page de texte est sur
 % le recto, pour l'impression. On l'utilise ici
 % pour que TeX sache que la table des matières etc. soit
 % sur la page qui suit.
%% TABLE DES MATIÈRES
\cleardoublepage
\pdfbookmark[chapter]{\contentsname}{toc}  % Crée un bouton sur
                                           % la bar de navigation
\anglais
\tableofcontents
 % LISTE DES TABLES
\cleardoublepage
\anglais
\english
\phantomsection  % Crée une section invisible (utile pour les hyperliens)
\listoftables
 % LISTE DES FIGURES
\cleardoublepage
\phantomsection
%% Il est obligatoire, selon les directives de la FESP,
%% pour une thèse ou un mémoire d'avoir une liste des sigles et
%% des abréviations.  Si vous considérez que de telles listes ne seraient pas
%% pertinentes (si, par exemple, vous n'utilisez aucun sigle ou abré.), son
%% inclusion ou omission est laissé à votre discrétion.  En cas de doute,
%% parlez-en à votre directeur de recherche, le coadministrateur ou au/à la
%% bibliothécaire.
%%
%% Le gabarit inclut un exemple d'une liste « fait à la main ».  Il existe des outils
%% plus sophistiqués si vous devez inclure une multitude de sigles et abréviations.
%% Par exemple, le package <glossaries> peut faire des index élaborés.  Comme
%% son utilisation est technique, il n'y a pas d'exemple directement dans ce gabarit.
%% On invite les gens qui aurait à l'utiliser à lire la documentation officielle,
%% soit en allant sur https://www.ctan.org/, soit en tapant dans un terminal :
%%
%% texdoc glossaries
%%

\chapter*{List of acronyms and abbreviations}
 % Option de colonnes: definir \colun ou \coldeux
%%% Exemple
%%% \def\colun{\bf} % Première colonne en gras
%%% Pour numéroté les entrées, on peut faire
%%% \newcount\abbrlist
%%% \abbrlist=0
%%% \def\plusun{\global\advance\abbrlist by 1\relax}
%%% \def\colun{\plusun\the\abbrlist. }
%%\def\coldeux{\relax}
\begin{twocolumnlist}{.2\textwidth}{.7\textwidth}
\end{twocolumnlist}
%% L'environnement <threecolumnlist> existe aussi pour trois colonnes.

%%------------------------------------------------- %
%%              pages vi                            %
%%------------------------------------------------- %

\chapter*{Acknowledgements}

...remerciements...

 %
 % Fin des pages liminaires.  À partir d'ici, les
 % premières pages des chapitres ne doivent pas
 % être numérotées
 %

\NoChapterPageNumber
\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                  %
%%   TEXTE DU MÉMOIRE :  introduction page 1,...    %
%%                                                  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter*{Introduction}

This paper presents a research-creation project in which I examine the unique problem space of pipe organ augmentation. As part of this project, I have developed a synthesis server in python, called OrganLab, which serves to emulate and mutate pipe-organ based sounds and is placed in dialogue with the acoustic instrument. The pipe organ can be considered the original synthesizer, and I extend this rich tradition with additive, subtractive, and FM synthesis, allowing me to access eects not possible with the original instrument, like interpolation between stops, glissandi, and inharmonic sounds. Since each partial is independently controlled, the harmonic spectrum can be exploded, contracted, and distorted, creating a rich palette of new timbral possibilities. 

These innovations are put into practice with the piece Élégies, written for my hyper-organ interface at l’église Saint-Édouard, where I've been organist since July of 2022. Based on the 10 Duino Élégies of Maria-Rainer Rilke, the piece incorporates the aural iconography of the space, making use of the sounds of bells, the fire alarm of the church, and the sounds of footsteps through its many corridors. This symbolic and spatial exploration mimics the aural exploration that seeks to navigate the continuum of acoustic and simulated--the sacred and the profane.

%%------------------------------------------------- %
%%                pages 1                           %
%%------------------------------------------------- %

\chapter{The Emergence and Relevance of Hyper-Instruments}

Hyper, or augmented instruments, situated at the confluence of traditional musical artistry and cutting-edge technological innovation, present a complex and diverse field of inquiry.
This chapter seeks to delve into this multifaceted world, highlighting the significance of hyper-instrument design and performance in contemporary music practices and the intricate process of developing and defining them within various musical contexts.

At the core of any musical instrument lies the interface between an artist's musical expression and the resultant sound in their environment.
Augmenting an instrument, therefore, becomes an exercise in expanding this interface.
This task is often far from straightforward, as musicians are typically engaged fully, utilizing their hands, mouth, and sometimes feet, to produce sound.
In such scenarios, where every faculty of mind and body is already employed, the question arises: where is there room for expansion?
Among the various approaches in interface design, are gestural mapping, sensitivity mapping through pressure sensors, and the integration of MIDI interfaces.
Each of these approaches has its unique challenges and opportunities, tailored to the specificities of the instrument being augmented.

The choice between convergence or divergence with the natural workings of the instrument is crucial.
For instance, augmenting a wind instrument involves navigating its intimate relationship with the performer's breathing and the air column.
Here, the lateral movements of the performer, which do not inherently affect pitch, provide an opportunity for introducing an independent dimension of expressivity and virtuosity.
Conversely, in an instrument like the piano, where pitch is intrinsically tied to the horizontal plane, any augmentation must consider this inherent constraint.
While this may limit the scope for independent controls, it ensures that any technological integration remains deeply rooted in the natural mechanics and traditions of piano playing.

This chapter aims to navigate the extensive landscape of hyper-instruments, examining methodologies and concepts that are shaping this evolving domain.
From the pioneering work of Tod Machover to adaptations in instruments like the hyper-cello, hyper-shakuhachi, hyper-piano, and hyper-flûte, we will explore how musicians and technologists are redefining musical expression.
Through this exploration, we seek to uncover the broader implications for music as a dynamic and evolving art form in the digital age.

\section{History and approaches}

\subsection{What is a hyper-instrument?}

A musical instrument is an interface \footcite{noauthor_instrument_nodate}. 
It allows one to express thought in audible form. 
Among the first instruments were undoubtedly the voice. 
The human voice allows us to express ourselves in a near infinite variety--from the most practical, logic oriented thoughts, to the most abstract of indefinite feelings. 
The voice provides an interface between their internal, subjective world, to the external. 

Yet a musical instrument is also a technology, which like any other, aids us in extending beyond our natural human capacities, yielding access to insights and abilities that would not otherwise be possible. 
A scientific instrument like the microscope allows us to magnify our visual capacity manifold, in order to see things that would be much too small for the naked eye. 
Similarily, a musical instrument can allow us to do things that would not be possible with the human voice. 
For instance, with a small flute, a person with a very low voice can sound very high frequencies, and vice versa. 
With a bowed instrument like the violin, one can sound the note indefinitely, bypassing the voices necessity to take breaths. 
With a lute, one can sound more than one note at a time. 

The results of a musical instrument are inherently subjective, and the goals of designing an expressive interface that is considered "good" are a reflection of the culture, the time, and the context of the people creating it. 
For instance, one culture may optimize for resonance and slow decay, whereas another may want shorter, more percussive sounds. 
One people may desire maximum expressivity through timbral variety, like with the human voice, while another may see timbral variability as a defect, and seek timbral consistency throughout the range of the instrument. 

The goals of innovation in musical interfaces are not uniform, and with an advance in one area of expressivity, another is lessened or lost. 
This process of aesthetic prioritization makes instrument design inherently personal. It nevertheless represents a process of innovation, albeit non-linear. 

In the twentieth and twenty-first centuries, with the advent of electronic and then digital technologies, people around the world have been reimagining the musical instrument from various perspectives. 
On the one hand, practices like digital lutherie generally seek to create interfaces centered in digital technologies, using various sensors, and computers to input and process data for expressive means. 
These kinds of approaches allow one to rethink what a musical instrument can be, without some of the physical restrictions and cultural baggage that come with a traditional instrument with a long history. 
These devices range from digital interfaces that mimick instrumental approaches, to gestural interfaces that generate sound simply by moving the instrument through the air.

The practice of augmented, or hyper-instruments takes a different approach. 
Rather than starting from scratch, the hyper-instrument seeks to integrate digital technologies with more traditional acoustic instruments. 
This approach has several advantages and obstacles. 
On the one hand, there is something to be said for longevity.
If an instrument has endured the test of time, and come to be used for generations upon generations, that demonstrates a creative and expressive capacity that is simply established by time.

On the other hand, one may easily pose the question: why ruin a good thing?
If the instrument already works, does what it needs to do, and has an established sound that we all love, what is there to add?
Of course, there is nothing wrong with the original instruments, and explorations of augmented instruments typically celebrate a multiplicity of approaches rather than holding the goal of a general improvement.
However, one can also make the argument that these instruments, so loved throughout time, deserve to be revisited with a modern perspective, both in terms of technology that simply wasn't available in the original periods of the instruments' evolutions, and in terms of the ideas and conceptions that the original builders and listeners didn't have access to. Furthermore, this spirit of exploration is the very same process that led to the creation of these instruments in the first place.

Pursuing this expansion comes with both great possibilities, and a certain burden of responsibility.
The historic traditions of these instruments give them a weight, not just in terms of the playing traditions, and the mechanisms involved in their interfaces, but also in the many, often unconscious, symbolic associations that we have formed with these instruments.
This cultural baggage is not something to be overcome however, but is a great strength.
If one is careful to take into consideration the nuances of the original interface, integrating them with their ideas in a way that complements, rather than fights the natural tendencies of the instrument, the result can be an enduring hommage, and a continuation of a tradition that crosses both culture and time.

\subsection{Tod Machover and the birth of the hyper-instrument}

The term hyper-instrument comes to us from Tod Machover, a leading figure at MIT's Media Lab, who has been carrying out pioneering research in hyper-instruments since the 1980s. 
His work aims to redefine the relationship between the performer and their instrument by merging human expressivity with machine precision. 
Rather than simply enhancing an instrument's capabilities, Machover's hyper-instruments explore new ways for performers to interact with their instruments, including feedback mechanisms where the machine can also make decisions that influence the performer.

The initial development of hyper-instruments involved integrating electronic keyboards (such as the Yamaha KX88 and Kurzweil Midiboard) and percussion controllers (including the Silicon Mallet, Octapad, and KAT) with a computer system \footcite{machover_hyper-instruments_1989}. 
MIDI data from these instruments was sent to a Macintosh II computer running custom software called Hyperlisp. 
This real-time MIDI/Lisp environment was specifically developed to analyze and process the input data, which was then sent to digital synthesizers, samplers, and outboard processing devices to generate the final musical output.

One of the most significant applications of hyper-instrument technology was the development of the hypercello, designed for the renowned cellist Yo-Yo Ma \footcite{levenson_taming_1994}. 
This instrument incorporated sensors to measure various parameters such as bow position, pressure, and finger placement. 
The data collected was processed by a computer to control and modify the sound in real-time, allowing the cellist to interact with the instrument in a highly nuanced and expressive manner.

The hypercello was prominently featured in Machover's composition "Begin Again Again...," performed by Yo-Yo Ma at the Concertgebouw in Amsterdam. 
This piece exemplified the potential of hyper-instruments to create complex, multi-layered musical experiences. 
The performance involved not only traditional cello playing but also interaction with a pre-programmed score, enabling Ma to control and shape the music dynamically. 
The integration of synthesized sounds and real-time digital processing created a unique blend of acoustic and electronic music.

The development of hyper-instruments involved significant technical challenges, particularly in managing the complex flow of data and ensuring accurate real-time processing. 
The hypercello, for instance, required precise synchronization between the performer's gestures and the computer-generated responses. 
This necessitated robust hardware and software capable of handling large amounts of data with high accuracy.

Not only does Machover seek to extend an instrument's expressive capacity, but cenral to his vision of the hyper-instrument is the redefinition of the relationship of the performer to their instrument \footcite{hoffman_q_2010}. 
For instance, his "double instruments" combine the gestures of two performers playing separate controllers, creating a new form of ensemble performance where musical results are produced collaboratively.

\subsection{Beilharz' hyper-shakuhachi}

The hyper-shaku, developed by Kirsty Beilharz, is an augmented version of the traditional Japanese bamboo shakuhachi flute. 
This project integrates motion sensors and computer vision to create an interactive performance environment that enhances both auditory and visual elements \footcite{beilharz_hyper-shaku_2006}. 

The hyper-shaku project builds on previous works in intelligent sensor environments and gesture-controlled interactive systems. 
The primary objective is to create a system where the performer's movements trigger both immediate auditory responses and generative visual processes. 
This dual-mode interaction distinguishes the hyper-shaku from other hyper-instruments, focusing on both the acoustic performance and the visual representation.

The system uses a combination of wireless sensors and computer vision to capture the performer's gestures. 
These inputs are processed in real-time using Max/MSP and Jitter software, which generates corresponding sound and visual displays. 
The motion data triggers various synthesis processes and generative design elements, creating a dynamic and responsive performance environment.

Gesture data is central to the hyper-shaku's functionality. 
The performer's movements, such as pitch inflections achieved by angling the chin and dramatic articulations, are captured and translated into both sound and visual outputs. 
This approach emphasizes the theatrical and spatial aspects of shakuhachi performance, enhancing the traditional sound with electronic augmentation.

The hyper-shaku can be used for both composed and improvisatory performances. 
It aims to re-invigorate interest in the shakuhachi by integrating it into a contemporary multimedia context. 
This hybridization of traditional and modern elements provides new opportunities for creative expression and audience engagement.

Kirsty Beilharz's hyper-shaku represents a significant advancement in the field of hyper-instruments. 
By combining auditory and visual augmentation, this project not only expands the capabilities of the shakuhachi but also explores new dimensions of interactive and generative performance. 
The hyper-shaku exemplifies how traditional instruments can be transformed and recontextualized through the use of modern technology.

\subsection{McPhersons hyper-piano}

The Magnetic Resonator Piano (MRP) is designed to augment the acoustic piano, allowing continuous control over pitch, dynamics, and timbre. 
The instrument employs 88 electromagnetic actuators to vibrate the strings independently of the traditional hammer mechanism, producing a wide range of sounds while preserving the acoustic richness of the piano. 
This system is controlled through an optical sensor strip on the keyboard, which measures the continuous position of each key, enabling nuanced performance techniques such as gradual key presses, taps, and vibrato gestures.

A common issue in DMI design is ensuring the instrument's relevance beyond its initial performance \footcite{mcpherson_problem_2012}. 
Many new instruments fail to attract a significant following, often because they are tailored to specific pieces or performers. 
The authors emphasize the importance of making the instrument useful to a broader community, allowing musicians to explore its capabilities and develop personal styles.

McPherson and Kim discuss how feedback from initial users can guide design refinements. 
For the MRP, they identified key areas for improvement based on musician input, including enhancing dynamic range, reducing attack time, enriching timbre, and simplifying control mappings. 
They also aimed to expand the instrument's capabilities without compromising its playability for traditional pianists.

To build a community around the MRP, the authors collaborated with six composers from Philadelphia and Princeton, each writing new pieces for the instrument. 
This project culminated in two concert performances, showcasing a diverse range of styles and techniques. 
The composers' engagement with the MRP provided valuable insights into its potential and limitations, driving further refinements.

Iterative Design: Initially link the instrument and piece, then iterate based on feedback to relax constraints and broaden its appeal.
Familiar Models: Connect new techniques and sounds to musicians' existing skills to facilitate adoption.
Audience Engagement: Convincing performances can attract potential collaborators from the audience.
Accessibility: Provide access to the instrument and designer support during the creation process to encourage ongoing use and development.

\subsection{Cléo Palacio-Quintin's hyper-flûte}

The hyper-flute, as developed by Palacio-Quintin, involves attaching various sensors to a standard flute. 
These sensors capture a range of data, including finger positions, breath pressure, and even specific gestures made by the performer. 
The data collected is then transmitted to a computer, where it can be processed in real-time to manipulate electronic sounds or control other digital effects. 
This setup allows the flutist to interact with electronic components seamlessly, creating a rich, hybrid soundscape.

One of the primary challenges addressed by Palacio-Quintin is the synchronization between the acoustic instrument and electronic components \footcite{palacio-quintin_composition_2012-1}. 
Traditional mixed music often required performers to follow rigid, pre-determined electronic tracks, which limited expressive freedom. 
By contrast, the hyper-flute allows for real-time interaction, where the performer's gestures and playing directly influence the electronic output. 
This dynamic interaction is achieved through programming and real-time signal processing, primarily using Max/MSP.

This choice also explains her lack of focus on sound spatialization, a popular domain among electroacoustic composers. 
The presence of an on-stage performer who projects an acoustic sound naturally delimits a sound space. 
To integrate the electroacoustic sound with the acoustic, her works are designed in stereo to be projected mainly by front-positioned speakers near the performer. 
The primary sound source remains the flute, and the electroacoustic sounds should emanate from it, justifying their projection near the source.

A significant focus of Palacio-Quintin's research is on gesture control, which she argues is crucial for creating a natural and intuitive interface between the performer and the electronic components. 
By utilizing gesture data, the hyper-flute can respond to the nuances of the performer's movements, allowing for a more fluid and expressive performance. 
This approach not only enhances the musical capabilities of the flute but also redefines the role of the performer, who becomes an active participant in shaping the electronic soundscape.

In terms of interaction, Palacio-Quintin’s approach aligns with Chadabe's vision, which advises allowing both the performer and the computer a degree of freedom to maintain a lively interactive space. 
The modes of interaction can vary; the music is not entirely fixed, and the computerized system remains flexible, giving the performer some freedom in their play. 
This aligns with the interest in improvised music, where each musician can be surprised by their colleagues' new musical ideas during the performance. 
George Lewis’s definition of improvisation in his article "Interacting with Latter-day Musical Automata" aptly fits this type of human-machine interaction: "Musical improvisation is (...) an interaction within a multi-dimensional environment, where structure and meaning arise from the analysis, generation, manipulation, and transformation of sonic symbols" [35, p.101]. 
Indeed, the hyper-flute’s electronic setup is a multidimensional environment, capable of analyzing, generating, manipulating, and transforming sound materials. 
This interactive system changes traditional subdivisions of musical practice, merging composer, score, work, performer, performance, instrument, and environment into one process.

In the context of hyper-flute, there are three main types of musical interaction between acoustic and electroacoustic components:

1.Digital Audio Effects on Acoustic Sound: The acoustic instrument's sound is transformed in real-time by digital signal processing. 
The computer serves as a direct extension of the instrument's sound, with the potential for gestural control over the effects to maintain real-time interactivity.

2.Sound Synthesis Controlled by Gestures: The computer generates sound independently, controlled by the performer's gestures, either captured directly or through sound analysis. 
This synthesis can diverge significantly from the acoustic sound.

3.Accompanying Electroacoustic Sounds: Pre-recorded or computer-generated sounds accompany the performer, independent of the performer's actions once initiated. 
This can include fixed sounds or algorithmically generated sounds, offering interaction through the performer's reaction to the evolving sound environment.

Prior to her doctorate, Palacio-Quintin primarily used the hyper-flute in improvised music contexts, focusing on the real-time transformation of the flute’s sound. 
During her doctoral studies, she aimed to explore forms of composition to deepen the use of other interaction categories.

\section{Hyper-organ}

When speaking of the instrument as a technology, perhaps no instrument is so emblematic of this tradition of continuous innovation than the pipe organ. 
Throughout it's millenia long history, it has continually been reimagined, from the one keyboard hydraulis of 300BC Greece, to the addition of multiple independantly controlled stops and keyboards, the extensive exploration of timbre, not just through the creation of new stops, but through the capacity to combine different stops, essentially bringing the role of instrument design into the hands of the performer. 
Then, throughout the 19th and 20th centuries, the pipe organ embraced pneumatic, then electro-pneumatic, and more recently, digital mechanisms of communication. 
The pipe organ has always played a role in challenging the threshold of the possible, on the one hand, looking back towards a rich history, and on the other hand, embracing and integrating technological innovations as they arise. 
At the same time, the pipe organ is unique in its diversity. 
Being an inherently modular instrument, one pipe organ can look very different from another. 
From one to the seven keyboards of the Boardwalk hall organ, with or without pedal-board, with or without enclosures, with an enormous variety of stops, and many more. 

In terms of the selection of stops, even a stop with the same name, such as Flute 8', can vary widely from one instrument to another based on the era of it's construction, the aesthetic goals of the builder and of the maintainers of the instrument. 
This means that each instrument tells the story of its unique heritage, and serves as a time capsule of shared historical and physical space.

In the context of augmented pipe organs, these considerations manifest in unique and innovative ways.
Projects led by visionaries like Lauren Redhead and the Orgelpark initiative are exemplary in demonstrating how technology can reimagine the traditional pipe organ.
These initiatives not only showcase the potential of technological integration with historic instruments but also highlight the diverse methodologies and creative opportunities within the field of hyper-instruments.

\subsection{Lauren Redhead}

Lauren Redhead doesn't explicitly use the term hyper-organ, or augmented instrument, instead choosing to align her work with the mixed music tradition of electronics and live performer. This is especially pertinent as Alistair Zaldua conceives of and controls the electronics in concert, strengthening the perspective of a duet, rather than a unified instrumental expression. Despite this, her pursuit of improvisation with a high degree of freedom, open forms, and interactivity is in line with the practices of Machover and Palatio-Quentin.

Redhead's approach emphasizes the organ's potential as an interface for electronic music. By utilizing the organ’s MIDI capabilities and incorporating live sound processing, she expands the instrument’s expressive range. This integration enables the organ to control electronic sounds and vice versa, allowing for a dynamic and interactive performance environment.

Lauren Redhead’s work often involves the use of open notation, graphic scores, and real-time generated notations, which challenge traditional notions of musical performance. Her compositions require performers to engage with the scores in a fluid and interpretative manner, often involving real-time decision-making and interaction with electronic elements.

In her paper "Sound and Space: Performing Music for Organ and Electronics," Redhead discusses the collaborative nature of her work with composers and performers. This collaboration is crucial in developing pieces that fully exploit the interaction between the organ and electronics. The performance space itself becomes an integral part of the instrument, influencing the sound and how it is perceived by the audience. This site-specific approach ensures that each performance is unique, tailored to the acoustics and spatial characteristics of the venue.

Redhead’s pursuit of improvisation with a high degree of freedom, open forms, and interactivity aligns her with the practices of pioneers like Tod Machover and Sophie Palazio-Quentin. These practices emphasize the importance of performer agency and real-time interaction, creating a dynamic and responsive musical experience.

One of Redhead’s significant projects is her work on the "Sound and Music Tour," where she performed a series of concerts across the UK, showcasing new works for organ and electronics. This tour highlighted the versatility of the organ when combined with live electronics and featured compositions that explored various aspects of this interaction.

The project "Diapason: Music for Organ and Electronics," released as a CD, documents some of these performances and provides a snapshot of the evolving relationship between the organ and electronic music. The recordings on this CD reflect the diverse sonic possibilities achieved through this combination, with pieces by various composers who have collaborated with Redhead.

Lauren Redhead’s contributions extend beyond performance and composition to include significant theoretical insights. Her work often references Nicholas Bourriaud’s concept of the 'journey form,' which she uses to articulate the relationship between creative, interpretative, and listening processes in music. This theoretical framework underpins her exploration of open notation and the iterative nature of musical interpretation.

In her book chapter "Notation as Process: Interpreting Open Scores and the ‘Journey Form’," Redhead delves into the performative and interpretative challenges posed by open notation. She argues that the interpretation of such scores is an ongoing process involving the composer, performer, and the score itself. This perspective shifts the focus from a finished musical product to the continuous evolution of the work through performance.

\subsection{Orgelpark}

\section{The question of coherence}

As we can see, augmented instrument design is far from a monolithic, and this inherent plurality is one of the most intriguing aspects of the field.
The term 'hyper-instrument' encompasses a broad spectrum of augmented musical tools, each with its unique characteristics and requirements.
Developing an intuitive interface for a hyper-instrument involves a deep understanding of the instrument's core essence and a thoughtful integration of technology that enhances without overshadowing its intrinsic qualities.
This diversity in approach underscores the difficulty in pinning down hyper-instruments as a singular practice.
Instead, they represent a constellation of practices, each illuminating different aspects of the intersection between modern and traditional technologies.

\chapter{Constructing an augmented interface for the pipe-organ}

In developing an interface for augmenting the pipe organ, several questions are central.
First of all, in what way will the interface be augmented?
Historically, many hyper-instruments have made use of gesture, or touch sensitivity as a way to augment the instrument.
As previously mentioned in chapter one, there is always the question of augmenting a part of the interface that is already integral to the functioning of the instrument, like adding a gestural mapping to the horizontal, left/right plane of a piano player, who already needs to use this space to navigate pitch space.
This ensures that the mapping is consistent with the innate gestures of playing the instrument, but does not allow for independant control of these parameters.
With the flute, on the other hand, this exact same mapping could be used more or less independently, as the horizontal left/right plane is not directly coupled with any musical parameter.

Taking the pipe organ, one could make the case that a gestural mapping would be logical, as it has the same issue of the piano, where the horizontal left/right plane is essential for navigating pitch space.
At the same time, gesture is a completely foreign concept to the organ, which operates almost exclusively in the discrete realm, a note being either on or off, and a stop being either engaged or disengaged.
The endlessly subtle, multidimensional contours of gesture couldn't be further removed.

This approach certainly has expressive potential, as would using pressure sensors on the keys, which could make use of another parameter not accessible through traditional pipe-organs.
At the same time, for the construction of my interface, I wanted to remain as true as possible to the tradition of the instrument itself, and what could be more traditional for the pipe organ than... adding another keyboard.

Midi keyboard have been used since the very beginnings of hyper-instruments, and have reached a state of ubiquity that in some circles they are considered passé, not worthy of more expressive and nuanced interfaces of the future.
At the same time, I argue that a midi keyboard is a particularly appropriate choice for the pipe organ, as it is exactly in line with the traditional method of interfacing with the instrument.

Here there comes another dilemna, however.
What does this midi interface play?
It would be all too easy to have the midi keyboard play some nice filtered saw waves or evolving pads and call it a day, but where would the sonic link be?
We could say that there is a link in interface, and a link in shared space, and that would be enough, but I wanted to go further and emulate the sound of the pipe-organ, based on analyses of the pipe organ of l'église Saint-Édouard, not with the goal of simple reproduction, but with the end of manipulating, transforming, and mutating these emulations in ways that would be impossible for the traditional instrument, in the hopes of achieving a dialectic, or a sort of continuum of acoustic and synthetic, which interact, overlap, disagree, and sometimes fuse.

Throughout the process of designing this interface the question has been posed, but how is this a hyper-instrument?
Isn't it a duet between two different instruments?
My case for a one-instrument perspective is reliant on the history of the pipe organ as one of the most modular instruments that exists.
Where one draws the line between what is considered one instrument, has never been as blurry as with this gargantuan instrument.
One could easily say that the positive division is one instrument, whereas the swell is another.
They are often even neatly separated in their own wooden boxes, called enclosures.
The interface is also already modular, with multiple keyboards representing different tonal palettes.
My keyboard is in line with this tradition, simply opening up yet more opportunities for timbral explorations.
This is the main thrust of my interface, but I also explore to a lesser extent, two other modalties of augmentation, which will be explored further throughout this chapter: audio processing and bed tracks.

\section{My background with the organ}
First, I’d like to provide an account of my background with the pipe organ.
In 2019, I travelled for several months in South America, beginning in Peru and moving up through Ecuador, ending in Colombia.
Upon returning from this trip, I moved into a house in Vancouver, agreeing to move in before visiting or even seeing photos of the house.
During my walk to the house on moving day, I was struck by the towering church, with nearly black stones, auspiciously stationed on the corner of this unsuspecting Kerrisdale intersection.
I then realized that my new home was directly beside this church.
After a couple weeks of living here, one day returning from a grocery trip, I heard music coming from the church and saw that the doors were open.
I peeked inside and saw not only that the choir was rehearsing, but also that behind them was a magnificent pipe organ.

I had always been interested in the pipe organ, starting from an obsession with several synthetic pipe organ sounds on my Roland RD300NX keyboard, especially what are called puff organ, and nason flute.
I used these sounds on my first album, Ad viger, on the track Cyclogram 1, originally written for two sopranos, but adapted for the organ.
Around this same time, in 2014, I approached Michael Murray, and he graciously invited me to the St.
Philips Anglican church, where I played the pipe organ for the first time.
He gave me a tour of the instrument, and had me stumble through reading some simple hymns in three staves, clumsily and comically attempting to manage the pedals for the first time.
At the time I felt I was too busy to take on the challenge of learning the pipe organ, and I was deterred by the inaccessibility of the instrument in comparison to an electronic or midi keyboard that I can fit in my bedroom.
The beauty and variety of the sounds of the instrument, as well as the delightful challenge of managing the pedal part with the other voices,  stayed in my mind in the following years.

Returning to 2019, I approached the music director of my neighbouring church, which I had by that time learned was the Pacific Spirit United Church, Bryn Nixon, and asked him if he would like any help with the music, mentioning that I had some keyboard skills, and that I’d be interested in trying the organ.
We scheduled a meeting, and he gave me a tour of the instrument, explaining the history of the instrument and its many stops, as well as giving me some pointers on what to focus on in terms of technique and repertoire.
At this point, I wasn’t sure if I would continue learning the organ or whether it would end with one or two sessions, but I was soon captivated by the instrument, and I received an email from Bryn telling me that the keys to the church were waiting for me in the office.
I bought some organ shoes, and I then spent many evenings quietly working through various physical and musical problems, slowly becoming more and more at ease at the console.
At the same time, Bryn employed me as a pianist, and as organist for simple pieces and accompaniments for the church choir.
This was a profoundly transformative experience for me, and gave me a regular outlet for both personal, and collaborative music making.

At this same time, I enrolled in part time studies at the University of British Columbia, in order to take two courses that I wasn’t able to fit into my Bachelors degree because of time table conflicts and course requirements: counterpoint and physics of music.
In order to receive part time student loans from StudentAidBC, they asked for proof of continuing education.
At the time I had no plans of continuing education, and the simplest option would have been to put UBC as my plan for graduate studies.
At the same time, being a curious person, I decided to do a bit of research on different institutions, and stumbled upon the University of Montréal.
I was immediately struck by what I felt was an openness and plurality of approaches and aesthetic that was coming out the University of Montréal music department, and soon enrolled as a student in their D.E.S.S.
one year diploma program in digital music.

I wanted to continue my studies of the pipe organ in Montréal, and I knew that the city of a thousand bell towers would have opportunities to continue this exploration, but it wasn’t until the summer after my D.E.S.S.
that I began to explore the many churches of Montréal, speaking with organists and priests in order to find people with which to collaborate with.
After several dead ends, I stumbled upon l’église Saint-Édouard, where the current organist Georges-Aimé, who was looking to retire, was all too thrilled to be relieved of his post.
I began as organist at this church in July of 2022, and began the master’s in music composition and sound design at l’Université de Montréal in the following September.

My proposed master’s project was based on treating the organ as an augmented instrument.
I had heard of augmented instruments since my time at UBC as part of what was then misleadingly named, the laptop orchestra.
This wasn’t a traditional orchestra, however, and included a cellist, a french horn player, a pianist, a trombone player, a singer, as well as several dancers.
This ensemble, which was later renamed interactive performance systems, focused extensively on gesture tracking through input from gametrackers, gyroscopes, and the microsoft Kinect 2, and consisted in a group of performers, and a group of coders.
I was in the coding group, creating interfaces for the performers.
This was a formative experience for me, but it wasn’t until the time during my D.E.S.S.
that I began considering the idea of crafting an interface for myself.
During my master’s program, I suddenly had access to l’église Saint-Édouard and the time to focus on a major project.

\section{Context: L'église Saint-Édouard}

\section{Modalities}

\subsection{Synthesis (intentions of mimicry and mutation}

\subsubsection{Additive}

\subsubsection{Subtractive}

\subsubsection{Analysis approach}

\subsubsection{Setup/physical constraints}

\customincludegraphics{IMG_3414_copy.jpg}{}

\subsection{Live processing and effects}

\subsubsection{Issues of microphone capture and amplification in live setting}

\customincludegraphics{stageplot_ste-edouard.png}{}

\subsubsection{Delay}

\subsubsection{Distorsion}

\subsubsection{Reverb}

\subsection{Bed tracks and triggers}

\chapter{Creative application}

The construction of a hyper-instrument is no easy task.
As we've seen in the previous chapter, there are many theoretical questions and technical challenges that present themselves along the way.
This creative chasm is rivalled, however, if not dwarfed, by the realm of creative possibilities presented in the application of this interface, once constructed.
The structure of this thesis attempts to provide a neat narrative, chapter by chapter, going from historical context, to interface design and construction, and finally ending with creative application, but this is merely for the reader's convenience, and to organize my own thinking in a coherent way.
The reality of the process has been anything but linear, and all three of these components have informed and directed the others in the course of this research.
My original vision of the master’s program before setting upon this challenge was to segment the work into these three stages, but my director, Pierre Michaud, urged me to begin the compositional portion as soon as possible.
This led to a situation which I’ve described as, “building a bridge while walking across it”.
While inherently precarious, this approach has the advantage that the two creative processes are intrinsically linked, forming a recursive bond.
The bridge may not be perfectly arched, and may meander from its intended path—but it nevertheless represents the unique path walked by its builder, advancing step by step, and somehow, miraculously, arriving at the other side.

This fact notwithstanding, for the purposes of this paper, I wanted to compartmentalize the roles I’ve played.
In this chapter I’d like to take the perspective of a composer and a performer as much as possible, imagining that the interface was designed by a third party, and is something that I’ve received and have to contend with.
I’ll attempt to describe not only the technical constraints of the interface, but the aesthetic implications of the environment, and the various ways that I’ve exploited these possibilities.

\section{Compositional considerations}

In my compositional world, the dialectic is central.
As far as I can tell, all thought stems from a discrimination, between self and other; between here and there; between this and that.
Of course, this binary, which is also intrinsic to the functioning of computers and of the brain of humans and other animals, is often not discrete in practice.
Though the polarities define the space of knowing, they frame a continuum which includes all gradations within that binary.
From an aesthetic perspective, one could say that beauty lies in finding some kind of balance in this continuum.
This is often called symmetry.
Of course, I am not the first artist to be struck by the beauty of symettry.
This has been a fascination of artists and mathematicians since at least the time of ancient Athens and the Pythagorean experiments with geometry.
Symmetry implies a formal balance between opposing parts.
From a visual perspective, this could mean the balance of luminosity vs shadow, or the density of visual information.
At least in the traditional realm of painting or drawing, all of these fulcrums are played out in a global binary which is the balance between right and left, and up and down.
In the auditory realm, which takes time as its canvas, the symmetry has to do with the dialectic between before and after.
Other polarities are then mapped onto this time space.
Parameters like spectral brightness vs darkness, an auditory version of luminosity, density of auditory information, or loudness vs softness, are all in constant flux in time space, creating proportions that are more or less symmetric.

A beginning artist might hear this proclamation and say, well wouldn’t the most beautiful thing be something perfectly symmetric?
With a single non-retrogradable rhythm and a palindromic melody based on one of the modes of limited transposition à la Messiaen?
However, the fascinating thing about symmetry is not that it’s beautiful, but that it isn’t.
Take the human form, or any animal for that matter.
If you crossed paths with a human that had an upside down head touching the pavement, or a face with upside down eyes and nose mirroring their normal features, you would likely be perplexed, fearful, and repulsed, cursing yourself for not having more pleasant thoughts before falling asleep.
The human form is laterally symmetric, but not vertically.
Yet even laterally, the symmetry is not a perfect symmetry.
We often don’t notice, but any human face has more or less pronounced differences in lateral features like shape and size of the eyes and ears.
Artificially constructed photos of faces with perfect symmetry are generally described as unnatural and eery, a phenomenon called uncanny valley.

This means that beauty is much more illusive, and entails a careful organization of symmetry and asymmetry, creating a balance of forces that nevertheless maintains an aspect of instability.
It’s funny to note that symmetry itself, which was originally our way of describing the concept of dialectic itself, has now become another dialectic: symmetry vs asymmetry.
This is the way thought works, from top to bottom, beginning to end, branching into infinite divides.
To me, this recursive act of co-definition, and the impossible fusion of two opposing elements, is the primordial creative force.
One could say that a single contradiction holds the entire world in its arms.
There are many more of these polarities which are fundamental to my thinking, and I’ll enumerate several of them in order to provide an account of my creative approach.

\subsection{Constraint vs freedom}

A key consideration in the design of any structure, be it physical, musical, social, or ethical, is the equilibrium between constraint and freedom.
On the one hand, too rigid a structure, and it lacks the ability to adapt to the needs of its environment.
In the case of a bridge, this might mean that the impact of an earthquake will simply break it altogether, as its lacking the suppleness needed to absorb the energy of the tremor.
In the arts, this may be a lack of spontaneity, of an exploratory sense of intrigue, relying too heavily on formulaic calculations without responding to the natural tendencies and needs of the medium.
On the other hand, one wouldn't want to attempt walking across a bridge with no rigidity whatsoever.
The rigidity provides the support to allow the bridge to serve its function.
In the same way, music with no sense of rigidity can seem like a nebulous path--a bridge that constantly falling away, letting us drop down into the murky depths below us.

This balance is essential, but is not straightforward in finding.
There are certainly artists who compose with very little or no conscious constraints.
Morten Feldman was often this way, emblematized in the famous story of John Cage asking him how he composed an early piece, and his replying "I don't know".
That being said, even Feldman may place unconscious constraints on his music, and at the very least he is constrained by the medium itself--the music, and the particular instruments, spaces, and players he's working with.

But beyond these inescapable constraints, there are also constraints at the compositional level.
These can be at the level of melody, harmony, counterpoint, meter, rhythm, form, and many more.
Constraining all of these parameters would be something akin to total serialism, like Boulez's Structures I and II.
These pieces and others like it could be said to be on the extreme side of the authoritarian polarity.
That's not to make an aesthetic judgement, but only to say that it does represent an extreme of controlled parameters.
On the other hand, constraining nothing and simply letting the music express itself, while a completely valid approach, can feel like leaping from our bridge into the unknown.

Often, constraining one, or several of these parameters, and building the other parameters intuitively based on the suggestions of the constrained parameters is an excellent, concrete way to make sure that there is a musical coherence, yet an expressive freedom.
An example of this kind of approach is the sitcom Curb your enthusiasm, which uses sketches showing the overall direction and main points of the story, constraining the overall form, yet doesn't provide the actor with their lines, allowing them to improvise based on the situation.
This leads to a wonderful situation where there is a sense of genuine interplay, surprise, and liveness that simply doesn't exist with fully scripted sitcoms.
The same is true in music.
Sietze de Vries, the great Netherlands-based organist, who is a proponent of this kind of a approach, often invokes the paradox "absolute control is the way to freedom", placing emphasis on the strict adherence to a constrained element, usually melodic, in order to hone one's creative potential.

Of course, before constraining an element, one first has to have some material to constrain.
This part of the process, often called inspiration, is one of the most challenging and mysterious parts of the creative process.
There are many approaches to the generation of ideas, and infinite subtle gradations within each approach that will vary from person to person, but they fall into two main categories, which I’ll call internal vs external.
The internal approach is an intuitive approach, it relies on bringing something that is heard or felt in the mind’s eye, and bringing that to conscious awareness.
This could be through singing or playing a melody, chord progression, rhythmic idea, etc.
The external approach relies on the use of data outside the artist’s inner world.
Rather than using audiation to generate material, one might use a series of numbers, either random or based on some mathematical series, or any conceivable data, mapping these values to a musical parameter or several, such as pitch, duration, loudness, etc.
The process of mapping data onto musical parameters is called signification, a good example being the piece Searsville 1891-2015, written by Marc Evanstein for the Stanford New Ensemble, which uses data from a CT scan of a sediment core from Searsville Dam at Stanford's Jasper Ridge Biological Preserve to map the wetter and dryer periods in the dam’s history with musical density, with wet periods being more dense and dry periods being more sparse.
https://www.youtube.com/watch?v=LAT6OCifCvQ.

The difference between this internal/intuitive approach, and the external/analytic approach is significant, John Cage going so far as to claim in rule eight of his ten rules for students and teachers, creation and analysis are independant processes that can’t be mixed.
Cage likely had a different definition of analytic thinking than I do, in the sense that he seems to equate it to analyzing his work once created, rather than analyzing data in order to generate material.
That being said, his words still hold true in this case.
The divide is akin to inverse ways of approaching the creative process itself.
I like to think of sculpture.
On the one hand, there’s the perspective that the sculpture is already inside the block of stone, and that the sculptors task is simply to make that form appear.
This is the internal approach, and in music it’s reminiscent of the operatic ideal during the time of Verdi, which was not to create a shockingly surprising melody, but to construct a melody that sounded familiar, as if it had always existed.

The analytic approach is more like making the stone into the sculpture.
Rather than revealing the statue within the stone, we create it, through trial and error, and force of will—arriving perhaps at a result that we never could have imagined.
Of course, these two approaches aren’t completely independent.
In my own experience I have historically leaned more towards the intuitive side, but I am very inspired by the generative power of the analytic approach, and in my ideal vision, the two processes form a sort of symbiotic relationship, nourishing eachother along the way.

Now, once material is generated, one can start using it as constraints, against which other elements can be built upon.
The question is then which elements to constrain, and they are not equal.
For example, for an improviser, it is often much more challenging to have constrained durations, with free pitch, either limiting the player to a certain set of durations and allowing them to choose freely between them, or placing the durations in a certain order, and allowing the player to play any pitch, than it is to do the inverse, giving the player a pitch set, either as a pool to be arbitrarily selected from, or as an ordered sequence, allowing the player to freely select the durations of each note event.

The first thought when considering constraints may be constraining a single musical parameter like pitch, duration, or loudness, and this is certainly a valid approach, but often it is more pragmatic to package several musical parameters together, essentially working at a higher level of abstraction.
This way the constraint is more in line with musical thought, embodying a holistic form of the musical experience.
In language this would be the equivalent of constraining someone to speak without using the letter t, or only using words that use the first thirteen letters of the alphabet, or perhaps to speak with every third word being an adjective.
It’s possible, and sounds like a fun, though somewhat sadistic party game, but it’s not a particularly natural way to represent semantic communication.
A more appropriate constraint might be to limit one to a subject, like speaking about things that are blue, or of a certain place or person.

In the western music tradition, oftentimes the word subject is used to describe an important melodic structure in a given piece of music.
Indeed, if one examines cultures from around the world and throughout time, often melody is considered a foundational element.
From the origins of notated western music, melody has been central, for centuries being sung monophonically by Gregorian monks, and later being used as a basis for early polyphonic music, beginning as what is known as organum, thought to have originated in the 10th century and culminating in the writings of the Notre Dame school masters Léonin and Pérotin in the 12th and early 13th century, and later in the complex motets of the ars nova school led by Guillaume de Machaut and Philippe de Vitry.
These flourishing periods of creation and polyphonic innovation have one important thing in common: they all rely on a constrained element.
This element came from the Gregorian chant which was so established at the time, placing the chant in the tenor voice and composing the other voice, or voices around this tenor.
This constrained voice is often called a cantus firmus, coming from the latin for fixed melody.

\subsection{Artificial vs real}

\subsection{Spatiality and sonic architecture}

\subsection{Metaphors}

\subsection{History vs contemporainity}

\subsection{Memory/decay}

\subsection{Dialectic and fusion}

As seen through the various axes discussed--the voice vs the organ, constraint vs freedom, artificial vs real, history vs contemporainity, etc., dialectic thought is very important to my approach.
In my philosphy, the binary is the most fundamental building block of thought.
Even the most basic idea precludes a distinction--between self and other, or between this and that, for instance.
To me the entire world exists inside this divine contradiction, and art in its highest form is like the impossible resolution of these opposing forces.
As far as we know, nuclear fusion is the most immense creative force of energy production that exists.
The triggering of this fusion process is not trivial however, requiring a temperature of more than one-hundred million degrees fahrenheit.
Perhaps the fusion, or synthesis, of ideas in the realm of art is just such a creative force, requiring an equally massive force to hold them together.

\section{As a performer (physical constraints)}
As far as the issues of interacting with the interface from the perspective of a performer, there are plenty of new obstacles to overcome and integrate. As a relative newcomer to the pipe organ in general, the traditional acoustic instrument already provides a significant challenge, and when I started playing the pipe organ at l’église Saint-Édouard, I found enough difficulty in playing the instrument as it was, that I was intimidated by taking on the additional challenge of the augmented component. I nevertheless began to incorporate little by little the practice of managing the fourth keyboard. 

Having a fourth keyboard may seem trivial after already traversing three manuals and a pedalboard, but the principal challenge was the angle. Whether at the piano, the harpsichord, or the organ, a keyboardist will generally place themselves somewhere near the centre of the instrument, facing it directly. My original plan was to place the keyboard underneath the others to maintain this continuity of angle, but it quickly became apparent that there was not enough space to make this feasible, and the idea was born to place the keyboard off to the side. This was a very new and uncomfortable experience for me. The practice of keyboards at 90 degrees is not unheard of. Artists like Chick Corea and Cory Henry have used this setup, and although normally they switch from one to the other, facing each instrument as normally, on rare occasions they will play both instruments at the same time, generally providing chordal pad sounds with the left hand while soling on the other keyboard with their right hands. 

While this approach may have a niche precedent in jazz circles, it seems to be new to the pipe organ world. In German and in Dutch, the manuals are often named by their position, including Oberwerk, or upper manual, and the Rückpositif which as the name suggests, originated as a separate manual placed behind the player, but eventually became incorporated into the main instrument in front of the player. A “Seitewerk” or “Seitepositif”, which would be the logical German translation of “side manual” seems to not exist at all, as far as I can tell. In any respect, this new orientation certainly took some getting used to, as it threw off my entire basis of proprioception. Not only this, but it introduced with it another problem. I mentioned that Chick Corea and Cory Henri typically play the solo with their right hand while chording with their left, but what if I want to put the melody in the acoustic organ and play the bass part with a synthesized sound? This may seem like a niche requirement, but is not an unreasonable orchestration choice. Yet if the midi keyboard is set to my right side, this means playing the bass part with my right hand, which sounds like a keyboardist’s uncanny, nervous dream. Placing the keyboard to my left, which was less practical at the console at l’église Saint-Édouard in any case, would mean that synthetic sounds in the melody would have to be played with the left hand, which is even stranger. 

Two obvious solutions present themselves here, one being to move the keyboard from side to side during a given performance, and the other is simply to have two keyboards on either side. In the first case, after some consideration, I realized that not only would cabling make it a nightmare, but it would take too much time. Of course I could hire a sort of stage crew to move the keyboard around, but in the spirit of DIY, I didn’t want to have to depend on this kind of arrangement, if only for the practical reason that I could rehearse without needing to manage a team. For the second option of having two keyboards, I don’t have an excellent reason why I haven’t chosen to go in this direction, and it is certainly something that could be explored in the future, however, my main contentions are that the instrument is already incredibly complex, and adding one keyboard is already stretching the limit, and two additional keyboard seems excessive. I also like the aesthetic of asymmetry, and of course there is the practical issue that two keyboards, would box me in, creating a claustrophobic environment. After reflecting on all of these options, I ultimately elected to maintain the original version of one keyboard on the right. Originally I thought I would just avoid putting the bass part in the synthesizer altogether, but in the end, being something of a masochist, I decided to treat playing the bass part in the right hand as a new avenue of virtuosity, incorporating it in small doses to begin integrating the new spatial and cognitive mappings.

Besides the issue of keyboard angle there are also the issues of sustain pedal, patch changes, sample triggering, and volume control. The midi keyboard permits the pianistic privilege of the sustain pedal, which is not found on a traditional pipe organ, and is particularly useful for drone sections where one can simply depress the damper pedal, freeing the hands to do other things. One of the things that the hands might be freed up to do, besides playing the other manuals or changing the registrations, is triggering samples or regulating the volume of the synthesized sounds. Originally, all of this was done through OSC, using a TouchOSC patch on my phone, but the internet connection in the organ loft is not reliable enough for this to be a sound solution, and MIDI was selected. This way, various keys are assigned to control changes on channel 6, stepping through a global navigation scheme which modifies the settings of both OrganLab and Ableton Live, as well as triggering samples. Two sliders are assigned respectively to the volume of Pyo and Ableton, with a third slider assigned to both volumes. 

\section{Tools}

\subsection{GrandOrgue}

\customincludegraphics{grand_orgue.png}{}

\subsection{OrganLab}

\subsection{Open Music}

\subsection{Ableton Live}

\section{Élégies}

\customincludegraphics{2024-04-28_MH_sat50.png}{}

\subsection{Context and precedents}

\subsection{1e Élégie}

\epigraph{\textit{Qui, si je criais, qui donc entendrait mon cri parmi les hiérarchies des Anges?}}{}

\subsubsection{Narrative context}
Rilke's Élégies begin not with a cry, but with the question of a cry.
A question seemingly addressed to no one.
A plea of solitude and desperation.
Perhaps this question is addressed to God, to the reader of his poem, or to himself.
In any case this question represents perhaps a sort of internal cry, representing the subjectivity of our protagonist's lament.
He goes on to evoke the level of solitude felt in his unknown setting, wondering if anyone is listening at all, calling into question the purpose of crying, or of communicating in any way.

From a sonic perspective, the issue of sound is immediately relevant, the cry evoking at once a sound source---the human voice---and the volume, and intensity of this sound.
The subjective nature of sound is also essential, with "qui donc entendrait" illustrating the importance of hearing and perception to the auditory experience.
This startling juxtaposition between the immense loudness of the subject's inner world, and the expansive quietude of the environment provides an immediate dialectic between inner and outer worlds.

The second part of the sentence describes the setting somewhat, though in obscure terms.
He depicts the subject as among the hierarchies of angels.
This brings another essential dialectic lense to Rilke's world: the spiritual and the human.
Here is a startling image.
We were quickly led to believe that we were alone, contemplating the futility of communication, when not only are we not alone, but we are found among a cacophonie of angels.
The reason that the angels might not hear the subject's plaintif cry is unclear. 
Maybe the angels are simply too far away.
Maybe they are making too much noise themselves to hear anything.
Or maybe they do not contain the capacity for subjective experience at all.
If we take this last case, this quickly paints an image of Rilke's angel, not as an anthropomorphized, man's best friend, but as a very different creature entirely.
The idea that they would hear nothing implies an austere, cold distance from the angels---the small human alone underneath a swarm of ethereal beings.

\subsubsection{Techniques}

Software:

One of the main advantages of a digital pipe-organ over an acoustic instrument, is the ability to use non-discrete pitches.
To represent the hierarchies of angels, I thought that it would be appropriate to make use of an extended glissando, starting in a lower register and moving upwards, signifying some kind of transcendental nature.
It might seem strange to represent a hierarchy with something non-discrete and essentially non-hierarchical, but the hierarchy is already present in the harmonic series.
The way that this glissando function works, is that over the course of a set amount of time (in this case two minutes), the frequency of each of the eight partials is incremented by one about ten times a second.
Because the frequency is being incremented, and not the midi equivalent, the harmonic series quickly becomes inharmonic.
For instance, if 100 and 200 are the first two partials, after an addition of 10, 110, and 210 are no longer multiples of eachother.
This process creates the sensation of an alien sort of movement, pulsating and morphing, yet not in a coherent, human way.    
\begin{figure}[H]
\begin{lstlisting}[language=Python]
glissC = [0 for i in range(8)]
def glissUp():
    global glissC
    for i in range(len(glissC)):
        glissC[i] == 0
    if glissC[0] < 600:
        stop1.setTrans(glissC)
        for i in range(len(glissC)):
            glissC[i] = glissC[i] + 2
    else:
        for i in range(len(glissC)):
            glissC[i] = 0
\end{lstlisting}
\caption{The glissUp function takes a python list and increments it until it reaches 600, this list is used to increment the first 8 partials of the synthesis module}
\end{figure}

Compositional:

For this movement, I wanted to represent the binary form of the poetic fragment with a juxtaposition of proportional notation and semi-metric notation.
The proportional notation is supposed to represent the angelic form.
Here again the choice is somewhat arbitrary.
One could easily make the case that metrical time is more hierarchical.
At the same time, metrical time has a long tail of history in western notation, and has for me a notion of human comprehensibility, whereas proportional notation is newer, with less historical associations.
Furthermore, proportional notation seems to give the sense of music frozen and static in time, which I associate with the timelessness of the ethereal.

\subsubsection{Composition Process}

This movement begins with a sung melodic fragment, which represents the principal melody of the piece, taken from a session of singing the poetry of Rilke in l'église Saint-Édouard in the fall of 2022.
The melody comes from the first line of the first elegie and has been modified only slightly from the original recording. 

\customincludegraphics{3-1-1_melody}{The main melody of the piece, based on the 19 syllables of the first line of Rilke's elegies.}

I wanted to then invoke a sonic interpretation of the cry.
Even though the cry is simply a hypothetical, and is not actually enacted in the poem, I wanted to represent the internal, subjective cry.
Of course there is no correct, objective way to represent subjectivity, but I've attempted a poetic approach.
To do this, I call upon a distinctly north american property of the pipe-organ: the crescendo pedal.
This pedal allows the player to quickly open or close many stops simply by flicking the foot.

One loses detailed control over the registration, yet gains access to a quick dynamic swell that surpasses the capacity of the normal expressive pedals.
I begin the piece with the crescendo pedal open about 60% of the way, with the acoustic and digital organs playing the same D minor chord.
At first, the acoustic instrument masks the digital sound completely, evoking the sens of solitude, but gradually the crescendo pedal closes and the acoustic instrument is masked, leaving the glissando as the focus.
The gradual widening of registral distance between the acoustic organ and the digital glissando evokes the sense of distance between the human and the angelic realms.

As far as the notation of the glissando, my original sketches included lines symbolizing the gesture of the sound and the many partials, and the ascent of the angels, but in the final manuscript, it felt like an unnecessary use of space on the page.
The organ-lab setting is desribed with the number one placed in a box, and upon placing the hand in the right position, the glissando will be handled by the software.

\customincludegraphics{3-1-2_gliss}{Original sketch of the glissandos of the first movement}

\customincludegraphics{3-1-3_system}{Final version of the first organ system, with the 1 in a box representing the first setting of Organ Lab, the sustain pedal used to maintain the glissando in the upper stave, and the crescendo pedal notated at the bottom.}

The choice of pitches in this movement come from a range of influences.
For one, one of the original inspirations of this piece was the Magnificat of Jehan Titelouze, and particular the sixth piece, Sexti Toni, which is based in F major.
For the first chord, I didn't want the brightness of F major, and decided to use the relative, D minor.
Not long after, on analysing the bells of l'église Saint-Édouard, I realized that they were also based in F major, which further jutified this decision.

Towards the end of the movement, the pipe organ slowly enters with ascending chords built around open fifths and fourths.
These mostly parallel, open harmonies for me evoke two influences.
On the one hand the organum tradition of Leonin and Perotin, and on the other hand, the open harmonies of Boards of Canada.
Ultimately, these chords rise like the synthesized glissando, representing the human who is caught under the ethereal.
Here the notation changes from proportional, to semi-metrical.
I use this word because the quarter note and half note of metrical notation are present, yet meter itself is not.

\customincludegraphics{3-1-4_system}{Rising chords based on open fifths and fourths}

\subsection{2e Élégie}

\epigraph{\textit{Tout Ange est terrible.
Et pourtant, malheur à moi!}}{}

\subsubsection{Narrative context}

In this movement, the haunting opening of Rilke's verse, "Tout Ange est terrible," forms the thematic crux.
The word 'terrible' presents a complex duality, oscillating between connotations of fear and awe and a sense of pathos and frailty.
This ambiguity is not just a linguistic puzzle but a profound emotional landscape that the music seeks to explore and express.
The latter part of the verse, "Et pourtant, malheur à moi!
pourtant je vous invoque," resonates as a solemn invocation, a plea before embarking on an arduous, soul-searching journey.
This movement attempts to capture the essence of this prayer, infused with both trepidation and a resigned yearning.

\subsubsection{Techniques}

Software:

The use of a gradual dynamic envelope is a pivotal element in this movement.
By enhancing the traditional bourdon sound with a brighter tone, and accentuating the fourth partial, a unique auditory experience is created.
This approach transcends the limitations of an acoustic pipe organ, illustrating the intersection of tradition and innovation.

\begin{figure}[H]
\begin{lstlisting}[language=Python]
def dynEnv():
    print('Enveloppe dynamique')
    stop1.setPart([1, 2, 3, 4, 4, 4, 0, 0])
    stop1.setMul([0.588, 0.338, 0.665, 0.773, 0.512, 0, 0, 0])
    stop1.setEnvAtt([0.285, 0.450, 0.327, 0.338, 0.385, 0.277, 0, 0])
    stop1.setEnvDec([0.02, 0.04, 0.085, 0.008, 0.008, 0.008, 0, 0])
    stop1.setEnvSus([0.446, 0.523, 0.404, 0.05, 0.05, 0.542, 0, 0])
    stop1.setEnvRel([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0, 0])
    stop1.setNoiseAtt(0.081)
    stop1.setNoiseDec(0.146)
    stop1.setNoiseSus(0.7)
    stop1.setNoiseRel(0.1)
    stop1.setNoiseMul(3)
    stop1.setNoiseFiltQ(3)
    stop1.setSumMul(0)
\end{lstlisting}
\caption{The dynEnv function alters the dynamic envelope of both the harmonic sound and the noise sound, while tripling the fourth partial}
\end{figure}

Compositional:

The structure of the movement employs a binary form to parallel the contrasting themes of Rilke's verse.
Part A, representing the 'terrible angels,' is characterized by dense chromatic harmonies and a rich, plein orgue registration, employing proportional notation for depth.
In contrast, Part B adopts a simpler harmonic language, employing metric notation.
This section features an inverted version of the initial melody, serving as a cantus firmus.
Interestingly, all durations in this voice are equalized as whole notes, played with the pedals along with the bourdon and flute 8’.
This structure serves both as an improvisational guide and a compositional framework, offering the performer the liberty to improvise around the notated cantus firmus, colored in red, with an alternative pre-composed version available as a guideline.

\subsubsection{Composition Process}

The creation of this movement began with the creation of the sung melodic line.
I considered using new motivic material, but ultimately decided that it would be better to use a variation of the original melody.
This is partly to maintain coherence, and partly in reference to the Magnificat of Jehan Titelouze which uses a repetition of a sung melodic fragment as a formal element.
This thematic variation is now infused with additional chromaticism, reflecting a progression into complexity and uncertainty.

\customincludegraphics{3-2-1_melody}{A variation of the first sung melody, with more chromaticism}

The creation of Section A of the organ composition began with a dissonant harmonization of the main melody, making extensive use of an altered dominant sharp 7 chord.
This chord, a variation of the classic altered dominant chord in jazz, swaps the flat seven for a sharp 7, alongside the dominant 7 sharp 13 flat 9, and the minor major 7.
These harmonies, particularly resonant with the plein orgue registration, evoke an intense, almost ethereal quality.
The subtle voice leading within this dense texture aims to create movement and emotional depth.
Section B's composition process, focusing on the outer voices, was governed by the principles of traditional counterpoint.
This involved careful resolution of dissonances and maintaining a fluid musical dialogue between the voices.
The movement, starting from a melodic D minor perspective, culminates in a half cadence to the A minor dominant, signifying a transition yet an unfinished journey.

\customincludegraphics{3-2-2_system}{System showing the progression from dense chromatic harmonies in proportional notation to metrical notation with outervoices braiding an inner cantus firmus}

\subsubsection{Linking}

The movement concludes with an auditory link to the next – a sound sample of footsteps echoing in the basement of the church, leading into the sprinkler room.
The mundane, yet eerily resonant sounds of the church's underbelly, including the hum of compressors, provide a sensory bridge to the third movement.
This blending of the sacred with the profane mirrors the thematic journey of the music, from the divine to the earthly, the celestial to the subterranean.

\subsection{3e Élégie}

\epigraph{\textit{Chanter l'Amante est une chose. C'en est une autre, hélas! de chanter cet occulte Fleuve-Dieu du sang.}}{}

\subsubsection{Narrative context}

I interpret this poetic fragment as a reflection on the difficulty of walking the path ahead--contemplating the dichotomy between the relative simplicity of singing of love, versus the profound challenge of vocalizing the sacred, yet enigmatic, 'God river of blood.' 
This highlights a dialectic between thought and action, and the contrast between superficial perception and embodied reality.
This narrative conflict sets the tone for the movement's musical exploration.
Musically, I interpret "chanter", in a few ways, on the one hand, as the voix humaine stop on the récit, with tremolo, which imitates the sung human voice.
"Chanter l'Amante" is represented by a simple song-like melody, and "Chanter cet occulte Fleuve-Dieu du sang" is interpreted as the compositional and timbral disintegration of the movement.

\subsubsection{Techniques}

Software:

I thought that it would be appropriate to emulate the tremolo of the voix humaine stop with frequency modulation of a voix humaine emulation in OrganLab.
With this idea, it seemed like a logical step to increase this frequency modulation in speed, while changing the index and ratio, such that the natural trembling of the voice, passes through a comic, exaggerated warble, gradually losing the sense of sung tremolo altogether, giving way to an inharmonic, bell-like sound.
This is all done by interpolating with SigTo() objects, and the entire process takes place over 180 seconds.
The one part that is not interpolated, due to limitations of ADSR objects in Pyo, is the envelope, which simply changes suddenly at the end of the 180 seconds.

\begin{figure}[H]
\begin{lstlisting}[language=Python]
bellCall1 = None
bellCall2 = None
bellCall3 = None
bellCall4 = None

def bell():
    global bellCall1, bellCall2, bellCall3, bellCall4 
    bellCall1 = CallAfter(stop1.setEnvAtt, time=180, arg=(.001, .001, .001, .001, 0.001, 0.001, 0.0001, 0.0006, 0.0007, 0.0005, 0.0006, 0.0003, 0.0005, 0.0003, 0.0006, 0.0005, 0.0004, 0.0002, 0.0001, 0.0001)).play()
    bellCall2 = CallAfter(stop1.setEnvDec, time=180, arg=(1.3, .05, .02, 0, 0, 0.04, .004, 0.04, .04, 0.04, .04, 0.04, .04, 0.04, .04, 0.04, .04, 0.04, .04, 0.04)).play()
    bellCall3 = CallAfter(stop1.setEnvSus, time=180, arg=(.4, .1, .02, .01, .01, 0.01, .01, 0.01, .01, 0.01, .01, 0.01, .01, 0.01, .01, 0.01, .01, 0.01, .002, 0.002)).play()
    bellCall4 = CallAfter(stop1.setEnvRel, time=180, arg=(2, 0.1, 0.1, .01, .03, 0.4, .04, 0.04, .04, 0.04, .04, 0.04, .04, 0.04, .04, 0.4, .04, 0.04, .04, 0.4)).play()
    setInterpol(180)
    stop1.setRamp(180)
    stop1.setMul([1, 0.01, 0.1, 0.01, 0.07, 0, 0.02, 0, 0.01, 0, 0.003, 0, 0.003, 0, 0.001, 0, 0.001, 0, 0.001, 0])
    stop1.setRatio(0.43982735)
    stop1.setIndex(4)
    stop1.setNoiseAtt(0.001)
    stop1.setNoiseDec(0.1)
    stop1.setNoiseSus(0.01)
    stop1.setNoiseRel(0.1)    
    stop1.setNoiseMul(0.9)
    stop1.setNoiseFiltQ(4)
    stop1.setPartScRat(1.02)
    print(bell)
\end{lstlisting}
\caption{This function morphes the ratio and index of frequency of modulation, as well as a very slight exponential expansion of the harmonic series on the attack, which creates an additionally inharmonic attack. The envelope is altered after 180 seconds through the bellCall function calls}
\end{figure}

Interpolation in general was a major goal of this project, and one of my initial ideas was the transition between harmonic organ sounds, and inharmonic and/or chaotic sounds.
This process is semantically justified through the passage from the simplicity of singing of love, versus the reality of singing the God river of blood.
It is as though the simple dream is interrupted by a descent into a more complextimbral world.

Compositional:

The movement adopts a song form to represent 'singing,' starting with an 12 bar melody as the A section.(Grove reference)
Instead of following a traditional AABA form, the A section is repeated incessantly in a variation form.
These variations expand exponentially at a rate of 5/4, symbolizing a narrative disruption akin to a rupture in the fabric of time.
During these variations, the left hand chords and rhythms are constrained, while the right hand is free to improvise. 
A composed version is offered as a suggestion, or as an alternative to improvisation.
To parallel the software techniques, frequency modulation is also applied at the notational level.
A patch in OpenMusic uses the A section melody as the carrier and the bass line as the modulator to generate chords.
Selected sonorities from this process are integrated into the variations, enhancing the movement's thematic and textural depth.

\subsubsection{Composition Process}

The movement begins with a sung fragment, introducing a lilting melody, rising by thirds and spelling out an e minor triad.
The organ then enters with a similar rising motive, harmonized with perfect fifths below the melody.
Throughout this process the tonal center is tenous, first suggesting B minor in the voice, then A minor in the organ, with a thin, hollow texture of 8' and 2' flutes, before pivoting towards D minor.
This sets the stage for the entrance of the main motive of this movement, a lamenting descending line, beginning on the third scale degree, in this case F, and moving down towards the fifth scale degree, C in this case.
This descending line is in stark contrast with the rising motive heard earlier.
Both the tonal and motivic contrast between the opening sung melody, and the following part create a dissonance, not in a harmonic sense, but in a temporal, or symbolic sense.
The relation between the sung material, and the organ material is not yet clear.
Various alternatives to the lilting rising motive were tried for the introduction.
The original intention was to write something that felt like a classic song introduction in the tin pan alley tradition.
In the end, none of my more elaborate introductions seemed as appealing as the stark simplicity of the original rising motive.

\customincludegraphics{3-3-1_system}{Sung melodic fragment introducing lilting ascending motive}

\customincludegraphics{3-3-2_system}{Organ introduction, referencing the rising motive, but leaning towards A minor instead of B minor, which then leads into the main, lamenting melody at the end of the system.}

As for the composition of the descending motive, the first four measures came to me around the same time as the rising motive, around september, 2022, when I first started as organist at l'église Saint-Édouard and started my masters program at l'Université de Montréal.
Throughout the following months, I found myself adding on pieces to this melody, and finding new variations.
Once I decided to move in the direction of song form, I tried to crystallize the melody into a version that would fit within the bounds of an 8, 12 or 16 bar section I did this by taking all my motivic fragments and shuffling them around in different orders until I found a version that felt like it had a logical progression.
In the end, the idea seemed to naturally solidify into 12 bars.

At first, the continuation of this movement was supposed to simply dissolve into a textural mass, as the sound of the voix humaine undergoes it's painful transition into a bell.
This was the working idea until I realized that variation form, a form that I was looking to incorporate into the piece at some point, would be particularily appropriate for song form, as there is a great jazz tradition of playing variations on traditional songs.
In order to facilitate these variations, and to reference the jazz tradition, I decided to find jazz chords that could harmonize the original melody.
The original accompaniment is with an octave ostinato, and didn't lend itself well to variations in the jazz style, leading to an alternative harmonisation.

\customincludegraphics{3-3-3_melody}{Descending "lament" motive, with jazz changes annotated above}

The obvious and simple choice would have been simply to simply repeat the twelve bars over and over again, but I felt that this simple repetition wasn't necessarily in line with the subject matter, devolving into the God river of blood.
I was also hearing continuations in 3/8, and wanted to find a way to alter the meter, while keeping in line with the variations idea.
It then came to me to incorporate exponential variations.
The idea is that each variation is either dilated, or expanded by a certain proportion.

In this case, I chose the proportion of 5/4, finding that a proportion of 2/1 wouldn't give me the metric variety I was looking for, and 3/2 would be too long.
5/4 seemed to give me the right amount of variety without being overly long.
In order to verify this, I drafted a patch in OpenMusic that calculates the total time of a given number of measures with a given time signature at a given tempo.
With 12 measures at a tempo of quarter note equals 64 beats per minute, I found that with four variations, the movement would take about 4.3 minutes, not including the introductory material.
This seemed like a reasonable length, and gave me the time signatures of 5/4, 25/16, and 125/64.

Obviously, these latter two are all but unreadable to mere mortals, and required some clever compartmentalization to yield a musical, communicative notation.
I decided to break up 25/15 into three measures of 3/8 plus one measure of 7/16.
The 125/64 was more problematic.
After many attempts, including a systematic attempt to find every possible decomposition of the fraction, using a site similar to this one: https://www.mathcelebrity.com/decompose-fraction.php?num=125%2F64&pl=Decompose, only to find that all possible combinations would need a measure with a denominator of 64.
Using 64th notes as the rythmic pulse is not particularily practical, so my first instinct was to round to the 32nd note, but I then considered the possibility of incorporating a metric modulation.
Through some trial and error, I found that making five eighth notes equal to the new quarter note would allow me to use a measure of 2/4 plus a measure of 9/32 to equal my 125/64 beast.
9/32 is still not a particularily common time signature, but the grouping of three 32nd notes in compound time makes it more tenable.

\customincludegraphics{3-3-4_system}{First variation, in 5/4, with constrained elements notated in red, and free, optional element notated with small noteheads}

\subsubsection{Linking}

As the movement concludes, the sound of the bells gradually fades, transitioning into the sound of singing glasses.
This transition not only serves as a bridge to the next movement but also symbolizes the continual evolution of the thematic elements–-from the tangible to the ethereal, from the known to the unknown.

\subsection{4e Élégie}

\epigraph{\textit{Vous, Arbres de la Vie, oh! quand donc hivernaux? Nous n'allons pas à l'unisson.}}{}

\subsubsection{Narrative context}

On the one hand, this poetic fragment speaks to the trees of life.
The object that these trees symbolize is not clear.
Are they the angels, the readers of the poem, or perhaps life itself?
In any case, the poem then places the trees in the inevitability of the impending winter.
This winter can be read as an intimation of death, or at least a dormant state.
The musical term \textit{'unisson'} and its deliberate negation in the next sentence express a sense of divergence or disharmony, which becomes a central theme in the musical interpretation.

From a musical standpoint, I wanted to set the listener in the winter, which for me entails a sense of space and vastness, of stillness.
To evoke this environment, I thought it would be appropriate to use sounds with slow attacks and decays, to create a sense of slow evolutive timbre.
As I had been working on the writing of Jardin de Givre, which made extensive use of singing wine glasses, and I had high quality chromatic samples of these wine glasses on hand, I decided to create a sampler out of these recordings, so that I could play the wine glasses on my midi keyboard.
This also presents a pun, as the french word for ice is glace, which is a homophone with glass, the material of the wine glasses, evoking the ice covered ground of the desolate winter landscape.
As far as the second part of the poetic fragment, I decided that this would be a good moment to bring back the glissando motive from the first movement, this time moving in contrary motion.

\subsubsection{Techniques}

Software:

In this movement I make use of sampling for the first time, in two senses.
In one sense, with a sampler VST in Ableton Live which uses some recordings of singing wine glasses recorded in my apartment.
This sampler is driven by my midi keyboard and can be played chromatically.
On the other hand, I have a longer sample that is not played on the keyboard, but is simply triggered, acting as a bed track, to support the finale of the movement, creating additional tension and filling the space.
Lastly, in Pyo, I decided to revisit the glissando motive from the first movement, this time rather than moving all the partials in the same direction, moving the even partials higher in pitch, and the odd partials lower in pitch, creating a kind of contrary motion, while at the same time exploding the harmonic series.
This process refers to the text "Nous n'allons pas à l'unisson", creating a sonic divergence in opposing directions in pitch space.

\begin{figure}[H]
\begin{lstlisting}[language=Python]
def glissCont():
    global glissC
    for i in range(len(glissC)):
        glissC[i] == 0
    if glissC[0] < 600:
        stop1.setTrans(glissC)
        for i in range(len(glissC)):
            if i % 2 == 0:
                glissC[i] = glissC[i] + 0.4
            else:
                glissC[i] = glissC[i] + -0.4
    else:
        for i in range(len(glissC)):
            glissC[i] = 0
    print("0", glissC[0])
    print("1", glissC[1])
\end{lstlisting}
\caption{The function glissCont defines a glissando where each even partial is gradually incremented, whereas each odd partial is gradually decremented. This function is later called by a Pattern object 10 times a second}
\end{figure}

Compositional :

Several compositional constraints and techniques are explored in this movement.
On the one hand, the application of an isorhythm as cantus-firmus, which is placed in the bass part but is played with the right hand on the midi keyboard.
The right hand is very much not accustomed to playing the bass part, and so this represents a sort of étude.
Seeing as the keyboard can't easily be switched from one side to another, and playing it with the left hand would make it impossible to play the acoustic organ with the right hand, this provides a unique limitation and possibility for new cognitive and proprioceptive pathways.

I also wanted to develop the ascending by thirds motive, and decided to use it as an ostinato that begins slowly, later becoming a fast arppegiation, which in turn becomes the accompaniment for a variation of the melody of lamentation.
This provides a first attempt of the synthesis of these two motives, so central to the composition as a whole.
The arpeggiations also form, both visually in the score, and sonically, a set of arches.
These arches are symbolic of the physical arches of l'église Saint-Édouard and represent the broader iconography of the church.

\subsubsection{Composition Process}

This movement begins with an evocation of winter's vast stillness.
Long, monophonic singing glass samples with gradual attack and decay, unachievable on a traditional pipe organ, form the foundation.
These melodies are particularily centered on different combinations of the minor third, referencing both the initial melody and the ascending motif from the third movement.
These minor thirds are explored and combined with other intervals to form a fairly free and wandering section with ambigous tonality.

\customincludegraphics{3-4-1_system}{Free monophonic section, using singing bell samples with long attacks and decays.}

This melody uses proportional notation, to add to the metric freedom of the evolutive sounds. Later, a meter of 4/4 is introduced, and as the movement progresses, the intial long melody is truncated into a chroma of 12 notes, set against a talea of four rhythms: a dotted half note, a half note, a dotted half note rest, and a whole note.
This cycle repeats twice, leading to a shift where the right hand retakes the treble pitches, and the bass moves to the pedals.
The introduction of an ascending triad motive ostinato in C minor follows, with an audio file triggered to reinforce the ostinato, pitch-shifted an octave higher for a brighter sound.
However, just as C minor becomes established, the movement shifts to G minor.
A low G in the bombarde from the bed track cues rapid arpeggiations in the left hand, while the right hand introduces a descending melody reminiscent of the lamentation motif from the third movement.
This section, with its faster left-hand figurations and the bed track's added tension, can be likened to a winter storm, symbolizing chaos and intensity.

The crescendo pedal is used strategically to create large-scale dynamic waves, swelling and fading over approximately four-measure intervals.
In the final phase, the MIDI keyboard transitions from singing wine glasses to glissandi in contrary motion, culminating in a single sustained chord that fades into an ambient mass.

\customincludegraphics{3-4-3_system}{The introduction of the arch motive, symbolizing the staggering arches of l'église Saint-Édouard}

\subsubsection{Linking}

The transition from the third to the fourth movement is seamless, moving from bell-like sounds to singing glass sounds.
This continuity, however, necessitates a deviation from the established structure of singing before each Elegie.
To address this, the melody of the fourth Elegie is placed after the fourth movement, followed immediately by the melody of the fifth Elegie.
This arrangement, while a stark contrast, serves a specific purpose: the first melody acts as an echo of the preceding movement, while the second heralds the beginning of something new.
The common thread between these contrasting elements is the shared instrument of the voice, providing a thematic and auditory link.

\subsection{5e Élégie}

\epigraph{\textit{Mais les errant, dis-moi qui sont-ils, ces voyageurs fugaces?}}{}

\subsubsection{Narrative context}

The fifth elegie dwells on the fleeting nature of life and the enigmatic journey of the soul.
This movement is situated between the third and fourth "stations" of the path.
Before it begins, the protagonist passes through the salle Morin, named after the first priest of l'église Saint-Édouard, where the hum of a refrigerator is heard, the complex sound becoming more and more intense.
Then the distant sound of a clock ticking comes to the forefront, dominating the texture and becoming more and more incessant.
As the intensity of the clock builds, the footsteps, which thusfar have always been near, suddenly begin to recede into the distance, calling the subjectivity of the listener/protagonist into question.
The sound of the dissolving footsteps against the growing intensity of the clock creates an eery dialectic, at once referenceing the non-unisson of the fourth movement, and the nebulousness of time in the fifth.
Just as the clock climaxes, it splits off into several iterations of itself, like time fractionning and dissolving into the fugal mass.
This is the third station on the journey through the church, paralleling the fifth and sixth stations of the cross.

Musically, the movement begins with the voice, singing the poetic fragment with a densely chromatic theme.
This theme is then explored at the organ, on the one hand with the cornet registration, which I associate with thick, incense filled air, and on the other hand, the pyo synthesis server doing stop interpolation, that is, transforming from one stop to another.
This creates a nebulous effect evokative of the voyageurs fugaces mentionned by Rilke.

It ends with a recording that represents the fourth station in the path, that of the furnace room.
Deep in the church's underbelly, we hear the opening of a heavy, sliding, steel door that gives way into the furnace room, gently clicking.
The use of the furnace at this point is a reference to the word fugaces, like the thick incense filled air, the fumes of the furnace spiral up into the sky above the church.

\subsubsection{Techniques}

Software:

In the origins of this project was a fascination with interpolation.
I find it very intriguing to take a sound that seems permanent, stable, fixed in place and historical context, and to transform it into something else.
This capability is one of the key advantages afforded to the use of digital synthesis.
A traditional pipe organ operates in a very binary fashion, and this is especially true of electro-pneumatic organs, in which pulling a stop simply sends a control voltage to open the valve to the rank or ranks in question.
Moreover, even with a tracker organ, where half stops exist and are exploited in some experimental music, they are not so much an interpolation of the timbral space of that stop and silence, but are more like a distortion of the normal register, much like overblowing a flute.
This is not to speak of the idea of morphing from one stop to another.
In order to do this with a traditional organ, one would need to transform the metal in real time.
It's simply not possible!

In the digital world, sound is not linked to physicality in such a direct way.
The loudspeaker opens us up to much more flexible ways of generating and manipulating air pressure waves.
Once I had developed my first iteration of the sound synthesis server in python, one of the first ideas that I wanted to try out was this idea of morphing between stops, as if the metal is being stretched or contracted in realtime.
There's not necessarily an objective way to go about doing this, and the most obvious solution would simply be to use a crossfade, making one stop quieter while another enters.
This would be akin to increasing the transparency of one photo while another increases in opacity.
I considered this approach, and when this visual analogy came to mind, I imagined another possibility, where each pixel moves to where it needs to be for the transformed state.
Technically, this doesn't really make sense, as it would necessitate that both the original and transformed image have the same collection of pixels, but in a different order, which would likely not be the case.
In reality the more appropriate analogy would probably be that each pixel stays exactly where it is, but gradually alters it's RGB values until it reaches the desired state.

In any case, this visualisation gave me the idea to manipulate the timbral space itself for my transformations, using the spectral profiles of each stop, and in particular the amplitudes of each partial, as my pixels--a sort of data primitive which can be morphed as needed.
I had already designed functions which instantiated the profiles of different stops.
The only functionality missing was to define a way to interpolate between these states.
After some research, it seemed that the most straightforward approach would be to replace the Sig() objects with SigTo() objects.
The former generates a constant value, and jumps to a new value when changed, whereas the latter object takes a second argument, that of time, which determines the rate at which it moves to a new value once set.

\begin{figure}[H]
\begin{lstlisting}[language=Python]
        for i in range(len(part)):
            self.amps.append(SigTo(mul[i], time=self.ramp))
            self.att.append(SigTo(att[i], time=self.inter))
            self.dec.append(SigTo(dec[i], time=self.inter))
            self.sus.append(SigTo(sus[i], time=self.inter))
            self.rel.append(SigTo(rel[i], time=self.inter))
            self.envs.append(MidiAdsr(self.note['velocity'], attack=att[i], decay=dec[i], sustain=sus[i], release=rel[i], mul=self.amps[-1]))
            self.part.append(SigTo(part[i], time=0.2))
            self.snds.append(Sine(freq=(self.part[i]**self.partSc) * (MToF(FToM(self.note['pitch']-0.15))) + Randi(-rand, rand, 5) + self.trans + self.mod, mul=self.envs[-1]))
            self.mixed.append(self.snds[-1].mix())
\end{lstlisting}
\caption{This snippet from stop.py uses SigTo() objects to manage interpolation between values.
self.ramp is used to determine the rate of transformation of the amplitudes of the harmonics, and self.inter, short for interpolation, governs the changes between envelope parameters.
For the moment self.inter is not functional, due to MidiAdsr() not allowing live signal for it's parameters (the first value is used at instantiation).} 
\end{figure}

This way of spectral morphing can work well at both large and small time scales.
The former giving a sense of evolutive expansion or contraction, depending on whether the spectrum is brightened or dampened.
The latter has a more elastic effect, as if the sound is a physical object and bounces like a rubber ball.
For this movement, I wanted to automate the morphing process, while experimenting with a range of medium time-scales.
To do this, I drafted a function called stopInter() which simply draws a number between zero and three, at an interval between 1 and 10 seconds.
This number represents the bourdon 8', principal 8', voix humaine 8', and cornet 5Rgs.
stops.
Each time a new time interval is selected, the ramp is changed accordingly, creating a sense of spectral instability and constant shifting, like the smoke of our "voyageurs fugaces".

\begin{figure}[H]
\begin{lstlisting}[language=Python]
stopInterPRand = Sig(1)
def stopInter():
    global stopInterPRand
    x = randint(0, 3)
    stopInterPRand.value = randint(1, 10)
    stop1.setRamp(stopInterPRand)
    print(x)
    if x == 0:
        bourdon()
    elif x == 1:
        principal()
    elif x == 2:
        voixHumaine()
    elif x == 3:
        cornet()
    print('stopInter', stopInterPRand)

stopInterP = Pattern(function=stopInter, time=Sig(stopInterPRand))
\end{lstlisting}
\caption{The function stopInterPRand interpolates between the spectral composition of the bourdon, the principal, the voix humaine, and the cornet.
The pattern stopInterP triggers the function at a randomly chosen time interval between 1 to 10 seconds.
The interpolation is five seconds long, which is defined in the midi\_nav.py file (see GitHub respository).}
\end{figure}

Compositional:

The use of the word fugace in the poetic fragment strongly suggests he traditional form of the fugue.
This realization struck me well into the conception of this piece, long after the idea to use interpolation to represent the nebulous, smoky evocations, and presented several technical challenges.
In some sense, these two ideas are completely opposed.
The interpolation idea was fundamentally ambient and evolutive, whereas the fugue has historically featured clearly defined rhythms and melodic motives as a key element.
Moreover, the interpolation works best with the sustain pedal depressed, and with a wide spread of intervals, to create an immersive resonance, but the fugue theme, which naturally sprung from the text, is tightly weaved and chromatic.

My solution to these problems is to treat these two ideas as largely distinct formal elements.
I view the more traditional fugal writing like a thread, or a dimly lit path, which at times becomes shrouded in darkness.
This "lost path" is represented by the ambient, evolutive sections using stop interpolation.
It is indeed as if our travellers give the impression of going somewhere, only to vanish in smoke.
At the same time, it has a psychological effect, as if following an idea, only to become lost in thought.

For these ambient sections, I allow myself to use non-fugal material, but in order to fuse the two ideas, I do reference the fugue theme from time to time, though with some important alterations.
In order to create an ambient texture, I generally apply augmentation to the durations, and to avoid extreme chromaticism, I use a technique borrowed from the fugue of the sixth movement of Messiaen's Vingt Regards sur l'enfant Jesus.
In this movment, Messiaen uses octave displacement throughout to disguise his subject.
Though in a very different paradigm, I use this technique to lessen the dissonance of the texture, and open up the resonance during the ambient sections.
I displace the notes according to the closest partiel in the spectral analysis of the bell of l'église Saint-Édouard.
This gives me a sound that is based in a physical object, tied to the space itself, and yet which maintains a level of chromaticism, yielding an uneasy tension, like difficult dreams.

\subsubsection{Composition Process}

\subsubsection{Linking}

\subsection{6e Élégie}

\epigraph{\textit{Figuier, depuis longtemps déjà ce m'est un signe que presque entièrement tu te dérobes à la gloire des fleures...}}{}

\subsubsection{Narrative context}

To me this is the crux of Rilke's texts, and the key narrative moment of the piece.
It is like the voice of God acknowledging that the subject is not living up to their fullest potential, that they are shrinking before the greatness of their task.
It is the point in the story where the protagonist must face his mortality, questioning the possibility of continuing, feeling crushed under the weight of burden, as a small fragile child.
For this movement I wanted to make use of what I call the dissociated bourdon.
In this synthesis patch, the harmonic content, and the transient noise content (marking the attacks of notes), are treated independently, where the noise sounds on every note onset, but the harmonic content sounds every sixth onset.
This creates an effect which is haunting and fragile--largely rhythmic.
The sound world is based in F minor, which refers to the minor triad contained in the harmonics of the spectral analysis of the lowest bell of l'église Saint-Édouard, and fragmented bursts of sound are heard interspersed with the clicking of the transient sounds.
Interestingly, there would have been a fuller contrapuntal message, but it is truncated, hidden, and lost to time.
This is a metaphor for the fig tree, hiding itself and dissociating from its truer form.
For this movement, I thought it would be appropriate to leave the stage, further dissociating the sound from the physical source.
This sort of theatrical element has interested me for a long time, and it provides a dramatic support to the narrative importance of this movement.

\subsubsection{Techniques}

Software:

The idea for the dissociated bourdon came as a result of an error of polyphony management when designing my initial synthesis program.
The issue was that my sound was separated into six channels, and instead of triggering each channel on each note event, pyo was cycling through the channels, and putting all the harmonic content on the first step.
The noise content was not subject to this polyphony issue and came through on each step, creating an intriguing texture, largely rhythmic, and punctuated with bursts of organ sound.
I quickly realized that I needed to sum the channels with a Mix() object before passing the sound to the output, but I was quite captivated by the result of this "happy error".
I kept that broken patch set aside, and soon after I tested it at the church, recording the result, and this is what ultimately became the bed track for this movement.

\begin{figure}[H]
\begin{lstlisting}[language=Python]
dissCount = 0
def dissocie(x):
    global dissCount
    print(x)
    if x != 0:
        dissCount += 1
        print(dissCount)
        if dissCount > 1:
            stop1.setMul([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
            print("set0")
        elif dissCount == 1 :
            stop1.setMul([1, 0.01, 0.1, 0.01, 0.07, 0, 0.02, 0, 0.01, 0, 0.003, 0, 0.003, 0, 0.001, 0, 0.001, 0, 0.001, 0])
            print("setnon0")
    if dissCount == 4:
        dissCount = 0
\end{lstlisting}
\caption{The function dissocie mutes the harmonic content of the organ synthesis every five notes.
For the other four attacks, only the noise of the transient and the air is heard.}
\end{figure}

Compositional:

Compositionally, the piece is largely freely improvised.
I instinctively reached for a pan-tonal palette, which seemed to adequately represent a sort of pathetic nature.
In this case, my improvisation was based in F minor.
This was not planned in any meaningful way at the time, but in retrospect, it is an appropriate choice, as the spectral analysis of the bells of l'église Saint-Édouard reveal a duality of F major and F minor.
My piece attempts to put this relationship in dialogue, and the fact that structurally, at the point of great doubt, the tonality arrives in F minor, feels thematically consistent.

\subsubsection{Theatrical elements}

Prior to the sixth movement, there are already present several important, though more subtle, theatrical elements.
At the beginning of the piece, the fact that I climb the stairs to arrive at the organ while the initial bed track plays can be seen as a theatrical staging.
From the perspective of the audience, they do not necessarily know whether this is simply a practical necessity, or a dramatic element--in other words, it is unclear whether the piece has already started during the bed track, or whether my arrival at the organ console initiated the beginning.
My sense is that initially, the natural interpretation is likely that the piece starts when I arrive, with the bed track serving as an introduction, but that in retrospect, once more theatric elements are introduced, the beginning of the piece is reinterpreted as having already begun without the audiences' knowledge.
Besides the initial entrance, the movements from the organ console to vocalise are a form of staging, though fairly restricted in scope.

In the sixth movement, I underline the narrative importance of this movement with the introduction of a much more marked theatrical element.
I leave the stage at the end of the fifth movement, before I do, triggering an audio clip which on the one hand transitions from the fifth movement with recordings of the final chord doing the stop interpolation, in order to give me time to leave the stage and enter the room without too much dead space.
Then, the audience hears a disembodied voice before the entrance of the dissociated bourdon.
This yields a sensation of alienation, highlighting the tension between the hopelessness of our protagonist and the call to a higher state of being.

Originally, I imagined that I would play the keyboard for this movement off stage, but this was a point of frustration, because it was technically infeasible.
As the keyboard is connected to my interface by a cable, it would require managing a very long cable, or having a separate setup with another computer, interface, and keyboard in another room, for only one movement.
Ultimately, in discussing the options with my co-supervisor Caroline Traube, we devised the plan to have this movement be entirely pre-recorded.
In this way, something is undeniably lost.
The immediacy of music performed in real-time is replaced with the automaton.
At the same time, from the narrative perspective, this is even better.
The timbral dissociation is linked with the physical disembodiment of the performer and their sound.
Since the performer is in another room, the audience is left to wonder what the source of this sound is.
Is the performer playing it from somewhere?
Or is it pre-recorded?
They are not told ahead of time so they are left to ponder this, putting the role of presence and physicality in music-making in question.

Just after the ambient, evolutive final chord from the fifth movement fugue ends and the dissociated bourdon of the sixth movement begins, the performer sings from another room sings an inversion of the principal melody, but begins with an incessant loop of the first gesture, "Figuier", as if frozen in time, or in thought.
As this syllable is uttered, the closed door of this room is slowly opened, mimicking the opening of the jalousies of an organ enclosure/expressive box.
After this disembodied voice is heard from afar, the recording of the dissociated bourdon enters.
Originally I considered triggering this recording with OSC, but during a testing of various techniques and excerpts from the Élégies, I found that the internet connection was not reliable enough to make this a good solution, and instead planned to simply time the sung material approximately with the space left between the end of the fifth movement and the sound file trigger of the sixth.

At the same time that I was experimenting with the role of physicality, and space, I wanted to increase the poignancy and effectiveness of this dramatic effect of alienation by further exploring of the auditory space, not only relocating the performer, but also the sound itself.
Instead of using the same speaker beside the pipe organ that has been used thus-far, a separate sound system located in the second balcony in the back of the church where the organ originally stood sounds the dissociated bourdon recording.
At the same time, a flashlight with a cold light is illuminated in this same space.
From a technical standpoint, this in theory could be controlled with software, LEDs, and a cabled connection, but it was more practical to simply ask a friend to stay in the balcony with a flashlight during the performance, awaiting the auditory cue, which is the end of my singing.
The friend not only turns on the light, but also triggers the recording, ensuring that they are well timed and synchronous.
Seeing as the light is behind the audience, it's possible that this won't even be noticed by certain audience members.
At the same time, seeing as the performer has left the stage, they will no longer have a visual aspect to attract their attention, and in the confusion of identifying their sound source, most of them will instinctually look behind them to find it, and doing so will see the light.
This simple visual elaboration signifies on the one hand, the simple, fragile, yet constant spiritual presence of our protagonist, symbolized as a fig tree, and simultaneously represents the phantom of the organ in its original home, now vacant.
It also invokes the separation of the traditional physical performer and the mechanized, automated version--the ghost in the machine.

\subsection{7e Élégie}

\epigraph{\textit{Non, plus d’imploration, voix maintenant mûrie, plus de clameur...}}{}

\subsubsection{Narrative context}

Following the fragile unsurety of the sixth movement, the seventh represents a call to action.
It is as if our protagonist, half awake from their frozen dream, summons themself with an urgent plea.
It isn't clear whether this plea is from within, or whether it comes from some external source, but it seems to imply some impending event.
To me this symbolizes a calm before the storm, a rising tension before the final climax.
I explore this tension is several ways.
On the one hand, the performer is still not present at the instrument, but makes several short appearances elsewhere in the church, creating a sense of uneasiness as these appearances are on the one hand unexpected, and as the space is relatively dark, the identity of the figure will be hard to determine, giving the impression of a multiplicity of our protagonist, none of which are wholly embodied, only existing for several moments, like many transient ghosts.

In several ways, the last four movements are merged into one.
In the most pragmatic terms, there is no break in between, unlike the preceding movements.
Narrative elements such as the delay from the ninth elegie is used at the end of the seventh movement in the form of an antiphonic echo, and the singing of hymns of praise from the text of the tenth elegie is evoked in the beginning of the ninth elegie.
This fusion of formal elements parallels the spiritual awakening of Rilke's poetry.
Attempts will be made throughout these last sections to carefully clarify where these overlapping elements are in play.

\subsubsection{Technique}

The techniques of this movement are different than all the preceding movements.
Instead of being based on synthesis in python, they are based in the electroacoustic domain.
Samples of bells, of the organ, and of the dissociated bourdon are transformed through pitch shifting and envelope shaping.
The sampling of the bells in particular, done using Ableton's Sampler plugin, attempts to take a sonic artefact, which is though inharmonic, remarquably stable.
That's to say, in a physical sense.
The carillon is in fact the heaviest instrument in the world by a large margin, and the thought of a pitch bend with the carillon would be unthinkable, for instance.
Unlike the voice, which is extremely maleable and versatile, the carillon is rooted and unmoveable.
The application of pitch envelopes in particular, gives the carillon a more organic quality, as if this massive, suspended, titan, is given liquid form, called down from the heavens to express itself in a vocal way.

Another key element is a sample of the fire alarms of the church, which sounded one day while I was recording and experimenting with dissonant clusters.
The sound surprised me, and I wasn’t sure what it represented at first.
I considered stopping to investigate the sound, but decided to continue.
This alarm gives a sense of dramatic tension and impending, appropriate with the poetic fragment.
Towards the end of the piece, the sound of footsteps, recorded in l'église Saint-Édouard with my Zoom H4N recorder, become more and more densely layered.
These footsteps were recorded by myself, as I ran towards the microphone.
The intention was to accent this sense of urgency, playing off the footsteps motive that has been heard intermittently throughout the piece, yet now completely decoupling the subject and the sound source, creating an almost cinematic effect, where we are no longer following over the shoulder of the protagonist, but we are suddenly viewing the protagonist, or some person or animal, which is running towards us.
This sound is spatialised in all five speaker, representing the first time since the introductory pad since all speakers are used.
The sound is passed from speaker to speaker, providing the sensation of being encircled by the sound.

These running footsteps were recorded in two sessions, and in the first session, I had previously been attempting to get a near-infinite reverb without feedback, trying different microphones, and different decay intervals, and most of the feedbacks quickly became out of control, needing to be shut off.
One of the feedbacks however, which was less of a feedback of the organ, and moreso of the space itself, as in my recollection I hadn't played a note as I had in other experiments, established a sort of stasis, neither growing nor diminishing, remaining in a constant periodic throbbing.
I liked this sound so much, that I decided to try recording the sound of the footsteps overtop of it.
This experience of running and accelerating towards the microphone, through the central aisle of the church, gave me an eerie sense of traversing a portal, a strong wave of shivers going up my spine each time I passed the microphone.

\subsubsection{Composition}

After the sixth movement, the light goes out leaving the audience in the dark and the silence for a moment.
Out of this silence, an electroacoustic piece begins using mainly samples of the church bells, but mutated with pitch and dynamic envelopes, and a sample of the fire alarms of the church, which began one day while I was experimenting with dissonant clusters.
The sound surprised me, and I wasn’t sure what it represented at first.
I considered stopping to investigate the sound, but decided to continue.
The alarm gives a sense of dramatic tension and impending, appropriate with the poetic fragment: “Non, plus d’imploration, voix maintenant mûrie, plus de clameur.”.
This text suggests a sense of immediacy for something, but this something is not clear.
The alarm helps with establishing this sense of a calm before the storm, and the thick clusters of organ chords create a sense of ominous tension.
This movement reaches a sort of energetic climax, where the sound of urgent footsteps are heard traversing the church and encircling the space, becoming layered and more and more dense.
At the same time, the throbbing of a sound of feedback that mysteriously appeared in the church one day when I was testing the reverb adds to the sense of tension.
Both the footsteps and the throbbing feedback represent the fifth station in the stations of the cross, (each station represents two actual stations, so this would correspond to the 9th and 10th stations), which corresponds to me entering the church from the exterior.
This movement then reaches a stage of quietude, with simply a low pulsating drone and the fire alarm and rumblings far off in the distance.

\subsubsection{Theatrical elements}

During this movement, the performer remains off stage.
The intention with this decision is to continue the sense of disembodiment established in the sixth movement, and whereas the musical element in the sixth could imaginably be played on a single instrument, due to its minimal, soloistic nature, the seventh movement is so densely layered that it is clearly an electroacoustic piece.
This has the effect of dissolving even further the relationship between the sound source and the physicality of the performer.
The audience is no longer imagining the musician's hands, but is now lost, in a dreamlike way, in visions, symbols, and memories.
This mirror's the effect of the footsteps described in the previous section.
Just as the sound of the footsteps approaching the microphone creates a sense of embodied experience, rather than simple voyeurism, the aural landscape is transformed from one that is voyeuristic and observational, to one that is more interiorized and subjective.
During this movement, rather than resting off stage as in the sixth, I traverse the space of the church, appearing briefly several times, first in the north entry of the east side of the church, and then in the balcony opposite to the organ (the south-transept).
During both of these apparitions, I appear, remain completely immobile for a short time, and then disappear.
The strangeness of these appearances, and the disconnect between the sonic experience and the human form, increases the sense of disembodiment and general tension, and parallels the themes of hauntology, acting as a phantom presence.
Due to the relatively dark conditions, my identity will be unclear, and audience members may question whether it is me or someone else, as if the phantom is duplicated, or transported throughout the space.

Once the music sonically reaches the low pulsating drone, I reenter the stage, putting on a lavalier microphone on, and approaching the organ.
In my mind, the moment I begin to play the organ represents neither the seventh nor the eighth movement, but rather a liminal space between the two.
The sound of the organ, based on the ascending motive, is echoed on the opposite balcony (the south transept), using a delay line in Ableton which is fed into a speaker placed on the opposing balcony.
This echo is reminiscent of the call and response practices of african and african diaspora, as well as the ancient practice of antiphony, where two choirs would be located in different parts of a church and would dialogue between them.
This echo is also evocative of several themes of the piece: the echo of history, as the historic practice of antiphony is reinstated in a modern context, and the spiritual aspect of the echo being like a recursion of subjectivity—the voice speaking to itself; the eye staring back into itself; the void rushing in to fill itself.
This echo, digitally rendered in Ableton, solves the problem of not being able to play the acoustic pipe organ from the other side of the church, and inherently integrates the antiphony idea with the technological experiments.
At the same time, the possibility of musically altered responses, like responding with a different harmony, is lost.

\subsection{8e Élégie}

\epigraph{\textit{A plein regards, la créature voit dans l’Ouvert.}}{}

\subsubsection{Narrative context}

\subsubsection{Techniques}

\subsubsection{Composition Process}

\subsubsection{Linking}

\subsection{9e Élégie}

\epigraph{\textit{Pourquoi, s’il est loisible aussi bien de remplir son délai d’existence en laurier, sombre un peu plus que tous les autres verts, avec ces vaguelettes...}}{}

\subsubsection{Narrative context}

\subsubsection{Techniques}

\subsubsection{Composition Process}

\subsubsection{Linking}

\subsection{10e Élégie}

\epigraph{\textit{Vienne le jour enfin, sortant de la voyance encolérée, où je chante la gloire et la jubilation aux Anges qui l’agréent. Que des marteaux du cœur au battement très clair aucun ne vienne à faux tomber sur une corde molle, ou encore douteuse ou prête à se briser.}}{}

\subsubsection{Narrative context}

\subsubsection{Techniques}

\subsubsection{Composition Process}

\subsubsection{Linking}

%%--------------%
%%     index    %
%%--------------%

%% S'il y a lieu, décommenter la ligne pour mettre votre index

%%\printindex

%%------------------------------------------------- %
%%         références --- bibliographie             %
%%------------------------------------------------- %
  % Enlever les commentaires de la prochaine commande si vous préférez que le
  % chapitre s'appelle « Références » plutôt que « Bibliographie » (au choix selon le contexte).
%%\let\bibname=\refname

%% Lorsque vous serez prêt à faire afficher votre bibliographie
%% et vos références, enlevez les commandaires des commandes suivantes
%% et donnez le nom de votre fichier .bib à la commande \bibliography{..}
%% (consultez l'exemple au besoin).  Vous pouvez utiliser le style de votre
%% choix.
%%\bibliographystyle{plain}     % Le style de la bibliographie. Notons que
                                        % les extensions ne sont pas données pour ces deux fichiers.
%%\def\bibname{R\'ef\'erences bibliographiques} % Nom obligatoire de la section des références.
                                              % On utilise \'e car le é cause des problèmes
                                              % dans la table des matière
%% ENGLISH
%%\def\bibname{References}
\printbibliography
%%\bibliography{ref}     % La base de données contenant des entrées bibliographiques.
                                    % Seules celles référencées dans le texte seront ajoutées
                                    % \`a la bibliographie.

%%------------------------------------------------- %
%%                  Annexe A                        %
%%------------------------------------------------- %

\appendix
\chapter{Le titre}

\section{Section un de l'Annexe A}

...texte...

\chapter{Les différentes parties et leur ordre d'apparition}

J'ajoute ici les différentes parties d'un mémoire ou d'une thèse ainsi
que leur ordre d'apparition tel que décrit dans le guide de
présentation des mémoires et des thèses de la Faculté des études
supérieures.  Pour plus d'information, consultez le guide sur le site
web de la facutlé (www.fes.umontreal.ca).

\newcount\colnum
\colnum=1
\def\i{\number\colnum. \global\advance\colnum by 1\ignorespaces}
\begin{table}[p]
  \begin{center}
    \begin{tabular}{|l|l|r|}\hline
       \noindent\hfil
         \textbf{\strut Ordre des éléments constitutifs du mémoire ou de la thèse}
         \hfil\span\omit\span\omit\\\hline % \span\omit pour couvrir plus d'une
                                           % case sans utiliser le package multirow ou autre
      \i &  La page de titre & obligatoire\\\hline
      \i &  La page d'identification des membres du jury & obligatoire\\\hline
      \i &  Le résumé en français et les mots clés français\kern3em& obligatoires\\\hline
      \i &  Le résumé en anglais et les mots clés anglais & obligatoires\\\hline
      \i &  Le résumé dans une autre langue que l'anglais & obligatoire \\
         &  ou le français (si le document est écrit dans &\\
         &  une autre langue que l'anglais ou le français)&\\\hline
      \i &  Le résumé de vulgarisation& facultatif\\\hline
      \i &  La table des matières, la liste des tableaux,& obligatoires\\
         &   la liste des figures ou autre &\\\hline
      \i &  La liste des sigles et des abréviations& obligatoire\\\hline
      \i &  La dédicace& facultative\\\hline
      \i &  Les remerciements & facultatifs\\\hline
      \i &  L'avant-propos & facultatif\\\hline
      \i &  Le corps de l'ouvrage& obligatoire\\\hline
      \i &  Les index& facultatif\\\hline
      \i &  Les références bibliographiques & obligatoires\\\hline
      \i &  Les annexes & facultatifs\\\hline
      \i &  Les documents spéciaux & facultatifs\\\hline
    \end{tabular}
  \end{center}
\end{table}

\end{document}

\endinput
%%
%% End of file `gabaritmem.tex'.
